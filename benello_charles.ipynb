{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a662e0be",
   "metadata": {},
   "source": [
    "## This is the midterm submission for Charles Benello on 10/20\n",
    "\n",
    "\n",
    "I used chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafdbf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "from typing import Union, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba669cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'Sheet1' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mdata/midterm_1_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m sheet_name_1, sheet_name_2, sheet_name_3 = \u001b[33m\"\u001b[39m\u001b[33mSheet1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSheet2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSheet3\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# fill these\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df2 = pd.read_excel(file_path, sheet_name=sheet_name_2, index_col=\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m df3 = pd.read_excel(file_path, sheet_name=sheet_name_3, index_col=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finm/lib/python3.12/site-packages/pandas/io/excel/_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finm/lib/python3.12/site-packages/pandas/io/excel/_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finm/lib/python3.12/site-packages/pandas/io/excel/_base.py:773\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     sheet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finm/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:582\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book[name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finm/lib/python3.12/site-packages/pandas/io/excel/_base.py:624\u001b[39m, in \u001b[36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sheet_names:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorksheet named \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Worksheet named 'Sheet1' not found"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "file_path_1 = \"data/midterm_1_stock_returns.xlsx\"\n",
    "file_path_2 = \"data/midterm_1_fund_returns.xlsx\"\n",
    "\n",
    "sheet_name_1, sheet_name_2, sheet_name_3 = \"Sheet1\", \"Sheet2\", \"Sheet3\"  # fill these\n",
    "\n",
    "df1 = pd.read_excel(file_path_1, sheet_name=sheet_name_1, index_col=0)\n",
    "df2 = pd.read_excel(file_path_2, sheet_name=sheet_name_2, index_col=0)\n",
    "# df3 = pd.read_excel(file_path, sheet_name=sheet_name_3, index_col=0)\n",
    "\n",
    "df1.index = pd.to_datetime(df1.index)\n",
    "df2.index = pd.to_datetime(df2.index)\n",
    "df3.index = pd.to_datetime(df3.index)\n",
    "\n",
    "x, y, z = df1.copy(), df2.copy(), df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7cd09",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405cd39",
   "metadata": {},
   "source": [
    "* Hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d546ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nportfolio_regularized_not_scaled = calc_tangency_weights(assets_excess_returns, return_port_ret=True, cov_mat=.5, name=\"Regularized\")\\nportfolio_regularized = portfolio_regularized_not_scaled * MU_MONTH_TARGET / portfolio_regularized_not_scaled.mean()\\nportfolio_regularized\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_tangency_weights(\n",
    "    returns: pd.DataFrame,\n",
    "    cov_mat: str = 1,\n",
    "    return_graphic: bool = False,\n",
    "    return_port_ret: bool = False,\n",
    "    target_ret_rescale_weights: Union[None, float] = None,\n",
    "    annual_factor: int = 12,\n",
    "    name: str = 'Tangency',\n",
    "    expected_returns: Union[pd.Series, pd.DataFrame] = None,\n",
    "    expected_returns_already_annualized: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates tangency portfolio weights based on the covariance matrix of returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): Time series of returns.\n",
    "    cov_mat (str, default=1): Covariance matrix for calculating tangency weights.\n",
    "    return_graphic (bool, default=False): If True, plots the tangency weights.\n",
    "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
    "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
    "    annual_factor (int, default=12): Factor for annualizing returns.\n",
    "    name (str, default='Tangency'): Name for labeling the weights and portfolio.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or pd.Series: Tangency portfolio weights or portfolio returns if `return_port_ret` is True.\n",
    "    \"\"\"\n",
    "    returns = returns.copy()\n",
    "    \n",
    "    if 'date' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "    returns.index.name = 'date'\n",
    "\n",
    "    if cov_mat == 1:\n",
    "        cov_inv = np.linalg.inv((returns.cov() * annual_factor))\n",
    "    else:\n",
    "        cov = returns.cov()\n",
    "        covmat_diag = np.diag(np.diag((cov)))\n",
    "        covmat = cov_mat * cov + (1 - cov_mat) * covmat_diag\n",
    "        cov_inv = np.linalg.pinv((covmat * annual_factor))  \n",
    "        \n",
    "    ones = np.ones(returns.columns.shape) \n",
    "    if expected_returns is not None:\n",
    "        mu = expected_returns\n",
    "        if not expected_returns_already_annualized:\n",
    "            mu *= annual_factor\n",
    "    else:\n",
    "        mu = returns.mean() * annual_factor\n",
    "    scaling = 1 / (np.transpose(ones) @ cov_inv @ mu)\n",
    "    tangent_return = scaling * (cov_inv @ mu)\n",
    "    tangency_wts = pd.DataFrame(\n",
    "        index=returns.columns,\n",
    "        data=tangent_return,\n",
    "        columns=[f'{name} Weights']\n",
    "    )\n",
    "    port_returns = returns @ tangency_wts.rename({f'{name} Weights': f'{name} Portfolio'}, axis=1)\n",
    "\n",
    "    if return_graphic:\n",
    "        tangency_wts.plot(kind='bar', title=f'{name} Weights')\n",
    "\n",
    "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
    "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
    "        tangency_wts[[f'{name} Weights']] *= scaler\n",
    "        port_returns *= scaler\n",
    "        tangency_wts = tangency_wts.rename(\n",
    "            {f'{name} Weights': f'{name} Weights Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
    "            axis=1\n",
    "        )\n",
    "        port_returns = port_returns.rename(\n",
    "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    if cov_mat != 1:\n",
    "        port_returns = port_returns.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
    "        tangency_wts = tangency_wts.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
    "        \n",
    "    if return_port_ret:\n",
    "        return port_returns\n",
    "    return tangency_wts\n",
    "\n",
    "def filter_columns_and_indexes(\n",
    "    df: pd.DataFrame,\n",
    "    keep_columns: Union[list, str],\n",
    "    drop_columns: Union[list, str],\n",
    "    keep_indexes: Union[list, str],\n",
    "    drop_indexes: Union[list, str],\n",
    "    drop_before_keep: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame based on specified columns and indexes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to be filtered.\n",
    "    keep_columns (list or str): Columns to keep in the DataFrame.\n",
    "    drop_columns (list or str): Columns to drop from the DataFrame.\n",
    "    keep_indexes (list or str): Indexes to keep in the DataFrame.\n",
    "    drop_indexes (list or str): Indexes to drop from the DataFrame.\n",
    "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    # Columns\n",
    "    if keep_columns is not None:\n",
    "        keep_columns = \"(?i)\" + \"|\".join(keep_columns) if isinstance(keep_columns, list) else \"(?i)\" + keep_columns\n",
    "    else:\n",
    "        keep_columns = None\n",
    "    if drop_columns is not None:\n",
    "        drop_columns = \"(?i)\" + \"|\".join(drop_columns) if isinstance(drop_columns, list) else \"(?i)\" + drop_columns\n",
    "    else:\n",
    "        drop_columns = None\n",
    "    if not drop_before_keep:\n",
    "        if keep_columns is not None:\n",
    "            df = df.filter(regex=keep_columns)\n",
    "    if drop_columns is not None:\n",
    "        df = df.drop(columns=df.filter(regex=drop_columns).columns)\n",
    "    if drop_before_keep:\n",
    "        if keep_columns is not None:\n",
    "            df = df.filter(regex=keep_columns)\n",
    "    # Indexes\n",
    "    if keep_indexes is not None:\n",
    "        keep_indexes = \"(?i)\" + \"|\".join(keep_indexes) if isinstance(keep_indexes, list) else \"(?i)\" + keep_indexes\n",
    "    else:\n",
    "        keep_indexes = None\n",
    "    if drop_indexes is not None:\n",
    "        drop_indexes = \"(?i)\" + \"|\".join(drop_indexes) if isinstance(drop_indexes, list) else \"(?i)\" + drop_indexes\n",
    "    else:\n",
    "        drop_indexes = None\n",
    "    if not drop_before_keep:\n",
    "        if keep_indexes is not None:\n",
    "            df = df.filter(regex=keep_indexes, axis=0)\n",
    "    if drop_indexes is not None:\n",
    "        df = df.drop(index=df.filter(regex=drop_indexes, axis=0).index)\n",
    "    if drop_before_keep:\n",
    "        if keep_indexes is not None:\n",
    "            df = df.filter(regex=keep_indexes, axis=0)\n",
    "    return df\n",
    "\n",
    "def calc_summary_statistics(\n",
    "    returns: Union[pd.DataFrame, List],\n",
    "    annual_factor: int = None,\n",
    "    provided_excess_returns: bool = None,\n",
    "    rf: Union[pd.Series, pd.DataFrame] = None,\n",
    "    var_quantile: Union[float, List] = .05,\n",
    "    timeframes: Union[None, dict] = None,\n",
    "    return_tangency_weights: bool = True,\n",
    "    correlations: Union[bool, List] = True,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None,\n",
    "    drop_before_keep: bool = False,\n",
    "    _timeframe_name: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates summary statistics for a time series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or List): Time series of returns.\n",
    "    annual_factor (int, default=None): Factor for annualizing returns.\n",
    "    provided_excess_returns (bool, default=None): Whether excess returns are already provided.\n",
    "    rf (pd.Series or pd.DataFrame, default=None): Risk-free rate data.\n",
    "    var_quantile (float or list, default=0.05): Quantile for Value at Risk (VaR) calculation.\n",
    "    timeframes (dict or None, default=None): Dictionary of timeframes to calculate statistics for each period.\n",
    "    return_tangency_weights (bool, default=True): If True, returns tangency portfolio weights.\n",
    "    correlations (bool or list, default=True): If True, returns correlations, or specify columns for correlations.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary statistics of the returns.\n",
    "    \"\"\"\n",
    "    returns = returns.copy()\n",
    "    if isinstance(rf, (pd.Series, pd.DataFrame)):\n",
    "        rf = rf.copy()\n",
    "        if provided_excess_returns is True:\n",
    "            raise Exception(\n",
    "                'rf is provided but excess returns were provided as well.'\n",
    "                'Remove \"rf\" or set \"provided_excess_returns\" to None or False'\n",
    "            )\n",
    "        \n",
    "    if isinstance(returns, list):\n",
    "        returns_list = returns[:]\n",
    "        returns = pd.DataFrame({})\n",
    "        for series in returns_list:\n",
    "            returns = returns.merge(series, right_index=True, left_index=True, how='outer')\n",
    "    \"\"\"\n",
    "    This functions returns the summary statistics for the input total/excess returns passed\n",
    "    into the function\n",
    "    \"\"\"\n",
    "    if 'date' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "    returns.index.name = 'date'\n",
    "\n",
    "    try:\n",
    "        returns.index = pd.to_datetime(returns.index.map(lambda x: x.date()))\n",
    "    except AttributeError:\n",
    "        print('Could not convert \"date\" index to datetime.date')\n",
    "        pass\n",
    "\n",
    "    returns = returns.apply(lambda x: x.astype(float))\n",
    "\n",
    "    if annual_factor is None:\n",
    "        print('Assuming monthly returns with annualization term of 12')\n",
    "        annual_factor = 12\n",
    "\n",
    "    if provided_excess_returns is None:\n",
    "        print(\n",
    "            'Assuming excess returns were provided to calculate Sharpe.'\n",
    "            ' If returns were provided (steady of excess returns), the column \"Sharpe\" is actually \"Mean/Volatility\"'\n",
    "        )\n",
    "        provided_excess_returns = True\n",
    "    elif provided_excess_returns is False:\n",
    "        if rf is not None:\n",
    "            if len(rf.index) != len(returns.index):\n",
    "                raise Exception('\"rf\" index must be the same lenght as \"returns\"')\n",
    "            print('\"rf\" is used to subtract returns to calculate Sharpe, but nothing else')\n",
    "\n",
    "    if isinstance(timeframes, dict):\n",
    "        all_timeframes_summary_statistics = pd.DataFrame({})\n",
    "        for name, timeframe in timeframes.items():\n",
    "            if timeframe[0] and timeframe[1]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
    "            elif timeframe[0]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:]\n",
    "            elif timeframe[1]:\n",
    "                timeframe_returns = returns.loc[:timeframe[1]]\n",
    "            else:\n",
    "                timeframe_returns = returns.copy()\n",
    "            if len(timeframe_returns.index) == 0:\n",
    "                raise Exception(f'No returns for {name} timeframe')\n",
    "            timeframe_returns = timeframe_returns.rename(columns=lambda c: c + f' {name}')\n",
    "            timeframe_summary_statistics = calc_summary_statistics(\n",
    "                returns=timeframe_returns,\n",
    "                annual_factor=annual_factor,\n",
    "                provided_excess_returns=provided_excess_returns,\n",
    "                rf=rf,\n",
    "                var_quantile=var_quantile,\n",
    "                timeframes=None,\n",
    "                correlations=correlations,\n",
    "                _timeframe_name=name,\n",
    "                keep_columns=keep_columns,\n",
    "                drop_columns=drop_columns,\n",
    "                keep_indexes=keep_indexes,\n",
    "                drop_indexes=drop_indexes,\n",
    "                drop_before_keep=drop_before_keep\n",
    "            )\n",
    "            all_timeframes_summary_statistics = pd.concat(\n",
    "                [all_timeframes_summary_statistics, timeframe_summary_statistics],\n",
    "                axis=0\n",
    "            )\n",
    "        return all_timeframes_summary_statistics\n",
    "\n",
    "    summary_statistics = pd.DataFrame(index=returns.columns)\n",
    "    summary_statistics['Mean'] = returns.mean()\n",
    "    summary_statistics['Annualized Mean'] = returns.mean() * annual_factor\n",
    "    summary_statistics['Vol'] = returns.std()\n",
    "    summary_statistics['Annualized Vol'] = returns.std() * np.sqrt(annual_factor)\n",
    "    try:\n",
    "        if not provided_excess_returns:\n",
    "            if type(rf) == pd.DataFrame:\n",
    "                rf = rf.iloc[:, 0].to_list()\n",
    "            elif type(rf) == pd.Series:\n",
    "                rf = rf.to_list()\n",
    "            else:\n",
    "                raise Exception('\"rf\" must be either a pd.DataFrame or pd.Series')\n",
    "            excess_returns = returns.apply(lambda x: x - rf)\n",
    "            summary_statistics['Sharpe'] = excess_returns.mean() / returns.std()\n",
    "        else:\n",
    "            summary_statistics['Sharpe'] = returns.mean() / returns.std()\n",
    "    except Exception as e:\n",
    "        print(f'Could not calculate Sharpe: {e}')\n",
    "    summary_statistics['Annualized Sharpe'] = summary_statistics['Sharpe'] * np.sqrt(annual_factor)\n",
    "    summary_statistics['Min'] = returns.min()\n",
    "    summary_statistics['Max'] = returns.max()\n",
    "    summary_statistics['Skewness'] = returns.skew()\n",
    "    summary_statistics['Excess Kurtosis'] = returns.kurtosis()\n",
    "    var_quantile = [var_quantile] if isinstance(var_quantile, (float, int)) else var_quantile\n",
    "    for var_q in var_quantile:\n",
    "        summary_statistics[f'Historical VaR ({var_q:.2%})'] = returns.quantile(var_q, axis = 0)\n",
    "        summary_statistics[f'Annualized Historical VaR ({var_q:.2%})'] = returns.quantile(var_q, axis = 0) * np.sqrt(annual_factor)\n",
    "        summary_statistics[f'Historical CVaR ({var_q:.2%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean()\n",
    "        summary_statistics[f'Annualized Historical CVaR ({var_q:.2%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean() * np.sqrt(annual_factor)\n",
    "    \n",
    "    wealth_index = 1000 * (1 + returns).cumprod()\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
    "\n",
    "    summary_statistics['Max Drawdown'] = drawdowns.min()\n",
    "    summary_statistics['Peak'] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns]\n",
    "    summary_statistics['Bottom'] = drawdowns.idxmin()\n",
    "\n",
    "    if return_tangency_weights:\n",
    "        tangency_weights = calc_tangency_weights(returns)\n",
    "        summary_statistics = summary_statistics.join(tangency_weights)\n",
    "    \n",
    "    recovery_date = []\n",
    "    for col in wealth_index.columns:\n",
    "        prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max()\n",
    "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T\n",
    "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
    "    summary_statistics['Recovery'] = recovery_date\n",
    "    try:\n",
    "        summary_statistics[\"Duration (days)\"] = [\n",
    "            (i - j).days if i != \"-\" else \"-\" for i, j in\n",
    "            zip(summary_statistics[\"Recovery\"], summary_statistics[\"Bottom\"])\n",
    "        ]\n",
    "    except (AttributeError, TypeError) as e:\n",
    "        print(f'Cannot calculate \"Drawdown Duration\" calculation because there was no recovery or because index are not dates: {str(e)}')\n",
    "\n",
    "    if correlations is True or isinstance(correlations, list):\n",
    "        returns_corr = returns.corr()\n",
    "        if _timeframe_name:\n",
    "            returns_corr = returns_corr.rename(columns=lambda c: c.replace(f' {_timeframe_name}', ''))\n",
    "        returns_corr = returns_corr.rename(columns=lambda c: c + ' Correlation')\n",
    "        if isinstance(correlations, list):\n",
    "            correlation_names = [c + ' Correlation' for c  in correlations]\n",
    "            not_in_returns_corr = [c for c in correlation_names if c not in returns_corr.columns]\n",
    "            if len(not_in_returns_corr) > 0:\n",
    "                not_in_returns_corr = \", \".join([c.replace(' Correlation', '') for c in not_in_returns_corr])\n",
    "                raise Exception(f'{not_in_returns_corr} not in returns columns')\n",
    "            returns_corr = returns_corr[[c + ' Correlation' for c  in correlations]]\n",
    "        summary_statistics = summary_statistics.join(returns_corr)\n",
    "    \n",
    "    return filter_columns_and_indexes(\n",
    "        summary_statistics,\n",
    "        keep_columns=keep_columns,\n",
    "        drop_columns=drop_columns,\n",
    "        keep_indexes=keep_indexes,\n",
    "        drop_indexes=drop_indexes,\n",
    "        drop_before_keep=drop_before_keep\n",
    "    )\n",
    "\n",
    "def calc_correlations(\n",
    "    returns: pd.DataFrame,\n",
    "    print_highest_lowest: bool = True,\n",
    "    matrix_size: Union[int, float] = 7,\n",
    "    return_heatmap: bool = True,\n",
    "    keep_columns: Union[list, str] = None,\n",
    "    drop_columns: Union[list, str] = None,\n",
    "    keep_indexes: Union[list, str] = None,\n",
    "    drop_indexes: Union[list, str] = None,\n",
    "    drop_before_keep: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the correlation matrix of the provided returns and optionally prints or visualizes it.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame): Time series of returns.\n",
    "    print_highest_lowest (bool, default=True): If True, prints the highest and lowest correlations.\n",
    "    matrix_size (int or float, default=7): Size of the heatmap for correlation matrix visualization.\n",
    "    return_heatmap (bool, default=True): If True, returns a heatmap of the correlation matrix.\n",
    "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
    "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
    "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
    "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
    "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
    "\n",
    "    Returns:\n",
    "    sns.heatmap or pd.DataFrame: Heatmap of the correlation matrix or the correlation matrix itself.\n",
    "    \"\"\"\n",
    "    returns = returns.copy()\n",
    "\n",
    "    if 'date' in returns.columns.str.lower():\n",
    "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
    "        returns = returns.set_index('date')\n",
    "    returns.index.name = 'date'\n",
    "\n",
    "    correlation_matrix = returns.corr()\n",
    "    if return_heatmap:\n",
    "        fig, ax = plt.subplots(figsize=(matrix_size * 1.5, matrix_size))\n",
    "        heatmap = sns.heatmap(\n",
    "            correlation_matrix, \n",
    "            xticklabels=correlation_matrix.columns,\n",
    "            yticklabels=correlation_matrix.columns,\n",
    "            annot=True,\n",
    "        )\n",
    "\n",
    "    if print_highest_lowest:\n",
    "        highest_lowest_corr = (\n",
    "            correlation_matrix\n",
    "            .unstack()\n",
    "            .sort_values()\n",
    "            .reset_index()\n",
    "            .set_axis(['asset_1', 'asset_2', 'corr'], axis=1)\n",
    "            .loc[lambda df: df.asset_1 != df.asset_2]\n",
    "        )\n",
    "        highest_corr = highest_lowest_corr.iloc[lambda df: len(df)-1, :]\n",
    "        lowest_corr = highest_lowest_corr.iloc[0, :]\n",
    "        print(f'The highest correlation ({highest_corr[\"corr\"]:.2%}) is between {highest_corr.asset_1} and {highest_corr.asset_2}')\n",
    "        print(f'The lowest correlation ({lowest_corr[\"corr\"]:.2%}) is between {lowest_corr.asset_1} and {lowest_corr.asset_2}')\n",
    "    \n",
    "    if return_heatmap:\n",
    "        return heatmap\n",
    "    else:\n",
    "        return filter_columns_and_indexes(\n",
    "            correlation_matrix,\n",
    "            keep_columns=keep_columns,\n",
    "            drop_columns=drop_columns,\n",
    "            keep_indexes=keep_indexes,\n",
    "            drop_indexes=drop_indexes,\n",
    "            drop_before_keep=drop_before_keep\n",
    "        )\n",
    "\n",
    "def calc_cummulative_returns(\n",
    "    returns: Union[pd.DataFrame, pd.Series],\n",
    "    return_plot: bool = True,\n",
    "    fig_size: tuple = (7, 5),\n",
    "    return_series: bool = False,\n",
    "    name: str = None,\n",
    "    timeframes: Union[None, dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates cumulative returns from a time series of returns.\n",
    "\n",
    "    Parameters:\n",
    "    returns (pd.DataFrame or pd.Series): Time series of returns.\n",
    "    return_plot (bool, default=True): If True, plots the cumulative returns.\n",
    "    fig_size (tuple, default=(7, 5)): Size of the plot for cumulative returns.\n",
    "    return_series (bool, default=False): If True, returns the cumulative returns as a DataFrame.\n",
    "    name (str, default=None): Name for the title of the plot or the cumulative return series.\n",
    "    timeframes (dict or None, default=None): Dictionary of timeframes to calculate cumulative returns for each period.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame or None: Returns cumulative returns DataFrame if `return_series` is True.\n",
    "    \"\"\"\n",
    "    if timeframes is not None:\n",
    "        for name, timeframe in timeframes.items():\n",
    "            if timeframe[0] and timeframe[1]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
    "            elif timeframe[0]:\n",
    "                timeframe_returns = returns.loc[timeframe[0]:]\n",
    "            elif timeframe[1]:\n",
    "                timeframe_returns = returns.loc[:timeframe[1]]\n",
    "            else:\n",
    "                timeframe_returns = returns.copy()\n",
    "            if len(timeframe_returns.index) == 0:\n",
    "                raise Exception(f'No returns for {name} timeframe')\n",
    "            calc_cummulative_returns(\n",
    "                timeframe_returns,\n",
    "                return_plot=True,\n",
    "                fig_size=fig_size,\n",
    "                return_series=False,\n",
    "                name=name,\n",
    "                timeframes=None\n",
    "            )\n",
    "        return\n",
    "    returns = returns.copy()\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.to_frame()\n",
    "    returns = returns.apply(lambda x: x.astype(float))\n",
    "    returns = returns.apply(lambda x: x + 1)\n",
    "    returns = returns.cumprod()\n",
    "    returns = returns.apply(lambda x: x - 1)\n",
    "    title = f'Cummulative Returns {name}' if name else 'Cummulative Returns'\n",
    "    if return_plot:\n",
    "        returns.plot(\n",
    "            title=title,\n",
    "            figsize=fig_size,\n",
    "            grid=True,\n",
    "            xlabel='Date',\n",
    "            ylabel='Cummulative Returns'\n",
    "        )\n",
    "    if return_series:\n",
    "        return returns\n",
    "\n",
    "\n",
    "#usage to get the cols \n",
    "'''\n",
    "calc_summary_statistics(\n",
    "    assets_excess_returns[['TIP', 'BWX', 'IEF']],\n",
    "    annual_factor=12,\n",
    "    provided_excess_returns=True,\n",
    "    keep_columns=[\n",
    "        \"Annualized Mean\", \"Annualized Vol\", \"Annualized Sharpe\",\n",
    "        \"Max Drawdown\", \"Peak\", \"Bottom\", \"Correlation\",\n",
    "        \"Annualized Historical VaR\", \"Kurtosis\", \"Skewness\"\n",
    "    ]\n",
    ").transpose()\n",
    "'''\n",
    "\n",
    "#Correlations\n",
    "'''\n",
    "calc_correlations(assets_excess_returns)\n",
    "'''\n",
    "\n",
    "# Cum returns \n",
    "'''\n",
    "calc_cummulative_returns(assets_excess_returns[['TIP', 'BWX', 'IEF']])\n",
    "'''\n",
    "\n",
    "# Sharpe V tangency weights\n",
    "\n",
    "'''\n",
    "analysis_sharpe_vs_tangent_weights = (\n",
    "    calc_summary_statistics(\n",
    "        assets_excess_returns,\n",
    "        annual_factor=12,\n",
    "        provided_excess_returns=True,\n",
    "        keep_columns=['Tangency Weights', 'Annualized Sharpe']\n",
    "    )\n",
    "    .sort_values('Annualized Sharpe', ascending=False)\n",
    ")\n",
    "analysis_sharpe_vs_tangent_weights\n",
    "'''\n",
    "\n",
    "#Tangency weights\n",
    "'''\n",
    "calc_tangency_weights(assets_excess_returns, return_graphic=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "## Allocations - with a mu month target \n",
    "\n",
    "\n",
    "# Equal weights - 1/n\n",
    "\n",
    "'''\n",
    "n_assets = len(assets_excess_returns.columns)\n",
    "MU_MONTH_TARGET = 0.010\n",
    "\n",
    "portfolio_equal_weights_not_scaled = create_portfolio(\n",
    "    assets_excess_returns,\n",
    "    weights=[1 / n_assets for _ in range(n_assets)],\n",
    "    port_name=\"Equal Weights\"\n",
    ")\n",
    "portfolio_equal_weights = portfolio_equal_weights_not_scaled * MU_MONTH_TARGET / portfolio_equal_weights_not_scaled.mean()\n",
    "portfolio_equal_weights\n",
    "'''\n",
    "\n",
    "\n",
    "#Risk parity - 1/sigma^2\n",
    "\n",
    "'''\n",
    "asset_variance_dict = assets_excess_returns.std().map(lambda x: x ** 2).to_dict()\n",
    "asset_inv_variance_dict = {asset: 1 / variance for asset, variance in asset_variance_dict.items()}\n",
    "portfolio_risk_parity_not_scaled = create_portfolio(\n",
    "    assets_excess_returns,\n",
    "    weights=asset_inv_variance_dict,\n",
    "    port_name=\"Risk Parity\"\n",
    ")\n",
    "portfolio_risk_parity = portfolio_risk_parity_not_scaled * MU_MONTH_TARGET / portfolio_risk_parity_not_scaled.mean()\n",
    "portfolio_risk_parity\n",
    "'''\n",
    "\n",
    "# Regularised - sigma + diag(sigma) / 2 \n",
    "'''\n",
    "portfolio_regularized_not_scaled = calc_tangency_weights(assets_excess_returns, return_port_ret=True, cov_mat=.5, name=\"Regularized\")\n",
    "portfolio_regularized = portfolio_regularized_not_scaled * MU_MONTH_TARGET / portfolio_regularized_not_scaled.mean()\n",
    "portfolio_regularized\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e70ecf",
   "metadata": {},
   "source": [
    "* HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5299e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ny = hf_series[['HFRIFWI Index']]\\nx = merrill_factors\\n\\nreplication_model_no_int = sm.OLS(y,x).fit()\\n\\n# No-intercept model\\nstats_OLS(replication_model_no_int,y,x).round(2)\\n\\nround(replication_model_no_int.predict(x).mean(),4)\\nround(replication_model.predict(sm.add_constant(x)).mean(),4)\\n\\nprint('Correlation of no intercept model to HFRIFWI: ', np.corrcoef(y['HFRIFWI Index'], replication_model_no_int.predict(x))[0][1])\\nprint('Correlation of intercept model to HFRIFWI: ', np.corrcoef(y['HFRIFWI Index'], replication_model.predict(sm.add_constant(x)))[0][1])\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stats(data, portfolio = None, portfolio_name = 'Portfolio', annualize = True):\n",
    "    \n",
    "    if portfolio is None:\n",
    "        returns = data\n",
    "    else:\n",
    "        returns = data @ portfolio\n",
    "    \n",
    "    output = returns.agg(['mean','std'])\n",
    "    output.loc['sharpe'] = output.loc['mean'] / output.loc['std']\n",
    "    \n",
    "    if annualize == True:\n",
    "        output.loc['mean'] *= 12\n",
    "        output.loc['std'] *= np.sqrt(12)\n",
    "        output.loc['sharpe'] *= np.sqrt(12)\n",
    "    \n",
    "    if portfolio is None:\n",
    "        pass\n",
    "    else:\n",
    "        output.columns = [portfolio_name]\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create function to calculate max drawdown and associated dates\n",
    "\n",
    "def max_drawdown(data, portfolio = None, portfolio_name = 'Portfolio'):\n",
    "    \n",
    "    if portfolio is None:\n",
    "        returns = data\n",
    "        output = pd.DataFrame(columns=returns.columns)\n",
    "    else:\n",
    "        returns = data @ portfolio\n",
    "        output = pd.DataFrame(columns=[portfolio_name])\n",
    "    \n",
    "    cumulative = (returns + 1).cumprod()\n",
    "    maximum = cumulative.expanding().max()\n",
    "    drawdown = cumulative / maximum - 1\n",
    "    \n",
    "    for col in output.columns:\n",
    "        \n",
    "        output.loc['MDD',col] = drawdown[col].min()\n",
    "        output.loc['Max Date',col] = cumulative[cumulative.index < drawdown[col].idxmin()][col]\\\n",
    "                                             .idxmax()\\\n",
    "                                             .date()\n",
    "        output.loc['Min Date',col] = drawdown[col].idxmin().date()\n",
    "        recovery_date = drawdown.loc[drawdown[col].idxmin():,col]\\\n",
    "                                             .apply(lambda x: 0 if x == 0 else np.nan)\\\n",
    "                                             .idxmax()\n",
    "        \n",
    "        if pd.isna(recovery_date):\n",
    "            output.loc['Recovery Date',col] = recovery_date\n",
    "            output.loc['Recovery Period',col] = np.nan\n",
    "        else:\n",
    "            output.loc['Recovery Date',col] = recovery_date.date()\n",
    "            output.loc['Recovery Period',col] = (output.loc['Recovery Date',col]\\\n",
    "                                             - output.loc['Min Date',col])\\\n",
    "                                             .days\n",
    "        \n",
    "    return output\n",
    "\n",
    "# Create function to retrieve other statistics\n",
    "\n",
    "def stats_tail_risk(data, portfolio = None, portfolio_name = 'Portfolio', VaR = 0.05):\n",
    "    \n",
    "    if portfolio is None:\n",
    "        returns = data\n",
    "    else:\n",
    "        returns = data @ portfolio\n",
    "    \n",
    "    output = returns.agg(['skew',\n",
    "                          'kurt'])\n",
    "    output.loc['VaR'] = returns.quantile(q = 0.05)\n",
    "    output.loc['CVaR'] = returns[returns <= output.loc['VaR']].mean()\n",
    "    output = pd.concat([output, max_drawdown(returns,portfolio,portfolio_name)])\n",
    "    \n",
    "    if portfolio is None:\n",
    "        pass\n",
    "    else:\n",
    "        output.columns = portfolio_name\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create function to display regression stats\n",
    "\n",
    "def stats_OLS(model,y,x):\n",
    "    \n",
    "    output = model.params.to_frame(name = y.columns[0])\n",
    "    \n",
    "    return output\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "def plot_max_drawdown(data, portfolio=None, portfolio_name='Portfolio', ax=None, show_legend=True):\n",
    "    \"\"\"\n",
    "    Plot drawdowns (cumulative peak-to-trough) and annotate the max drawdown window.\n",
    "    Works with a returns DataFrame/Series, or with a weight vector via `portfolio`.\n",
    "    \"\"\"\n",
    "    # Resolve returns shape and column names\n",
    "    if portfolio is None:\n",
    "        returns = data.copy()\n",
    "        if isinstance(returns, pd.Series):\n",
    "            returns = returns.to_frame(name=returns.name or portfolio_name)\n",
    "    else:\n",
    "        # matrix @ vector -> Series\n",
    "        returns = (data @ portfolio).to_frame(name=portfolio_name)\n",
    "\n",
    "    # Compute drawdowns\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.cummax()\n",
    "    drawdown = cumulative / running_max - 1.0\n",
    "\n",
    "    # Prep axes\n",
    "    created_ax = False\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        created_ax = True\n",
    "\n",
    "    # Plot each series' drawdown\n",
    "    for col in drawdown.columns:\n",
    "        dd = drawdown[col]\n",
    "        ax.plot(dd.index, dd.values, label=col, linewidth=1.5)\n",
    "\n",
    "        # Identify MDD stats for annotation/shading\n",
    "        min_date = dd.idxmin()\n",
    "        mdd_val = dd.loc[min_date]\n",
    "\n",
    "        # Peak date = last running max before min (highest cumulative up to min_date)\n",
    "        peak_date = cumulative.loc[:min_date, col].idxmax()\n",
    "\n",
    "        # Recovery date = first time drawdown returns to 0 after min_date (if ever)\n",
    "        post = dd.loc[min_date:]\n",
    "        recovery_candidates = post[post >= -1e-12]\n",
    "        recovery_date = recovery_candidates.index[0] if len(recovery_candidates) else None\n",
    "\n",
    "        # Shade peak -> recovery (or to series end if unrecovered)\n",
    "        shade_end = recovery_date if recovery_date is not None else dd.index[-1]\n",
    "        ax.axvspan(peak_date, shade_end, alpha=0.12)\n",
    "\n",
    "        # Annotate the trough\n",
    "        ax.scatter([min_date], [mdd_val], zorder=3)\n",
    "        ax.annotate(\n",
    "            f\"MDD: {mdd_val:.2%}\\n{min_date.date()}\",\n",
    "            xy=(min_date, mdd_val),\n",
    "            xytext=(10, 10),\n",
    "            textcoords=\"offset points\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", alpha=0.7),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=1),\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Drawdown (Peak  Trough  Recovery)\")\n",
    "    ax.set_ylabel(\"Drawdown\")\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    if show_legend and drawdown.shape[1] > 1:\n",
    "        ax.legend(frameon=False)\n",
    "    if created_ax:\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return drawdown\n",
    "\n",
    "# Stats - mean std, sharpe\n",
    "'''\n",
    "stats(hf_series) # assumes annualised\n",
    "'''\n",
    "\n",
    "# Stats with the tail risk\n",
    "'''\n",
    "stats_tail_risk(hf_series) # assumes annualised \n",
    "'''\n",
    "\n",
    "\n",
    "# For generating market beta, treynor and IR\n",
    "'''\n",
    "x = sm.add_constant(merrill_factors['SPY US Equity'])\n",
    "\n",
    "regression_stats = pd.DataFrame(index = ['$B^{SPY}$','Treynor','IR'],columns = hf_series.columns)\n",
    "\n",
    "for hf in hf_series.columns:\n",
    "    \n",
    "    y = hf_series[hf]\n",
    "    \n",
    "    model = sm.OLS(y,x).fit()\n",
    "    \n",
    "    beta = model.params.iloc[1]\n",
    "    treynor = 12 * y.mean() / beta\n",
    "    ir = np.sqrt(12) * model.params.iloc[0] / model.resid.std()\n",
    "    \n",
    "    regression_stats[hf] = pd.Series([beta,treynor,ir],index = ['$B^{SPY}$','Treynor','IR'])\n",
    "\n",
    "regression_stats.round(3)\n",
    "'''\n",
    "\n",
    "\n",
    "# OLS stats\n",
    "'''\n",
    "y = hf_series[['HFRIFWI Index']]\n",
    "x = sm.add_constant(merrill_factors)\n",
    "\n",
    "replication_model = sm.OLS(y,x).fit()\n",
    "\n",
    "replication_stats = stats_OLS(replication_model,y,x).round(3)\n",
    "replication_stats\n",
    "\n",
    "print(f'R-Squared: {round(replication_model.rsquared,2)}')\n",
    "\n",
    "\n",
    "print(f'The volatility of the tracking error is {round(replication_model.resid.std()*np.sqrt(12),4)}')\n",
    "\n",
    "'''\n",
    "\n",
    "# OOS OLS \n",
    "'''\n",
    "t = 60\n",
    "n = len(hf_series['HFRIFWI Index'])\n",
    "\n",
    "data = hf_series[['HFRIFWI Index']].copy()\n",
    "data['Replication'] = np.nan\n",
    "\n",
    "for i in range(t, n):\n",
    "    \n",
    "    y = hf_series['HFRIFWI Index'].iloc[i - 60:i]\n",
    "    x = sm.add_constant(merrill_factors).iloc[i - 60:i]\n",
    "    \n",
    "    m = sm.OLS(y,x).fit()\n",
    "    \n",
    "    oos_val = sm.add_constant(merrill_factors).iloc[i].to_numpy().reshape((7))\n",
    "\n",
    "    rep_val = m.predict(oos_val)\n",
    "    \n",
    "    data['Replication'].iloc[i] = rep_val\n",
    "\n",
    "    \n",
    "# We can simplify by doing the following:\n",
    "\n",
    "x = sm.add_constant(merrill_factors)\n",
    "y = hf_series['HFRIFWI Index']\n",
    "\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "rolling = RollingOLS(y,x,window=60).fit()\n",
    "rolling_betas = rolling.params\n",
    "replication_rolling = (rolling_betas.shift() * x).dropna().sum(axis=1)\n",
    "\n",
    "\n",
    "oos_loss = (data.dropna().diff(axis=1)**2)['Replication'].sum()\n",
    "oos_mean = data.dropna()['HFRIFWI Index'].mean()\n",
    "oos_loss_null = ((data.dropna()['HFRIFWI Index'] - oos_mean)**2).sum()\n",
    "\n",
    "oos_r2 = 1 - oos_loss / oos_loss_null\n",
    "\n",
    "print(f'The OOS R-Squared of the replication is {round(oos_r2,4)}')\n",
    "data.corr().round(3)\n",
    "\n",
    "'''\n",
    "\n",
    "#OLS no int \n",
    "'''\n",
    "\n",
    "y = hf_series[['HFRIFWI Index']]\n",
    "x = merrill_factors\n",
    "\n",
    "replication_model_no_int = sm.OLS(y,x).fit()\n",
    "\n",
    "# No-intercept model\n",
    "stats_OLS(replication_model_no_int,y,x).round(2)\n",
    "\n",
    "round(replication_model_no_int.predict(x).mean(),4)\n",
    "round(replication_model.predict(sm.add_constant(x)).mean(),4)\n",
    "\n",
    "print('Correlation of no intercept model to HFRIFWI: ', np.corrcoef(y['HFRIFWI Index'], replication_model_no_int.predict(x))[0][1])\n",
    "print('Correlation of intercept model to HFRIFWI: ', np.corrcoef(y['HFRIFWI Index'], replication_model.predict(sm.add_constant(x)))[0][1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b3907",
   "metadata": {},
   "source": [
    "* HW3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c6d3540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nmu = (\\n    res_stats[(\"1965 - 1999\", \"mean\")].loc[(\"logs\", \"SPX\")]\\n    - res_stats[(\"2000 - 2024\", \"mean\")].loc[(\"logs\", \"SPX\")]\\n)\\nsigma = res_stats[(\"1965 - 1999\", \"vol\")].loc[(\"logs\", \"SPX\")]\\n\\nprint(\\n    f\"Probability of underperformance in 2000-2024: {prob_underperformance(mu, sigma, 24):,.2%}\"\\n)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log returns \n",
    "'''\n",
    "def calc_analytics_by_year(data, agg_years):\n",
    "    res = []\n",
    "    for y in agg_years:\n",
    "        sub = data.loc[str(y[0]) : str(y[1])]\n",
    "        res.append({\"mean\": sub.mean() * 12, \"vol\": sub.std() * np.sqrt(12)})\n",
    "    return pd.DataFrame(res, index=[f\"{i[0]} - {i[1]}\" for i in agg_years]).stack()\n",
    "\n",
    "\n",
    "df = pd.read_excel(\n",
    "    \"barnstable_analysis_data.xlsx\", sheet_name=\"data\", parse_dates=True, index_col=0\n",
    ")\n",
    "df[\"excess_returns\"] = df[\"SPX\"] - df[\"TB1M\"]\n",
    "df_subsample = df.loc[\"1965\":\"1999\"]\n",
    "\n",
    "\n",
    "agg_years = [(1965, 1999), (2000, 2024), (1926, 2024)]\n",
    "\n",
    "sum_stats = df.apply(calc_analytics_by_year, agg_years=agg_years).T\n",
    "log_sum_stats = np.log(1 + df).apply(calc_analytics_by_year, agg_years=agg_years).T\n",
    "\n",
    "res_stats = pd.concat([sum_stats, log_sum_stats])\n",
    "res_stats.index = pd.MultiIndex.from_product(\n",
    "    [[\"levels\", \"logs\"], sum_stats.index.to_list()]\n",
    ")\n",
    "res_stats.style.format(\"{:,.2%}\")\n",
    "'''\n",
    "\n",
    "\n",
    "#prob underperformance \n",
    "\n",
    "'''\n",
    "def prob_underperformance(mu, sigma, h):\n",
    "    return norm.cdf(np.sqrt(h) * (-mu / sigma))\n",
    "\n",
    "\n",
    "mu, sigma = res_stats[\"1965 - 1999\"].loc[(\"logs\", \"excess_returns\")]\n",
    "\n",
    "print(\n",
    "    f\"SPX underperforming risk-free rate in the 15 years after 1999: {prob_underperformance(mu=mu, sigma=sigma, h=15):,.2%}\"\n",
    ")\n",
    "print(\n",
    "    f\"SPX underperforming risk-free rate in the 30 years after 1999: {prob_underperformance(mu=mu, sigma=sigma, h=30):,.2%}\"\n",
    ")\n",
    "\n",
    "h = np.linspace(0, 30, 300)\n",
    "\n",
    "probabilities = prob_underperformance(mu=mu, sigma=sigma, h=h)\n",
    "\n",
    "\n",
    "plt.plot(h, probabilities)\n",
    "plt.title(\n",
    "    \"Probability of SPX Underperforming Risk-Free Rate\\nas a Function of Investment Horizon\"\n",
    ")\n",
    "plt.xlabel(\"Investment Horizon (Years)\")\n",
    "plt.ylabel(\"Probability of Underperformance\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Full sample \n",
    "\n",
    "''' \n",
    "# 1965-2024 period\n",
    "mu = np.log(1 + df.loc[\"1965\":\"2023\"]).mean() * 12\n",
    "sigma = np.log(1 + df.loc[\"1965\":\"2023\"]).std() * np.sqrt(12)\n",
    "h = np.arange(30) + 1\n",
    "\n",
    "mu_excess = mu[\"excess_returns\"]\n",
    "sigma_excess = sigma[\"excess_returns\"]\n",
    "\n",
    "probabilities_full_sample = prob_underperformance(mu=mu_excess, sigma=sigma_excess, h=h)\n",
    "print(\n",
    "    f\"SPX underperforming risk-free rate in the 30 years after 2024: {prob_underperformance(mu=mu_excess, sigma=sigma_excess, h=30):,.2%}\"\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "#in sample of out of sample likelihood\n",
    "\n",
    "''' \n",
    "mu = (\n",
    "    res_stats[(\"1965 - 1999\", \"mean\")].loc[(\"logs\", \"SPX\")]\n",
    "    - res_stats[(\"2000 - 2024\", \"mean\")].loc[(\"logs\", \"SPX\")]\n",
    ")\n",
    "sigma = res_stats[(\"1965 - 1999\", \"vol\")].loc[(\"logs\", \"SPX\")]\n",
    "\n",
    "print(\n",
    "    f\"Probability of underperformance in 2000-2024: {prob_underperformance(mu, sigma, 24):,.2%}\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ad128",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db47005",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6780e7",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba520aec",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf4c3e",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d5e87",
   "metadata": {},
   "source": [
    "# 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5ec53",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd5489",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889bb75",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810bc415",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bc4fb",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20dc5a",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712b6e6",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2804dca",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c62462",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f4070",
   "metadata": {},
   "source": [
    "# 3.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
