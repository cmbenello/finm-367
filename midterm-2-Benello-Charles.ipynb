{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "843444ef65feb825",
      "metadata": {},
      "source": [
        "# Open Midterm 2\n",
        "\n",
        "## FINM 36700 - 2025\n",
        "\n",
        "### UChicago Financial Mathematics\n",
        "\n",
        "* Charles Benello\n",
        "* cmbenello@uchicago.edu\n",
        "* Studendid - 12248247\n",
        "\n",
        "i use chatgpt/codex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "997eb0226a65b7d9",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6837d39e",
      "metadata": {},
      "source": [
        "## Scoring\n",
        "\n",
        "| Problem | Points |\n",
        "|---------|--------|\n",
        "| 1       | 30     |\n",
        "| 2       | 20     |\n",
        "| 2       | 20     |\n",
        "| 2       | 30     |\n",
        "\n",
        "**Numbered problems are worth 5pts unless specified otherwise.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79af3374",
      "metadata": {},
      "source": [
        "## Submission\n",
        "\n",
        "You should submit a **single** Jupyter notebook (`.ipynb`) file containing all of your code and answers to Canvas. \n",
        "\n",
        "Note: If any other files are required to run your notebook, please include them **and only them** in a single `.zip` file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cc7c3ccc4e8996d",
      "metadata": {},
      "source": [
        "## Data\n",
        "\n",
        "**All data files are found in at the course web-book.**\n",
        "\n",
        "https://markhendricks.github.io/finm-portfolio/.\n",
        "\n",
        "The exam uses the data found in `commodity_factors.xlsx`\n",
        "* sheet `factors`\n",
        "* sheet `returns`\n",
        "\n",
        "Both tabs contain **daily** returns for a set of commodity futures from `January 2010` to `October 2025`.\n",
        "* approximate 252 observations per year for purposes of annualization\n",
        "\n",
        "Factors are\n",
        "* **LVL**: A level factor of commodity data\n",
        "* **HMS**: Hard Minus Soft commodities\n",
        "* **IMO**: Input Minus Output commodities\n",
        "\n",
        "Returns are\n",
        "* various commodity futures across energy, metals, livestock, and agriculture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cc245ce4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1c8bcd15",
      "metadata": {},
      "outputs": [],
      "source": [
        "factors = pd.read_excel('./data/commodity_factors.xlsx', sheet_name='factors', index_col=0, parse_dates=True)\n",
        "factors = pd.read_excel('data/commodity_factors.xlsx', sheet_name='factors', index_col=0, parse_dates=True)\n",
        "factors = factors[[\"LVL\", \"HMS\", \"IMO\"]]\n",
        "returns = pd.read_excel('./data/commodity_factors.xlsx', sheet_name='returns', index_col=0, parse_dates=True)\n",
        "returns = pd.read_excel('data/commodity_factors.xlsx', sheet_name='returns', index_col=0, parse_dates=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5667cc3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "##helper functions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "pd.options.display.float_format = \"{:,.4f}\".format\n",
        "from typing import Union, List\n",
        "from pandas import Timestamp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import re\n",
        "\n",
        "def read_excel_default(excel_name: str, index_col : int = 0, parse_dates: bool =True, print_sheets: bool = False, sheet_name: str = None, **kwargs):\n",
        "    \"\"\"\n",
        "    Reads an Excel file and returns a DataFrame with specified options.\n",
        "\n",
        "    Parameters:\n",
        "    excel_name (str): The path to the Excel file.\n",
        "    index_col (int, default=0): Column to use as the row labels of the DataFrame.\n",
        "    parse_dates (bool, default=True): Boolean to parse dates.\n",
        "    print_sheets (bool, default=False): If True, prints the names and first few rows of all sheets.\n",
        "    sheet_name (str or int, default=None): Name or index of the sheet to read. If None, reads the first sheet.\n",
        "    **kwargs: Additional arguments passed to `pd.read_excel`.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame containing the data from the specified Excel sheet.\n",
        "\n",
        "    Notes:\n",
        "    - If `print_sheets` is True, the function will print the names and first few rows of all sheets and return None.\n",
        "    - The function ensures that the index name is set to 'date' if the index column name is 'date' or 'dates', or if the index contains date-like values.\n",
        "    \"\"\"\n",
        "    if print_sheets:\n",
        "        n = 0\n",
        "        while True:\n",
        "            try:\n",
        "                sheet = pd.read_excel(excel_name, sheet_name=n)\n",
        "                print(f'Sheet {n}:')\n",
        "                print(\", \".join(list(sheet.columns)))\n",
        "                print(sheet.head(3))\n",
        "                n += 1\n",
        "                print('\\n' * 2)\n",
        "            except:\n",
        "                return\n",
        "    sheet_name = 0 if sheet_name is None else sheet_name\n",
        "    returns = pd.read_excel(excel_name, index_col=index_col, parse_dates=parse_dates,  sheet_name=sheet_name, **kwargs)\n",
        "    if returns.index.name is not None:\n",
        "        if returns.index.name.lower() in ['date', 'dates']:\n",
        "            returns.index.name = 'date'\n",
        "    elif isinstance(returns.index[0], (datetime.date, datetime.datetime)):\n",
        "        returns.index.name = 'date'\n",
        "    return returns\n",
        "\n",
        "\n",
        "def calc_cummulative_returns(\n",
        "    returns: Union[pd.DataFrame, pd.Series],\n",
        "    return_plot: bool = True,\n",
        "    fig_size: tuple = (7, 5),\n",
        "    return_series: bool = False,\n",
        "    name: str = None,\n",
        "    timeframes: Union[None, dict] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates cumulative returns from a time series of returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame or pd.Series): Time series of returns.\n",
        "    return_plot (bool, default=True): If True, plots the cumulative returns.\n",
        "    fig_size (tuple, default=(7, 5)): Size of the plot for cumulative returns.\n",
        "    return_series (bool, default=False): If True, returns the cumulative returns as a DataFrame.\n",
        "    name (str, default=None): Name for the title of the plot or the cumulative return series.\n",
        "    timeframes (dict or None, default=None): Dictionary of timeframes to calculate cumulative returns for each period.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or None: Returns cumulative returns DataFrame if `return_series` is True.\n",
        "    \"\"\"\n",
        "    if timeframes is not None:\n",
        "        for name, timeframe in timeframes.items():\n",
        "            if timeframe[0] and timeframe[1]:\n",
        "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
        "            elif timeframe[0]:\n",
        "                timeframe_returns = returns.loc[timeframe[0]:]\n",
        "            elif timeframe[1]:\n",
        "                timeframe_returns = returns.loc[:timeframe[1]]\n",
        "            else:\n",
        "                timeframe_returns = returns.copy()\n",
        "            if len(timeframe_returns.index) == 0:\n",
        "                raise Exception(f'No returns for {name} timeframe')\n",
        "            calc_cummulative_returns(\n",
        "                timeframe_returns,\n",
        "                return_plot=True,\n",
        "                fig_size=fig_size,\n",
        "                return_series=False,\n",
        "                name=name,\n",
        "                timeframes=None\n",
        "            )\n",
        "        return\n",
        "    returns = returns.copy()\n",
        "    if isinstance(returns, pd.Series):\n",
        "        returns = returns.to_frame()\n",
        "    returns = returns.apply(lambda x: x.astype(float))\n",
        "    returns = returns.apply(lambda x: x + 1)\n",
        "    returns = returns.cumprod()\n",
        "    returns = returns.apply(lambda x: x - 1)\n",
        "    title = f'Cummulative Returns {name}' if name else 'Cummulative Returns'\n",
        "    if return_plot:\n",
        "        returns.plot(\n",
        "            title=title,\n",
        "            figsize=fig_size,\n",
        "            grid=True,\n",
        "            xlabel='Date',\n",
        "            ylabel='Cummulative Returns'\n",
        "        )\n",
        "    if return_series:\n",
        "        return returns\n",
        "\n",
        "\n",
        "def calc_summary_statistics(\n",
        "    returns: Union[pd.DataFrame, List],\n",
        "    annual_factor: int = None,\n",
        "    provided_excess_returns: bool = None,\n",
        "    rf: Union[pd.Series, pd.DataFrame] = None,\n",
        "    var_quantile: Union[float, List] = .05,\n",
        "    timeframes: Union[None, dict] = None,\n",
        "    return_tangency_weights: bool = True,\n",
        "    correlations: Union[bool, List] = True,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False,\n",
        "    _timeframe_name: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates summary statistics for a time series of returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame or List): Time series of returns.\n",
        "    annual_factor (int, default=None): Factor for annualizing returns.\n",
        "    provided_excess_returns (bool, default=None): Whether excess returns are already provided.\n",
        "    rf (pd.Series or pd.DataFrame, default=None): Risk-free rate data.\n",
        "    var_quantile (float or list, default=0.05): Quantile for Value at Risk (VaR) calculation.\n",
        "    timeframes (dict or None, default=None): Dictionary of timeframes to calculate statistics for each period.\n",
        "    return_tangency_weights (bool, default=True): If True, returns tangency portfolio weights.\n",
        "    correlations (bool or list, default=True): If True, returns correlations, or specify columns for correlations.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary statistics of the returns.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "    if isinstance(rf, (pd.Series, pd.DataFrame)):\n",
        "        rf = rf.copy()\n",
        "        if provided_excess_returns is True:\n",
        "            raise Exception(\n",
        "                'rf is provided but excess returns were provided as well.'\n",
        "                'Remove \"rf\" or set \"provided_excess_returns\" to None or False'\n",
        "            )\n",
        "        \n",
        "    if isinstance(returns, list):\n",
        "        returns_list = returns[:]\n",
        "        returns = pd.DataFrame({})\n",
        "        for series in returns_list:\n",
        "            returns = returns.merge(series, right_index=True, left_index=True, how='outer')\n",
        "    \"\"\"\n",
        "    This functions returns the summary statistics for the input total/excess returns passed\n",
        "    into the function\n",
        "    \"\"\"\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    try:\n",
        "        returns.index = pd.to_datetime(returns.index.map(lambda x: x.date()))\n",
        "    except AttributeError:\n",
        "        print('Could not convert \"date\" index to datetime.date')\n",
        "        pass\n",
        "\n",
        "    returns = returns.apply(lambda x: x.astype(float))\n",
        "\n",
        "    if annual_factor is None:\n",
        "        print('Assuming monthly returns with annualization term of 12')\n",
        "        annual_factor = 12\n",
        "\n",
        "    if provided_excess_returns is None:\n",
        "        print(\n",
        "            'Assuming excess returns were provided to calculate Sharpe.'\n",
        "            ' If returns were provided (steady of excess returns), the column \"Sharpe\" is actually \"Mean/Volatility\"'\n",
        "        )\n",
        "        provided_excess_returns = True\n",
        "    elif provided_excess_returns is False:\n",
        "        if rf is not None:\n",
        "            if len(rf.index) != len(returns.index):\n",
        "                raise Exception('\"rf\" index must be the same lenght as \"returns\"')\n",
        "            print('\"rf\" is used to subtract returns to calculate Sharpe, but nothing else')\n",
        "\n",
        "    if isinstance(timeframes, dict):\n",
        "        all_timeframes_summary_statistics = pd.DataFrame({})\n",
        "        for name, timeframe in timeframes.items():\n",
        "            if timeframe[0] and timeframe[1]:\n",
        "                timeframe_returns = returns.loc[timeframe[0]:timeframe[1]]\n",
        "            elif timeframe[0]:\n",
        "                timeframe_returns = returns.loc[timeframe[0]:]\n",
        "            elif timeframe[1]:\n",
        "                timeframe_returns = returns.loc[:timeframe[1]]\n",
        "            else:\n",
        "                timeframe_returns = returns.copy()\n",
        "            if len(timeframe_returns.index) == 0:\n",
        "                raise Exception(f'No returns for {name} timeframe')\n",
        "            timeframe_returns = timeframe_returns.rename(columns=lambda c: c + f' {name}')\n",
        "            timeframe_summary_statistics = calc_summary_statistics(\n",
        "                returns=timeframe_returns,\n",
        "                annual_factor=annual_factor,\n",
        "                provided_excess_returns=provided_excess_returns,\n",
        "                rf=rf,\n",
        "                var_quantile=var_quantile,\n",
        "                timeframes=None,\n",
        "                correlations=correlations,\n",
        "                _timeframe_name=name,\n",
        "                keep_columns=keep_columns,\n",
        "                drop_columns=drop_columns,\n",
        "                keep_indexes=keep_indexes,\n",
        "                drop_indexes=drop_indexes,\n",
        "                drop_before_keep=drop_before_keep\n",
        "            )\n",
        "            all_timeframes_summary_statistics = pd.concat(\n",
        "                [all_timeframes_summary_statistics, timeframe_summary_statistics],\n",
        "                axis=0\n",
        "            )\n",
        "        return all_timeframes_summary_statistics\n",
        "\n",
        "    summary_statistics = pd.DataFrame(index=returns.columns)\n",
        "    summary_statistics['Mean'] = returns.mean()\n",
        "    summary_statistics['Annualized Mean'] = returns.mean() * annual_factor\n",
        "    summary_statistics['Vol'] = returns.std()\n",
        "    summary_statistics['Annualized Vol'] = returns.std() * np.sqrt(annual_factor)\n",
        "    try:\n",
        "        if not provided_excess_returns:\n",
        "            if type(rf) == pd.DataFrame:\n",
        "                rf = rf.iloc[:, 0].to_list()\n",
        "            elif type(rf) == pd.Series:\n",
        "                rf = rf.to_list()\n",
        "            else:\n",
        "                raise Exception('\"rf\" must be either a pd.DataFrame or pd.Series')\n",
        "            excess_returns = returns.apply(lambda x: x - rf)\n",
        "            summary_statistics['Sharpe'] = excess_returns.mean() / returns.std()\n",
        "        else:\n",
        "            summary_statistics['Sharpe'] = returns.mean() / returns.std()\n",
        "    except Exception as e:\n",
        "        print(f'Could not calculate Sharpe: {e}')\n",
        "    summary_statistics['Annualized Sharpe'] = summary_statistics['Sharpe'] * np.sqrt(annual_factor)\n",
        "    summary_statistics['Min'] = returns.min()\n",
        "    summary_statistics['Max'] = returns.max()\n",
        "    summary_statistics['Skewness'] = returns.skew()\n",
        "    summary_statistics['Excess Kurtosis'] = returns.kurtosis()\n",
        "    var_quantile = [var_quantile] if isinstance(var_quantile, (float, int)) else var_quantile\n",
        "    for var_q in var_quantile:\n",
        "        summary_statistics[f'Historical VaR ({var_q:.2%})'] = returns.quantile(var_q, axis = 0)\n",
        "        summary_statistics[f'Annualized Historical VaR ({var_q:.2%})'] = returns.quantile(var_q, axis = 0) * np.sqrt(annual_factor)\n",
        "        summary_statistics[f'Historical CVaR ({var_q:.2%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean()\n",
        "        summary_statistics[f'Annualized Historical CVaR ({var_q:.2%})'] = returns[returns <= returns.quantile(var_q, axis = 0)].mean() * np.sqrt(annual_factor)\n",
        "    \n",
        "    wealth_index = 1000 * (1 + returns).cumprod()\n",
        "    previous_peaks = wealth_index.cummax()\n",
        "    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
        "\n",
        "    summary_statistics['Max Drawdown'] = drawdowns.min()\n",
        "    summary_statistics['Peak'] = [previous_peaks[col][:drawdowns[col].idxmin()].idxmax() for col in previous_peaks.columns]\n",
        "    summary_statistics['Bottom'] = drawdowns.idxmin()\n",
        "\n",
        "    if return_tangency_weights:\n",
        "        tangency_weights = calc_tangency_weights(returns)\n",
        "        summary_statistics = summary_statistics.join(tangency_weights)\n",
        "    \n",
        "    recovery_date = []\n",
        "    for col in wealth_index.columns:\n",
        "        prev_max = previous_peaks[col][:drawdowns[col].idxmin()].max()\n",
        "        recovery_wealth = pd.DataFrame([wealth_index[col][drawdowns[col].idxmin():]]).T\n",
        "        recovery_date.append(recovery_wealth[recovery_wealth[col] >= prev_max].index.min())\n",
        "    summary_statistics['Recovery'] = recovery_date\n",
        "    try:\n",
        "        summary_statistics[\"Duration (days)\"] = [\n",
        "            (i - j).days if i != \"-\" else \"-\" for i, j in\n",
        "            zip(summary_statistics[\"Recovery\"], summary_statistics[\"Bottom\"])\n",
        "        ]\n",
        "    except (AttributeError, TypeError) as e:\n",
        "        print(f'Cannot calculate \"Drawdown Duration\" calculation because there was no recovery or because index are not dates: {str(e)}')\n",
        "\n",
        "    if correlations is True or isinstance(correlations, list):\n",
        "        returns_corr = returns.corr()\n",
        "        if _timeframe_name:\n",
        "            returns_corr = returns_corr.rename(columns=lambda c: c.replace(f' {_timeframe_name}', ''))\n",
        "        returns_corr = returns_corr.rename(columns=lambda c: c + ' Correlation')\n",
        "        if isinstance(correlations, list):\n",
        "            correlation_names = [c + ' Correlation' for c  in correlations]\n",
        "            not_in_returns_corr = [c for c in correlation_names if c not in returns_corr.columns]\n",
        "            if len(not_in_returns_corr) > 0:\n",
        "                not_in_returns_corr = \", \".join([c.replace(' Correlation', '') for c in not_in_returns_corr])\n",
        "                raise Exception(f'{not_in_returns_corr} not in returns columns')\n",
        "            returns_corr = returns_corr[[c + ' Correlation' for c  in correlations]]\n",
        "        summary_statistics = summary_statistics.join(returns_corr)\n",
        "    \n",
        "    return filter_columns_and_indexes(\n",
        "        summary_statistics,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_negative_pct(\n",
        "    returns: Union[pd.DataFrame, pd.Series, list],\n",
        "    calc_positive: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of negative or positive returns in the provided data.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame, pd.Series, or list): Time series of returns.\n",
        "    calc_positive (bool, default=False): If True, calculates the percentage of positive returns.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame with the percentage of negative or positive returns, number of returns, and the count of negative/positive returns.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "    if isinstance(returns, list):\n",
        "        returns_list = returns[:]\n",
        "        returns = pd.DataFrame({})\n",
        "        for series in returns_list:\n",
        "            returns = returns.merge(series, right_index=True, left_index=True, how='outer')\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "        \n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    if isinstance(returns, pd.Series):\n",
        "        returns = returns.to_frame()\n",
        "    returns = returns.apply(lambda x: x.astype(float))\n",
        "    prev_len_index = returns.apply(lambda x: len(x))\n",
        "    returns  =returns.dropna(axis=0)\n",
        "    new_len_index = returns.apply(lambda x: len(x))\n",
        "    if not (prev_len_index == new_len_index).all():\n",
        "        print('Some columns had NaN values and were dropped')\n",
        "    if calc_positive:\n",
        "        returns = returns.applymap(lambda x: 1 if x > 0 else 0)\n",
        "    else:\n",
        "        returns = returns.applymap(lambda x: 1 if x < 0 else 0)\n",
        "\n",
        "    negative_statistics = (\n",
        "        returns\n",
        "        .agg(['mean', 'count', 'sum'])\n",
        "        .set_axis(['% Negative Returns', 'Nº Returns', 'Nº Negative Returns'], axis=0)\n",
        "    )\n",
        "\n",
        "    if calc_positive:\n",
        "        negative_statistics = negative_statistics.rename(lambda i: i.replace('Negative', 'Positive'), axis=0)\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        negative_statistics,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def filter_columns_and_indexes(\n",
        "    df: pd.DataFrame,\n",
        "    keep_columns: Union[list, str],\n",
        "    drop_columns: Union[list, str],\n",
        "    keep_indexes: Union[list, str],\n",
        "    drop_indexes: Union[list, str],\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Filters a DataFrame based on specified columns and indexes.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame to be filtered.\n",
        "    keep_columns (list or str): Columns to keep in the DataFrame.\n",
        "    drop_columns (list or str): Columns to drop from the DataFrame.\n",
        "    keep_indexes (list or str): Indexes to keep in the DataFrame.\n",
        "    drop_indexes (list or str): Indexes to drop from the DataFrame.\n",
        "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, (pd.DataFrame, pd.Series)):\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    # Columns\n",
        "    if keep_columns is not None:\n",
        "        keep_columns = \"(?i)\" + \"|\".join(keep_columns) if isinstance(keep_columns, list) else \"(?i)\" + keep_columns\n",
        "    else:\n",
        "        keep_columns = None\n",
        "    if drop_columns is not None:\n",
        "        drop_columns = \"(?i)\" + \"|\".join(drop_columns) if isinstance(drop_columns, list) else \"(?i)\" + drop_columns\n",
        "    else:\n",
        "        drop_columns = None\n",
        "    if not drop_before_keep:\n",
        "        if keep_columns is not None:\n",
        "            df = df.filter(regex=keep_columns)\n",
        "    if drop_columns is not None:\n",
        "        df = df.drop(columns=df.filter(regex=drop_columns).columns)\n",
        "    if drop_before_keep:\n",
        "        if keep_columns is not None:\n",
        "            df = df.filter(regex=keep_columns)\n",
        "    # Indexes\n",
        "    if keep_indexes is not None:\n",
        "        keep_indexes = \"(?i)\" + \"|\".join(keep_indexes) if isinstance(keep_indexes, list) else \"(?i)\" + keep_indexes\n",
        "    else:\n",
        "        keep_indexes = None\n",
        "    if drop_indexes is not None:\n",
        "        drop_indexes = \"(?i)\" + \"|\".join(drop_indexes) if isinstance(drop_indexes, list) else \"(?i)\" + drop_indexes\n",
        "    else:\n",
        "        drop_indexes = None\n",
        "    if not drop_before_keep:\n",
        "        if keep_indexes is not None:\n",
        "            df = df.filter(regex=keep_indexes, axis=0)\n",
        "    if drop_indexes is not None:\n",
        "        df = df.drop(index=df.filter(regex=drop_indexes, axis=0).index)\n",
        "    if drop_before_keep:\n",
        "        if keep_indexes is not None:\n",
        "            df = df.filter(regex=keep_indexes, axis=0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def calc_cross_section_regression(\n",
        "    returns: Union[pd.DataFrame, List],\n",
        "    factors: Union[pd.DataFrame, List],\n",
        "    annual_factor: int = None,\n",
        "    provided_excess_returns: bool = None,\n",
        "    rf: pd.Series = None,\n",
        "    return_model: bool = False,\n",
        "    name: str = None,\n",
        "    return_mae: bool = True,\n",
        "    intercept_cross_section: bool = True,\n",
        "    return_historical_premium: bool = True,\n",
        "    return_annualized_premium: bool = True,\n",
        "    compare_premiums: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs a cross-sectional regression on the provided returns and factors.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame or list): Time series of returns.\n",
        "    factors (pd.DataFrame or list): Time series of factor data.\n",
        "    annual_factor (int, default=None): Factor for annualizing returns.\n",
        "    provided_excess_returns (bool, default=None): Whether excess returns are already provided.\n",
        "    rf (pd.Series, default=None): Risk-free rate data for subtracting from returns.\n",
        "    return_model (bool, default=False): If True, returns the regression model.\n",
        "    name (str, default=None): Name for labeling the regression.\n",
        "    return_mae (bool, default=True): If True, returns the mean absolute error of the regression.\n",
        "    intercept_cross_section (bool, default=True): If True, includes an intercept in the cross-sectional regression.\n",
        "    return_historical_premium (bool, default=True): If True, returns the historical premium of factors.\n",
        "    return_annualized_premium (bool, default=True): If True, returns the annualized premium of factors.\n",
        "    compare_premiums (bool, default=False): If True, compares the historical and estimated premiums.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or model: Cross-sectional regression output or the model if `return_model` is True.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "    factors = factors.copy()\n",
        "    if isinstance(rf, (pd.Series, pd.DataFrame)):\n",
        "        rf = rf.copy()\n",
        "\n",
        "    if compare_premiums:\n",
        "        return_historical_premium = True\n",
        "        return_annualized_premium = True\n",
        "\n",
        "    if isinstance(returns, list):\n",
        "        returns_list = returns[:]\n",
        "        returns = pd.DataFrame({})\n",
        "        for series in returns_list:\n",
        "            returns = returns.merge(series, right_index=True, left_index=True, how='outer')\n",
        "\n",
        "    if annual_factor is None:\n",
        "        print('Assuming monthly returns with annualization term of 12')\n",
        "        annual_factor = 12\n",
        "\n",
        "    if isinstance(factors, list):\n",
        "        factors_list = returns[:]\n",
        "        factors = pd.DataFrame({})\n",
        "        for series in factors_list:\n",
        "            factors = factors.merge(series, right_index=True, left_index=True, how='outer')\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    if provided_excess_returns is None:\n",
        "        print('Assuming excess returns were provided')\n",
        "        provided_excess_returns = True\n",
        "    elif provided_excess_returns is False:\n",
        "        if rf is not None:\n",
        "            if len(rf.index) != len(returns.index):\n",
        "                raise Exception('\"rf\" index must be the same lenght as \"returns\"')\n",
        "            print('\"rf\" is used to subtract returns')\n",
        "            returns = returns.sub(rf, axis=0)\n",
        "    \n",
        "    time_series_regressions = calc_iterative_regression(returns, factors, annual_factor=annual_factor, warnings=False)\n",
        "    time_series_betas = time_series_regressions.filter(regex='Beta$', axis=1)\n",
        "    time_series_historical_returns = time_series_regressions[['Fitted Mean']]\n",
        "    cross_section_regression = calc_regression(\n",
        "        time_series_historical_returns, time_series_betas,\n",
        "        annual_factor=annual_factor, intercept=intercept_cross_section,\n",
        "        return_model=return_model, warnings=False\n",
        "    )\n",
        "\n",
        "    if return_model:\n",
        "        return cross_section_regression\n",
        "    cross_section_regression = cross_section_regression.rename(columns=lambda c: c.replace(' Beta Beta', ' Lambda').replace('Alpha', 'Eta'))\n",
        "    if name is None:\n",
        "        name = \" + \".join([c.replace(' Lambda', '') for c in cross_section_regression.filter(regex=' Lambda$', axis=1).columns])\n",
        "    cross_section_regression.index = [f'{name} Cross-Section Regression']\n",
        "    cross_section_regression.drop([\n",
        "        'Information Ratio', 'Annualized Information Ratio', 'Tracking Error', 'Annualized Tracking Error', 'Fitted Mean', 'Annualized Fitted Mean'\n",
        "    ], axis=1, inplace=True)\n",
        "    if return_annualized_premium:\n",
        "        factors_annualized_premium = (\n",
        "            cross_section_regression\n",
        "            .filter(regex=' Lambda$', axis=1)\n",
        "            .apply(lambda x: x * annual_factor)\n",
        "            .rename(columns=lambda c: c.replace(' Lambda', ' Annualized Lambda'))\n",
        "        )\n",
        "        cross_section_regression = cross_section_regression.join(factors_annualized_premium)\n",
        "\n",
        "    if return_historical_premium:\n",
        "        print('Lambda represents the premium calculated by the cross-section regression and the historical premium is the average of the factor excess returns')\n",
        "        factors_historical_premium = factors.mean().to_frame(f'{name} Cross-Section Regression').transpose().rename(columns=lambda c: c + ' Historical Premium')\n",
        "        cross_section_regression = cross_section_regression.join(factors_historical_premium)\n",
        "        if return_annualized_premium:\n",
        "            factors_annualized_historical_premium = (\n",
        "                factors_historical_premium\n",
        "                .apply(lambda x: x * annual_factor)\n",
        "                .rename(columns=lambda c: c.replace(' Historical Premium', ' Annualized Historical Premium'))\n",
        "            )\n",
        "            cross_section_regression = cross_section_regression.join(factors_annualized_historical_premium)\n",
        "\n",
        "    if compare_premiums:\n",
        "        cross_section_regression = cross_section_regression.filter(regex='Lambda$|Historical Premium$', axis=1)\n",
        "        cross_section_regression = cross_section_regression.transpose()\n",
        "        cross_section_regression['Factor'] = cross_section_regression.index.str.extract(f'({\"|\".join(list(factors.columns))})').values\n",
        "        cross_section_regression['Premium Type'] = cross_section_regression.index.str.replace(f'({\"|\".join(list(factors.columns))})', '')\n",
        "        premiums_comparison = cross_section_regression.pivot(index='Factor', columns='Premium Type', values=f'{name} Cross-Section Regression')\n",
        "        premiums_comparison.columns.name = None\n",
        "        premiums_comparison.index.name = None\n",
        "        premiums_comparison.join(calc_tangency_weights(factors))\n",
        "        premiums_comparison = premiums_comparison.join(factors.corr().rename(columns=lambda c: c + ' Correlation'))\n",
        "        return filter_columns_and_indexes(\n",
        "            premiums_comparison,\n",
        "            keep_columns=keep_columns,\n",
        "            drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes,\n",
        "            drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    \n",
        "    if return_mae:\n",
        "        cross_section_regression['TS MAE'] = time_series_regressions['Alpha'].abs().mean()\n",
        "        cross_section_regression['TS Annualized MAE'] = time_series_regressions['Annualized Alpha'].abs().mean()\n",
        "        cross_section_regression_model = calc_regression(\n",
        "            time_series_historical_returns, time_series_betas,\n",
        "            annual_factor=annual_factor, intercept=intercept_cross_section,\n",
        "            return_model=True, warnings=False\n",
        "        )\n",
        "        cross_section_regression['CS MAE'] = cross_section_regression_model.resid.abs().mean()\n",
        "        cross_section_regression['CS Annualized MAE'] = cross_section_regression['CS MAE'] * annual_factor\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        cross_section_regression,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def get_best_and_worst(\n",
        "    summary_statistics: pd.DataFrame,\n",
        "    stat: str = 'Annualized Sharpe',\n",
        "    return_df: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Identifies the best and worst assets based on a specified statistic.\n",
        "\n",
        "    Parameters:\n",
        "    summary_statistics (pd.DataFrame): DataFrame containing summary statistics.\n",
        "    stat (str, default='Annualized Sharpe'): The statistic to compare assets by.\n",
        "    return_df (bool, default=True): If True, returns a DataFrame with the best and worst assets.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or None: DataFrame with the best and worst assets if `return_df` is True.\n",
        "    \"\"\"\n",
        "    summary_statistics = summary_statistics.copy()\n",
        "\n",
        "    if len(summary_statistics.index) < 2:\n",
        "        raise Exception('\"summary_statistics\" must have at least two lines in order to do comparison')\n",
        "\n",
        "    if stat not in summary_statistics.columns:\n",
        "        raise Exception(f'{stat} not in \"summary_statistics\"')\n",
        "    summary_statistics.rename(columns=lambda c: c.replace(' ', '').lower())\n",
        "    best_stat = summary_statistics[stat].max()\n",
        "    worst_stat = summary_statistics[stat].min()\n",
        "    asset_best_stat = summary_statistics.loc[lambda df: df[stat] == df[stat].max()].index[0]\n",
        "    asset_worst_stat = summary_statistics.loc[lambda df: df[stat] == df[stat].min()].index[0]\n",
        "    print(f'The asset with the highest {stat} is {asset_best_stat}: {best_stat:.5f}')\n",
        "    print(f'The asset with the lowest {stat} is {asset_worst_stat}: {worst_stat:.5f}')\n",
        "    if return_df:\n",
        "        return pd.concat([\n",
        "            summary_statistics.loc[lambda df: df.index == asset_best_stat],\n",
        "            summary_statistics.loc[lambda df: df.index == asset_worst_stat]\n",
        "        ])\n",
        "    \n",
        "\n",
        "def calc_correlations(\n",
        "    returns: pd.DataFrame,\n",
        "    print_highest_lowest: bool = True,\n",
        "    matrix_size: Union[int, float] = 7,\n",
        "    return_heatmap: bool = True,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the correlation matrix of the provided returns and optionally prints or visualizes it.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of returns.\n",
        "    print_highest_lowest (bool, default=True): If True, prints the highest and lowest correlations.\n",
        "    matrix_size (int or float, default=7): Size of the heatmap for correlation matrix visualization.\n",
        "    return_heatmap (bool, default=True): If True, returns a heatmap of the correlation matrix.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): Whether to drop specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    sns.heatmap or pd.DataFrame: Heatmap of the correlation matrix or the correlation matrix itself.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    correlation_matrix = returns.corr()\n",
        "    if return_heatmap:\n",
        "        fig, ax = plt.subplots(figsize=(matrix_size * 1.5, matrix_size))\n",
        "        heatmap = sns.heatmap(\n",
        "            correlation_matrix, \n",
        "            xticklabels=correlation_matrix.columns,\n",
        "            yticklabels=correlation_matrix.columns,\n",
        "            annot=True,\n",
        "        )\n",
        "\n",
        "    if print_highest_lowest:\n",
        "        highest_lowest_corr = (\n",
        "            correlation_matrix\n",
        "            .unstack()\n",
        "            .sort_values()\n",
        "            .reset_index()\n",
        "            .set_axis(['asset_1', 'asset_2', 'corr'], axis=1)\n",
        "            .loc[lambda df: df.asset_1 != df.asset_2]\n",
        "        )\n",
        "        highest_corr = highest_lowest_corr.iloc[lambda df: len(df)-1, :]\n",
        "        lowest_corr = highest_lowest_corr.iloc[0, :]\n",
        "        print(f'The highest correlation ({highest_corr[\"corr\"]:.2%}) is between {highest_corr.asset_1} and {highest_corr.asset_2}')\n",
        "        print(f'The lowest correlation ({lowest_corr[\"corr\"]:.2%}) is between {lowest_corr.asset_1} and {lowest_corr.asset_2}')\n",
        "    \n",
        "    if return_heatmap:\n",
        "        return heatmap\n",
        "    else:\n",
        "        return filter_columns_and_indexes(\n",
        "            correlation_matrix,\n",
        "            keep_columns=keep_columns,\n",
        "            drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes,\n",
        "            drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    \n",
        "\n",
        "def calc_tangency_weights(\n",
        "    returns: pd.DataFrame,\n",
        "    cov_mat: str = 1,\n",
        "    return_graphic: bool = False,\n",
        "    return_port_ret: bool = False,\n",
        "    target_ret_rescale_weights: Union[None, float] = None,\n",
        "    annual_factor: int = 12,\n",
        "    name: str = 'Tangency',\n",
        "    expected_returns: Union[pd.Series, pd.DataFrame] = None,\n",
        "    expected_returns_already_annualized: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates tangency portfolio weights based on the covariance matrix of returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of returns.\n",
        "    cov_mat (str, default=1): Covariance matrix for calculating tangency weights.\n",
        "    return_graphic (bool, default=False): If True, plots the tangency weights.\n",
        "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
        "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
        "    annual_factor (int, default=12): Factor for annualizing returns.\n",
        "    name (str, default='Tangency'): Name for labeling the weights and portfolio.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or pd.Series: Tangency portfolio weights or portfolio returns if `return_port_ret` is True.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "    \n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    if cov_mat == 1:\n",
        "        cov_inv = np.linalg.inv((returns.cov() * annual_factor))\n",
        "    else:\n",
        "        cov = returns.cov()\n",
        "        covmat_diag = np.diag(np.diag((cov)))\n",
        "        covmat = cov_mat * cov + (1 - cov_mat) * covmat_diag\n",
        "        cov_inv = np.linalg.pinv((covmat * annual_factor))  \n",
        "        \n",
        "    ones = np.ones(returns.columns.shape) \n",
        "    if expected_returns is not None:\n",
        "        mu = expected_returns\n",
        "        if not expected_returns_already_annualized:\n",
        "            mu *= annual_factor\n",
        "    else:\n",
        "        mu = returns.mean() * annual_factor\n",
        "    scaling = 1 / (np.transpose(ones) @ cov_inv @ mu)\n",
        "    tangent_return = scaling * (cov_inv @ mu)\n",
        "    tangency_wts = pd.DataFrame(\n",
        "        index=returns.columns,\n",
        "        data=tangent_return,\n",
        "        columns=[f'{name} Weights']\n",
        "    )\n",
        "    port_returns = returns @ tangency_wts.rename({f'{name} Weights': f'{name} Portfolio'}, axis=1)\n",
        "\n",
        "    if return_graphic:\n",
        "        tangency_wts.plot(kind='bar', title=f'{name} Weights')\n",
        "\n",
        "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
        "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
        "        tangency_wts[[f'{name} Weights']] *= scaler\n",
        "        port_returns *= scaler\n",
        "        tangency_wts = tangency_wts.rename(\n",
        "            {f'{name} Weights': f'{name} Weights Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        port_returns = port_returns.rename(\n",
        "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    if cov_mat != 1:\n",
        "        port_returns = port_returns.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
        "        tangency_wts = tangency_wts.rename(columns=lambda c: c.replace('Tangency', f'Tangency Regularized {cov_mat:.2f}'))\n",
        "        \n",
        "    if return_port_ret:\n",
        "        return port_returns\n",
        "    return tangency_wts\n",
        "\n",
        "\n",
        "def calc_equal_weights(\n",
        "    returns: pd.DataFrame,\n",
        "    return_graphic: bool = False,\n",
        "    return_port_ret: bool = False,\n",
        "    target_ret_rescale_weights: Union[float, None] = None,\n",
        "    name: str = 'Equal Weights'\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates equal weights for the portfolio based on the provided returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of returns.\n",
        "    return_graphic (bool, default=False): If True, plots the equal weights.\n",
        "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
        "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
        "    name (str, default='Equal Weights'): Name for labeling the portfolio.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or pd.Series: Equal portfolio weights or portfolio returns if `return_port_ret` is True.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    equal_wts = pd.DataFrame(\n",
        "        index=returns.columns,\n",
        "        data=[1 / len(returns.columns)] * len(returns.columns),\n",
        "        columns=[f'{name}']\n",
        "    )\n",
        "    port_returns = returns @ equal_wts.rename({f'{name}': f'{name} Portfolio'}, axis=1)\n",
        "\n",
        "    if return_graphic:\n",
        "        equal_wts.plot(kind='bar', title=f'{name}')\n",
        "\n",
        "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
        "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
        "        equal_wts[[f'{name}']] *= scaler\n",
        "        port_returns *= scaler\n",
        "        equal_wts = equal_wts.rename(\n",
        "            {f'{name}': f'{name} Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        port_returns = port_returns.rename(\n",
        "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "    if return_port_ret:\n",
        "        return port_returns\n",
        "    return equal_wts\n",
        "\n",
        "\n",
        "def calc_risk_parity_weights(\n",
        "    returns: pd.DataFrame,\n",
        "    return_graphic: bool = False,\n",
        "    return_port_ret: bool = False,\n",
        "    target_ret_rescale_weights: Union[None, float] = None,\n",
        "    name: str = 'Risk Parity'\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates risk parity portfolio weights based on the variance of each asset.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of returns.\n",
        "    return_graphic (bool, default=False): If True, plots the risk parity weights.\n",
        "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
        "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
        "    name (str, default='Risk Parity'): Name for labeling the portfolio.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or pd.Series: Risk parity portfolio weights or portfolio returns if `return_port_ret` is True.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    risk_parity_wts = pd.DataFrame(\n",
        "        index=returns.columns,\n",
        "        data=[1 / returns[asset].var() for asset in returns.columns],\n",
        "        columns=[f'{name} Weights']\n",
        "    )\n",
        "    port_returns = returns @ risk_parity_wts.rename({f'{name} Weights': f'{name} Portfolio'}, axis=1)\n",
        "\n",
        "    if return_graphic:\n",
        "        risk_parity_wts.plot(kind='bar', title=f'{name} Weights')\n",
        "\n",
        "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
        "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
        "        risk_parity_wts[[f'{name} Weights']] *= scaler\n",
        "        port_returns *= scaler\n",
        "        risk_parity_wts = risk_parity_wts.rename(\n",
        "            {f'{name} Weights': f'{name} Weights Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        port_returns = port_returns.rename(\n",
        "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        \n",
        "    if return_port_ret:\n",
        "        return port_returns\n",
        "    return risk_parity_wts\n",
        "\n",
        "\n",
        "def calc_gmv_weights(\n",
        "    returns: pd.DataFrame,\n",
        "    return_graphic: bool = False,\n",
        "    return_port_ret: bool = False,\n",
        "    target_ret_rescale_weights: Union[float, None] = None,\n",
        "    name: str = 'GMV'\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates Global Minimum Variance (GMV) portfolio weights.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of returns.\n",
        "    return_graphic (bool, default=False): If True, plots the GMV weights.\n",
        "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
        "    target_ret_rescale_weights (float or None, default=None): Target return for rescaling weights.\n",
        "    name (str, default='GMV'): Name for labeling the portfolio.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or pd.Series: GMV portfolio weights or portfolio returns if `return_port_ret` is True.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    ones = np.ones(returns.columns.shape)\n",
        "    cov = returns.cov()\n",
        "    cov_inv = np.linalg.inv(cov)\n",
        "    scaling = 1 / (np.transpose(ones) @ cov_inv @ ones)\n",
        "    gmv_tot = scaling * cov_inv @ ones\n",
        "    gmv_wts = pd.DataFrame(\n",
        "        index=returns.columns,\n",
        "        data=gmv_tot,\n",
        "        columns=[f'{name} Weights']\n",
        "    )\n",
        "    port_returns = returns @ gmv_wts.rename({f'{name} Weights': f'{name} Portfolio'}, axis=1)\n",
        "\n",
        "    if isinstance(target_ret_rescale_weights, (float, int)):\n",
        "        scaler = target_ret_rescale_weights / port_returns[f'{name} Portfolio'].mean()\n",
        "        gmv_wts[[f'{name} Weights']] *= scaler\n",
        "        port_returns *= scaler\n",
        "        gmv_wts = gmv_wts.rename(\n",
        "            {f'{name} Weights': f'{name} Weights Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "        port_returns = port_returns.rename(\n",
        "            {f'{name} Portfolio': f'{name} Portfolio Rescaled Target {target_ret_rescale_weights:.2%}'},\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    if return_graphic:\n",
        "        gmv_wts.plot(kind='bar', title=f'{name} Weights')\n",
        "\n",
        "    if return_port_ret:\n",
        "        return port_returns\n",
        "\n",
        "    return gmv_wts\n",
        "\n",
        "\n",
        "def calc_target_ret_weights(\n",
        "    target_ret: float,\n",
        "    returns: pd.DataFrame,\n",
        "    return_graphic: bool = False,\n",
        "    return_port_ret: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the portfolio weights to achieve a target return by combining Tangency and GMV portfolios.\n",
        "\n",
        "    Parameters:\n",
        "    target_ret (float): Target return for the portfolio.\n",
        "    returns (pd.DataFrame): Time series of asset returns.\n",
        "    return_graphic (bool, default=False): If True, plots the portfolio weights.\n",
        "    return_port_ret (bool, default=False): If True, returns the portfolio returns.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Weights of the Tangency and GMV portfolios, along with the combined target return portfolio.\n",
        "    \"\"\"\n",
        "    returns = returns.copy()\n",
        "    \n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "    \n",
        "    mu_tan = returns.mean() @ calc_tangency_weights(returns, cov_mat = 1)\n",
        "    mu_gmv = returns.mean() @ calc_gmv_weights(returns)\n",
        "    \n",
        "    delta = (target_ret - mu_gmv[0]) / (mu_tan[0] - mu_gmv[0])\n",
        "    mv_weights = (delta * calc_tangency_weights(returns, cov_mat=1)).values + ((1 - delta) * calc_gmv_weights(returns)).values\n",
        "    \n",
        "    mv_weights = pd.DataFrame(\n",
        "        index=returns.columns,\n",
        "        data=mv_weights,\n",
        "        columns=[f'Target {target_ret:.2%} Weights']\n",
        "    )\n",
        "    port_returns = returns @ mv_weights.rename({f'Target {target_ret:.2%} Weights': f'Target {target_ret:.2%} Portfolio'}, axis=1)\n",
        "\n",
        "    if return_graphic:\n",
        "        mv_weights.plot(kind='bar', title=f'Target Return of {target_ret:.2%} Weights')\n",
        "\n",
        "    if return_port_ret:\n",
        "        return port_returns\n",
        "\n",
        "    mv_weights['Tangency Weights'] = calc_tangency_weights(returns, cov_mat=1).values\n",
        "    mv_weights['GMV Weights'] = calc_gmv_weights(returns).values\n",
        "\n",
        "    return mv_weights\n",
        "\n",
        "\n",
        "def calc_regression(\n",
        "    y: Union[pd.DataFrame, pd.Series],\n",
        "    X: Union[pd.DataFrame, pd.Series],\n",
        "    intercept: bool = True,\n",
        "    annual_factor: Union[None, int] = None,\n",
        "    warnings: bool = True,\n",
        "    return_model: bool = False,\n",
        "    return_fitted_values: bool = False,\n",
        "    name_fitted_values: str = None,\n",
        "    calc_treynor_info_ratios: bool = True,\n",
        "    timeframes: Union[None, dict] = None,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False,\n",
        "    calc_sortino_ratio: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs an OLS regression on the provided data with optional intercept, timeframes, and statistical ratios.\n",
        "\n",
        "    Parameters:\n",
        "    y (pd.DataFrame or pd.Series): Dependent variable for the regression.\n",
        "    X (pd.DataFrame or pd.Series): Independent variable(s) for the regression.\n",
        "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
        "    annual_factor (int or None, default=None): Factor for annualizing regression statistics.\n",
        "    warnings (bool, default=True): If True, prints warnings about assumptions.\n",
        "    return_model (bool, default=False): If True, returns the regression model object.\n",
        "    return_fitted_values (bool, default=False): If True, returns the fitted values of the regression.\n",
        "    name_fitted_values (str, default=None): Name for the fitted values column.\n",
        "    calc_treynor_info_ratios (bool, default=True): If True, calculates Treynor and Information ratios.\n",
        "    timeframes (dict or None, default=None): Dictionary of timeframes to run separate regressions for each period.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "    calc_sortino_ratio (bool, default=False): If True, calculates the Sortino ratio.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame or model: Regression summary statistics or the model if `return_model` is True.\n",
        "    \"\"\"\n",
        "    y = y.copy()\n",
        "    X = X.copy()\n",
        "\n",
        "    y_name = y.name if isinstance(y, pd.Series) else y.columns[0]\n",
        "    X_names = \" + \".join(list(X.columns))\n",
        "    X_names = \"Intercept + \" + X_names if intercept else X_names\n",
        "\n",
        "    return_model = return_model if not return_fitted_values else True\n",
        "\n",
        "    if annual_factor is None:\n",
        "        print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
        "        annual_factor = 12\n",
        "    \n",
        "    if 'date' in X.columns.str.lower():\n",
        "        X = X.rename({'Date': 'date'}, axis=1)\n",
        "        X = X.set_index('date')\n",
        "    X.index.name = 'date'\n",
        "    \n",
        "    if warnings:\n",
        "        print('\"calc_regression\" assumes excess returns to calculate Information and Treynor Ratios')\n",
        "    if intercept:\n",
        "        X = sm.add_constant(X)\n",
        "    \n",
        "    y_name = y.name if isinstance(y, pd.Series) else y.columns[0]\n",
        "\n",
        "    if len(X.index) != len(y.index):\n",
        "        print(f'y has lenght {len(y.index)} and X has lenght {len(X.index)}. Joining y and X by index...')\n",
        "        df = y.join(X, how='left')\n",
        "        df = df.dropna()\n",
        "        y = df[y_name]\n",
        "        X = df.drop(y_name, axis=1)\n",
        "        if len(X.index) < 4:\n",
        "            raise Exception('Indexes of y and X do not match and there are less than 4 observations. Cannot calculate regression')\n",
        "\n",
        "    if isinstance(timeframes, dict):\n",
        "        all_timeframes_regressions = pd.DataFrame({})\n",
        "        for name, timeframe in timeframes.items():\n",
        "            if timeframe[0] and timeframe[1]:\n",
        "                timeframe_y = y.loc[timeframe[0]:timeframe[1]]\n",
        "                timeframe_X = X.loc[timeframe[0]:timeframe[1]]\n",
        "            elif timeframe[0]:\n",
        "                timeframe_y = y.loc[timeframe[0]:]\n",
        "                timeframe_X = X.loc[timeframe[0]:]\n",
        "            elif timeframe[1]:\n",
        "                timeframe_y = y.loc[:timeframe[1]]\n",
        "                timeframe_X = X.loc[:timeframe[1]]\n",
        "            else:\n",
        "                timeframe_y = y.copy()\n",
        "                timeframe_X = X.copy()\n",
        "            if len(timeframe_y.index) == 0 or len(timeframe_X.index) == 0:\n",
        "                raise Exception(f'No returns for {name} timeframe')\n",
        "            timeframe_regression = calc_regression(\n",
        "                y=timeframe_y,\n",
        "                X=timeframe_X,\n",
        "                intercept=intercept,\n",
        "                annual_factor=annual_factor,\n",
        "                warnings=False,\n",
        "                return_model=False,\n",
        "                calc_treynor_info_ratios=calc_treynor_info_ratios,\n",
        "                timeframes=None,\n",
        "                keep_columns=keep_columns,\n",
        "                drop_columns=drop_columns,\n",
        "                keep_indexes=keep_indexes,\n",
        "                drop_indexes=drop_indexes,\n",
        "                drop_before_keep=drop_before_keep\n",
        "            )\n",
        "            timeframe_regression.index = [timeframe_regression.index + \" \" + name]\n",
        "            all_timeframes_regressions = pd.concat(\n",
        "                [all_timeframes_regressions, timeframe_regression],\n",
        "                axis=0\n",
        "            )\n",
        "        return all_timeframes_regressions\n",
        "\n",
        "    try:\n",
        "        model = sm.OLS(y, X, missing=\"drop\", hasconst=intercept)\n",
        "    except ValueError:\n",
        "        y = y.reset_index(drop=True)\n",
        "        X = X.reset_index(drop=True)\n",
        "        model = sm.OLS(y, X, missing=\"drop\", hasconst=intercept)\n",
        "        if warnings:\n",
        "            print(f'\"{y_name}\" Required to reset indexes to make regression work. Try passing \"y\" and \"X\" as pd.DataFrame')\n",
        "    results = model.fit()\n",
        "    summary = dict()\n",
        "\n",
        "    if return_model:\n",
        "        if not return_fitted_values:\n",
        "            return results\n",
        "        else:\n",
        "            fitted_values = results.fittedvalues\n",
        "            if name_fitted_values is None:\n",
        "                name_fitted_values = f'{y_name} ~ {X_names}'\n",
        "            fitted_values = fitted_values.to_frame(name_fitted_values)\n",
        "            return fitted_values\n",
        "\n",
        "    inter = results.params[0] if intercept else None\n",
        "    betas = results.params[1:] if intercept else results.params\n",
        "\n",
        "    summary[\"Alpha\"] = inter if inter is not None else '-'\n",
        "    summary[\"Annualized Alpha\"] = inter * annual_factor if inter is not None else '-'\n",
        "    summary[\"R-Squared\"] = results.rsquared\n",
        "\n",
        "    if isinstance(X, pd.Series):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    X_assets = X.columns[1:] if intercept else X.columns\n",
        "    for i, asset_name in enumerate(X_assets):\n",
        "        summary[f\"{asset_name} Beta\"] = betas[i]\n",
        "\n",
        "    if calc_treynor_info_ratios:\n",
        "        if len([c for c in X.columns if c != 'const']) == 1:\n",
        "            summary[\"Treynor Ratio\"] = (y.mean() / betas[0])\n",
        "            summary[\"Annualized Treynor Ratio\"] = summary[\"Treynor Ratio\"] * annual_factor\n",
        "        summary[\"Information Ratio\"] = (inter / results.resid.std()) if intercept else \"-\"\n",
        "        summary[\"Annualized Information Ratio\"] = summary[\"Information Ratio\"] * np.sqrt(annual_factor) if intercept else \"-\"\n",
        "    summary[\"Tracking Error\"] = results.resid.std()\n",
        "    summary[\"Annualized Tracking Error\"] = results.resid.std() * np.sqrt(annual_factor)\n",
        "    summary['Fitted Mean'] = results.fittedvalues.mean()\n",
        "    summary['Annualized Fitted Mean'] = summary['Fitted Mean'] * annual_factor\n",
        "    if calc_sortino_ratio:\n",
        "        try:\n",
        "            summary['Sortino Ratio'] = summary['Fitted Mean'] / y[y < 0].std()\n",
        "            summary['Annualized Sortino Ratio'] = summary['Sortino Ratio'] * np.sqrt(annual_factor)\n",
        "        except Exception as e:\n",
        "            print(f'Cannot calculate Sortino Ratio: {str(e)}. Set \"calc_sortino_ratio\" to False or review function')\n",
        "    y_name = f\"{y_name} no Intercept\" if not intercept else y_name\n",
        "    return filter_columns_and_indexes(\n",
        "        pd.DataFrame(summary, index=[y_name]),\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_strategy_oos(\n",
        "    y: Union[pd.Series, pd.DataFrame],\n",
        "    X: Union[pd.Series, pd.DataFrame],\n",
        "    intercept: bool = True,\n",
        "    rolling_size: Union[None, int] = 60,\n",
        "    expanding: bool = False,\n",
        "    lag_number: int = 1,\n",
        "    weight_multiplier: float = 100,\n",
        "    weight_min: Union[None, float] = None,\n",
        "    weight_max: Union[None, float] = None,\n",
        "    name: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates an out-of-sample strategy based on rolling or expanding window regression.\n",
        "\n",
        "    Parameters:\n",
        "    y (pd.Series or pd.DataFrame): Dependent variable (strategy returns).\n",
        "    X (pd.Series or pd.DataFrame): Independent variable(s) (predictors).\n",
        "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
        "    rolling_size (int or None, default=60): Size of the rolling window for in-sample fitting.\n",
        "    expanding (bool, default=False): If True, uses an expanding window instead of rolling.\n",
        "    lag_number (int, default=1): Number of lags to apply to the predictors.\n",
        "    weight_multiplier (float, default=100): Multiplier to adjust strategy weights.\n",
        "    weight_min (float or None, default=None): Minimum allowable weight.\n",
        "    weight_max (float or None, default=None): Maximum allowable weight.\n",
        "    name (str, default=None): Name for labeling the strategy returns.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Time series of strategy returns.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    try:\n",
        "        y = y.copy()\n",
        "        X = X.copy()\n",
        "    except:\n",
        "        pass\n",
        "    replication_oos = calc_replication_oos(\n",
        "        y=y,\n",
        "        X=X,\n",
        "        intercept=intercept,\n",
        "        rolling_size=rolling_size,\n",
        "        lag_number=lag_number,\n",
        "        expanding=expanding\n",
        "    )\n",
        "    actual_returns = replication_oos['Actual']\n",
        "    predicted_returns = replication_oos['Prediction']\n",
        "    strategy_weights = predicted_returns * weight_multiplier\n",
        "    weight_min = weight_min if weight_min is not None else strategy_weights.min()\n",
        "    weight_max = weight_max if weight_max is not None else strategy_weights.max()\n",
        "    strategy_weights = strategy_weights.clip(lower=weight_min, upper=weight_max)\n",
        "    strategy_returns = (actual_returns * strategy_weights).to_frame()\n",
        "    if name:\n",
        "        strategy_returns.columns = [name]\n",
        "    else:\n",
        "        strategy_returns.columns = [f'{y.columns[0]} Strategy']\n",
        "    return strategy_returns\n",
        "    \n",
        "\n",
        "def calc_iterative_regression(\n",
        "    multiple_y: Union[pd.DataFrame, pd.Series],\n",
        "    X: Union[pd.DataFrame, pd.Series],\n",
        "    annual_factor: Union[None, int] = 12,\n",
        "    intercept: bool = True,\n",
        "    warnings: bool = True,\n",
        "    calc_treynor_info_ratios: bool = True,\n",
        "    calc_sortino_ratio: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs iterative regression across multiple dependent variables (assets).\n",
        "\n",
        "    Parameters:\n",
        "    multiple_y (pd.DataFrame or pd.Series): Dependent variables for multiple assets.\n",
        "    X (pd.DataFrame or pd.Series): Independent variable(s) (predictors).\n",
        "    annual_factor (int or None, default=12): Factor for annualizing regression statistics.\n",
        "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
        "    warnings (bool, default=True): If True, prints warnings about assumptions.\n",
        "    calc_treynor_info_ratios (bool, default=True): If True, calculates Treynor and Information ratios.\n",
        "    calc_sortino_ratio (bool, default=False): If True, calculates the Sortino ratio.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary statistics for each asset regression.\n",
        "    \"\"\"\n",
        "    multiple_y = multiple_y.copy()\n",
        "    X = X.copy()\n",
        "\n",
        "    if 'date' in multiple_y.columns.str.lower():\n",
        "        multiple_y = multiple_y.rename({'Date': 'date'}, axis=1)\n",
        "        multiple_y = multiple_y.set_index('date')\n",
        "    multiple_y.index.name = 'date'\n",
        "\n",
        "    if 'date' in X.columns.str.lower():\n",
        "        X = X.rename({'Date': 'date'}, axis=1)\n",
        "        X = X.set_index('date')\n",
        "    X.index.name = 'date'\n",
        "\n",
        "    regressions = pd.DataFrame({})\n",
        "    for asset in multiple_y.columns:\n",
        "        y = multiple_y[[asset]]\n",
        "        new_regression = calc_regression(\n",
        "            y, X, annual_factor=annual_factor, intercept=intercept, warnings=warnings,\n",
        "            calc_treynor_info_ratios=calc_treynor_info_ratios,\n",
        "            calc_sortino_ratio=calc_sortino_ratio,\n",
        "        )\n",
        "        warnings = False\n",
        "        regressions = pd.concat([regressions, new_regression], axis=0)\n",
        "    \n",
        "    return filter_columns_and_indexes(\n",
        "        regressions,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_replication_oos(\n",
        "    y: Union[pd.Series, pd.DataFrame],\n",
        "    X: Union[pd.Series, pd.DataFrame],\n",
        "    intercept: bool = True,\n",
        "    rolling_size: Union[None, int] = 60,\n",
        "    expanding: bool = False,\n",
        "    return_r_squared_oos: float = False,\n",
        "    lag_number: int = 1,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs out-of-sample replication of a time series regression with rolling or expanding windows.\n",
        "\n",
        "    Parameters:\n",
        "    y (pd.Series or pd.DataFrame): Dependent variable (actual returns).\n",
        "    X (pd.Series or pd.DataFrame): Independent variable(s) (predictors).\n",
        "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
        "    rolling_size (int or None, default=60): Size of the rolling window for in-sample fitting.\n",
        "    expanding (bool, default=False): If True, uses an expanding window instead of rolling.\n",
        "    return_r_squared_oos (float, default=False): If True, returns the out-of-sample R-squared.\n",
        "    lag_number (int, default=1): Number of lags to apply to the predictors.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary statistics for the out-of-sample replication.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    try:\n",
        "        y = y.copy()\n",
        "        X = X.copy()\n",
        "    except:\n",
        "        pass\n",
        "    if isinstance(X, pd.Series):\n",
        "        X = pd.DataFrame(X)\n",
        "    if 'date' in X.columns.str.lower():\n",
        "        X = X.rename({'Date': 'date'}, axis=1)\n",
        "        X = X.set_index('date')\n",
        "    X.index.name = 'date'\n",
        "\n",
        "    X = X.shift(lag_number)\n",
        "\n",
        "    df = y.join(X, how='inner')\n",
        "    y = df.iloc[:, [0]].copy()\n",
        "    X = df.iloc[:, 1:].copy()\n",
        "\n",
        "    if intercept:\n",
        "        X = sm.add_constant(X)\n",
        "\n",
        "    summary_pred = pd.DataFrame({})\n",
        "\n",
        "    for i, last_is_date in enumerate(y.index):\n",
        "        if i < (rolling_size):\n",
        "            continue\n",
        "        y_full = y.iloc[:i].copy()\n",
        "        if expanding:\n",
        "            y_rolling = y_full.copy()\n",
        "        else:\n",
        "            y_rolling = y_full.iloc[-rolling_size:]\n",
        "        X_full = X.iloc[:i].copy()\n",
        "        if expanding:\n",
        "            X_rolling = X_full.copy()\n",
        "        else:\n",
        "            X_rolling = X_full.iloc[-rolling_size:]\n",
        "\n",
        "        reg = sm.OLS(y_rolling, X_rolling, hasconst=intercept, missing='drop').fit()\n",
        "        y_pred = reg.predict(X.iloc[i, :])\n",
        "        naive_y_pred = y_full.mean()\n",
        "        y_actual = y.iloc[i]\n",
        "        summary_line = (\n",
        "            reg.params\n",
        "            .to_frame()\n",
        "            .transpose()\n",
        "            .rename(columns=lambda c: c.replace('const', 'Alpha') if c == 'const' else c + ' Lag Beta')\n",
        "        )\n",
        "        summary_line['Prediction'] = y_pred[0]\n",
        "        summary_line['Naive Prediction'] = naive_y_pred.squeeze()\n",
        "        summary_line['Actual'] = y_actual.squeeze()\n",
        "        summary_line.index = [y.index[i]]\n",
        "        summary_pred = pd.concat([summary_pred, summary_line], axis=0)\n",
        "\n",
        "    summary_pred['Prediction Error'] = summary_pred['Prediction'] - summary_pred['Actual']\n",
        "    summary_pred['Naive Prediction Error'] = summary_pred['Naive Prediction'] - summary_pred['Actual']\n",
        "\n",
        "    rss = (np.array(summary_pred['Prediction Error']) ** 2).sum()\n",
        "    tss = (np.array(summary_pred['Naive Prediction Error']) ** 2).sum()\n",
        "\n",
        "    oos_rsquared = 1 - rss / tss\n",
        "\n",
        "    if return_r_squared_oos:\n",
        "        return pd.DataFrame(\n",
        "            {'R^Squared OOS': oos_rsquared},\n",
        "            index=[\n",
        "                y.columns[0] + \" ~ \" + \n",
        "                \" + \".join([\n",
        "                    c.replace('const', 'Alpha') if c == 'const' else c + ' Lag Beta' for c in X.columns\n",
        "                ])]\n",
        "        )\n",
        "\n",
        "    print(\"OOS R^Squared: {:.4%}\".format(oos_rsquared))\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        summary_pred,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_replication_oos_not_lagged_features(\n",
        "    y: Union[pd.Series, pd.DataFrame],\n",
        "    X: Union[pd.Series, pd.DataFrame],\n",
        "    intercept: bool = True,\n",
        "    rolling_size: Union[None, int] = 60,\n",
        "    return_r_squared_oos: float = False,\n",
        "    r_squared_time_series: bool = False,\n",
        "    return_parameters: bool = True,\n",
        "    oos: int = 1,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs out-of-sample replication without lagged features.\n",
        "\n",
        "    Parameters:\n",
        "    y (pd.Series or pd.DataFrame): Dependent variable (actual returns).\n",
        "    X (pd.Series or pd.DataFrame): Independent variable(s) (predictors).\n",
        "    intercept (bool, default=True): If True, includes an intercept in the regression.\n",
        "    rolling_size (int or None, default=60): Size of the rolling window for in-sample fitting.\n",
        "    return_r_squared_oos (float, default=False): If True, returns the out-of-sample R-squared.\n",
        "    r_squared_time_series (bool, default=False): If True, calculates time-series R-squared.\n",
        "    return_parameters (bool, default=True): If True, returns regression parameters.\n",
        "    oos (int, default=1): Number of periods for out-of-sample evaluation.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary statistics for the out-of-sample replication.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    try:\n",
        "        y = y.copy()\n",
        "        X = X.copy()\n",
        "    except:\n",
        "        pass\n",
        "    if isinstance(X, pd.Series):\n",
        "        X = pd.DataFrame(X)\n",
        "    if 'date' in X.columns.str.lower():\n",
        "        X = X.rename({'Date': 'date'}, axis=1)\n",
        "        X = X.set_index('date')\n",
        "    X.index.name = 'date'\n",
        "\n",
        "    oos_print = \"In-Sample\" if oos == 0 else f\"{oos}OS\"\n",
        "\n",
        "    summary = defaultdict(list)\n",
        "\n",
        "    if isinstance(y, pd.Series):\n",
        "        y = pd.DataFrame(y)\n",
        "    y_name = y.columns[0]\n",
        "    \n",
        "    for idx in range(rolling_size, len(y.index)+1-oos, 1):\n",
        "        X_rolling = X.iloc[idx-rolling_size:idx].copy()\n",
        "        y_rolling = y.iloc[idx-rolling_size:idx, 0].copy()\n",
        "\n",
        "        y_oos = y.iloc[idx-1+oos, 0].copy()\n",
        "        X_oos = X.iloc[idx-1+oos, :].copy()\n",
        "\n",
        "        if intercept:\n",
        "            X_rolling = sm.add_constant(X_rolling)\n",
        "\n",
        "        try:\n",
        "            regr = sm.OLS(y_rolling, X_rolling, missing=\"drop\", hasconst=intercept).fit()\n",
        "        except ValueError:\n",
        "            y_rolling = y_rolling.reset_index(drop=True)\n",
        "            X_rolling = X_rolling.reset_index(drop=True)\n",
        "            regr = sm.OLS(y_rolling, X_rolling, missing=\"drop\", hasconst=intercept).fit()\n",
        "\n",
        "        for jdx, coeff in enumerate(regr.params.index):\n",
        "            if coeff != 'const':\n",
        "                summary[f\"{coeff} Beta {oos_print}\"].append(regr.params[jdx])\n",
        "            else:\n",
        "                summary[f\"{coeff} {oos_print}\"].append(regr.params[jdx])\n",
        "\n",
        "        if intercept:\n",
        "            y_pred = regr.params[0] + (regr.params[1:] @ X_oos)\n",
        "        else:\n",
        "            y_pred = regr.params @ X_oos\n",
        "\n",
        "        summary[f\"{y_name} Replicated\"].append(y_pred)\n",
        "        summary[f\"{y_name} Actual\"].append(y_oos)\n",
        "\n",
        "    summary = pd.DataFrame(summary, index=X.index[rolling_size-1+oos:])\n",
        "\n",
        "    if r_squared_time_series:\n",
        "        time_series_error = pd.DataFrame({})\n",
        "        for idx in range(rolling_size, len(y.index)+1-oos, 1):\n",
        "            y_rolling = y.iloc[idx-rolling_size:idx, 0].copy()\n",
        "            y_oos = y.iloc[idx-1+oos, 0].copy()\n",
        "            time_series_error.loc[y.index[idx-1+oos], 'Naive Error'] = y_oos - y_rolling.mean()\n",
        "        time_series_error['Model Error'] = summary[f\"{y_name} Actual\"] - summary[f\"{y_name} Replicated\"]\n",
        "        oos_rsquared = (\n",
        "            1 - time_series_error['Model Error'].apply(lambda x: x ** 2).sum()\n",
        "            / time_series_error['Naive Error'].apply(lambda x: x ** 2).sum()\n",
        "        )\n",
        "    else:\n",
        "        oos_rsquared = (\n",
        "            1 - (summary[f\"{y_name} Actual\"] - summary[f\"{y_name} Replicated\"]).var()\n",
        "            / summary[f\"{y_name} Actual\"].var()\n",
        "        )\n",
        "\n",
        "    if return_r_squared_oos:\n",
        "        return oos_rsquared\n",
        "    \n",
        "    if not return_parameters:\n",
        "        summary = summary[[f\"{y_name} Actual\", f\"{y_name} Replicated\"]]\n",
        "\n",
        "    if not intercept:\n",
        "        summary = summary.rename(columns=lambda c: c.replace(' Replicated', f' Replicated no Intercept'))\n",
        "\n",
        "    if not intercept:\n",
        "        print(f\"R^Squared {oos_print} without Intercept: {oos_rsquared:.2%}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"R^Squared {oos_print}: {oos_rsquared:.2%}\")\n",
        "\n",
        "    summary = summary.rename(columns=lambda c: (\n",
        "        c.replace(' Replicated', f' Replicated {oos_print}').replace(' Actual', f' Actual {oos_print}')\n",
        "    ))\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        summary,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def create_portfolio(\n",
        "    returns: pd.DataFrame,\n",
        "    weights: Union[dict, list],\n",
        "    port_name: Union[None, str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a portfolio by applying the specified weights to the asset returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of asset returns.\n",
        "    weights (dict or list): Weights to apply to the returns. If a list is provided, it will be converted into a dictionary.\n",
        "    port_name (str or None, default=None): Name for the portfolio. If None, a name will be generated based on asset weights.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The portfolio returns based on the provided weights.\n",
        "    \"\"\"\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    if isinstance(weights, list):\n",
        "        returns = returns.iloc[:, :len(weights)]\n",
        "        weights = dict(zip(returns.columns, weights))\n",
        "\n",
        "    returns = returns[list(weights.keys())]\n",
        "    port_returns = pd.DataFrame(returns @ list(weights.values()))\n",
        "\n",
        "    if port_name is None:\n",
        "        port_name = \" + \".join([f\"{n} ({w:.2%})\" for n, w in weights.items()])\n",
        "    port_returns.columns = [port_name]\n",
        "    return port_returns\n",
        "\n",
        "\n",
        "def calc_ewma_volatility(\n",
        "        excess_returns: pd.Series,\n",
        "        theta : float = 0.94,\n",
        "        initial_vol : float = .2 / np.sqrt(252)\n",
        "    ) -> pd.Series:\n",
        "    var_t0 = initial_vol ** 2\n",
        "    ewma_var = [var_t0]\n",
        "    for i in range(len(excess_returns.index)):\n",
        "        new_ewma_var = ewma_var[-1] * theta + (excess_returns.iloc[i] ** 2) * (1 - theta)\n",
        "        ewma_var.append(new_ewma_var)\n",
        "    ewma_var.pop(0) # Remove var_t0\n",
        "    ewma_vol = [np.sqrt(v) for v in ewma_var]\n",
        "    return pd.Series(ewma_vol, index=excess_returns.index)\n",
        "\n",
        "\n",
        "def calc_garch_volatility(\n",
        "        excess_returs: pd.Series,\n",
        "        p: int = 1,\n",
        "        q: int = 1\n",
        "    ) -> pd.Series:\n",
        "    model = arch_model(excess_returs, vol='Garch', p=p, q=q)\n",
        "    fitted_model = model.fit(disp='off')\n",
        "    fitted_values = fitted_model.conditional_volatility\n",
        "    return fitted_values\n",
        "\n",
        "\n",
        "def calc_garch_volatility(\n",
        "        excess_returs: pd.Series,\n",
        "        p: int = 1,\n",
        "        q: int = 1\n",
        "    ):\n",
        "    model = arch_model(excess_returs, vol='Garch', p=p, q=q)\n",
        "    fitted_model = model.fit(disp='off')\n",
        "    fitted_values = fitted_model.conditional_volatility\n",
        "    return pd.Series(fitted_values, index=excess_returs.index)\n",
        "\n",
        "\n",
        "def calc_var_cvar_summary(\n",
        "    returns: Union[pd.Series, pd.DataFrame],\n",
        "    quantile: Union[None, float] = .05,\n",
        "    window: Union[None, str] = None,\n",
        "    return_hit_ratio: bool = False,\n",
        "    filter_first_hit_ratio_date: Union[None, str, datetime.date] = None,\n",
        "    return_stats: Union[str, list] = ['Returns', 'VaR', 'CVaR', 'Vol'],\n",
        "    full_time_sample: bool = False,\n",
        "    z_score: float = None,\n",
        "    shift: int = 1,\n",
        "    normal_vol_formula: bool = False,\n",
        "    ewma_theta : float = .94,\n",
        "    ewma_initial_vol : float = .2 / np.sqrt(252),\n",
        "    garch_p: int = 1,\n",
        "    garch_q: int = 1,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates a summary of VaR (Value at Risk) and CVaR (Conditional VaR) for the provided returns.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.Series or pd.DataFrame): Time series of returns.\n",
        "    quantile (float or None, default=0.05): Quantile to calculate the VaR and CVaR.\n",
        "    window (str or None, default=None): Window size for rolling calculations.\n",
        "    return_hit_ratio (bool, default=False): If True, returns the hit ratio for the VaR.\n",
        "    return_stats (str or list, default=['Returns', 'VaR', 'CVaR', 'Vol']): Statistics to return in the summary.\n",
        "    full_time_sample (bool, default=False): If True, calculates using the full time sample.\n",
        "    z_score (float, default=None): Z-score for parametric VaR calculation.\n",
        "    shift (int, default=1): Period shift for VaR/CVaR calculations.\n",
        "    normal_vol_formula (bool, default=False): If True, uses the normal volatility formula.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary of VaR and CVaR statistics.\n",
        "    \"\"\"\n",
        "    if window is None:\n",
        "        print('Using \"window\" of 60 periods, since none was specified')\n",
        "        window = 60\n",
        "    if isinstance(returns, pd.DataFrame):\n",
        "        returns_series = returns.iloc[:, 0]\n",
        "        returns_series.index = returns.index\n",
        "        returns = returns_series.copy()\n",
        "\n",
        "    summary = pd.DataFrame({})\n",
        "\n",
        "    # Returns\n",
        "    summary[f'Returns'] = returns\n",
        "\n",
        "    # VaR\n",
        "    summary[f'Expanding {window:.0f} Historical VaR ({quantile:.2%})'] = returns.expanding(min_periods=window).quantile(quantile)\n",
        "    summary[f'Rolling {window:.0f} Historical VaR ({quantile:.2%})'] = returns.rolling(window=window).quantile(quantile)\n",
        "    if normal_vol_formula:\n",
        "        summary[f'Expanding {window:.0f} Volatility'] = returns.expanding(window).std()\n",
        "        summary[f'Rolling {window:.0f} Volatility'] = returns.rolling(window).std()\n",
        "    else:\n",
        "        summary[f'Expanding {window:.0f} Volatility'] = np.sqrt((returns ** 2).expanding(window).mean())\n",
        "        summary[f'Rolling {window:.0f} Volatility'] = np.sqrt((returns ** 2).rolling(window).mean())\n",
        "    summary[f'EWMA {ewma_theta:.2f} Volatility'] = calc_ewma_volatility(returns, theta=ewma_theta, initial_vol=ewma_initial_vol)\n",
        "    summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility'] = calc_garch_volatility(returns, p=garch_p, q=garch_q)\n",
        "\n",
        "    z_score = norm.ppf(quantile) if z_score is None else z_score\n",
        "    summary[f'Expanding {window:.0f} Parametric VaR ({quantile:.2%})'] = summary[f'Expanding {window:.0f} Volatility'] * z_score\n",
        "    summary[f'Rolling {window:.0f} Parametric VaR ({quantile:.2%})'] = summary[f'Rolling {window:.0f} Volatility'] * z_score\n",
        "    summary[f'EWMA {ewma_theta:.2f} Parametric VaR ({quantile:.2%})'] = summary[f'EWMA {ewma_theta:.2f} Volatility'] * z_score\n",
        "    summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametric VaR ({quantile:.2%})'] = summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility'] * z_score\n",
        "\n",
        "    if return_hit_ratio:\n",
        "        shift_stats = [\n",
        "            f'Expanding {window:.0f} Historical VaR ({quantile:.2%})',\n",
        "            f'Rolling {window:.0f} Historical VaR ({quantile:.2%})',\n",
        "            f'Expanding {window:.0f} Parametric VaR ({quantile:.2%})',\n",
        "            f'Rolling {window:.0f} Parametric VaR ({quantile:.2%})',\n",
        "            f'EWMA {ewma_theta:.2f} Parametric VaR ({quantile:.2%})',\n",
        "            f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametric VaR ({quantile:.2%})'\n",
        "        ]\n",
        "        summary_shift = summary.copy()\n",
        "        summary_shift[shift_stats] = summary_shift[shift_stats].shift()\n",
        "        if filter_first_hit_ratio_date:\n",
        "            if isinstance(filter_first_hit_ratio_date, (datetime.date, datetime.datetime)):\n",
        "                filter_first_hit_ratio_date = filter_first_hit_ratio_date.strftime(\"%Y-%m-%d\")\n",
        "            summary_shift = summary_shift.loc[filter_first_hit_ratio_date:]\n",
        "        summary_shift = summary_shift.dropna(axis=0)\n",
        "        summary_shift[shift_stats] = summary_shift[shift_stats].apply(lambda x: (x - summary_shift['Returns']) > 0)\n",
        "        hit_ratio = pd.DataFrame(summary_shift[shift_stats].mean(), columns=['Hit Ratio'])\n",
        "        hit_ratio['Hit Ratio Error'] = (hit_ratio['Hit Ratio'] - quantile) / quantile\n",
        "        hit_ratio['Hit Ratio Absolute Error'] = abs(hit_ratio['Hit Ratio Error'])\n",
        "        hit_ratio = hit_ratio.sort_values('Hit Ratio Absolute Error')\n",
        "        return filter_columns_and_indexes(\n",
        "            hit_ratio,\n",
        "            keep_columns=keep_columns,\n",
        "            drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes,\n",
        "            drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "\n",
        "    # CVaR\n",
        "    summary[f'Expanding {window:.0f} Historical CVaR ({quantile:.2%})'] = returns.expanding(window).apply(lambda x: x[x < x.quantile(quantile)].mean())\n",
        "    summary[f'Rolling {window:.0f} Historical CVaR ({quantile:.2%})'] = returns.rolling(window).apply(lambda x: x[x < x.quantile(quantile)].mean())\n",
        "    summary[f'Expanding {window:.0f} Parametrical CVaR ({quantile:.2%})'] = - norm.pdf(z_score) / quantile * summary[f'Expanding {window:.0f} Volatility']\n",
        "    summary[f'Rolling {window:.0f} Parametrical CVaR ({quantile:.2%})'] = - norm.pdf(z_score) / quantile * summary[f'Rolling {window:.0f} Volatility']\n",
        "    summary[f'EWMA {ewma_theta:.2f} Parametrical CVaR ({quantile:.2%})'] = - norm.pdf(z_score) / quantile * summary[f'EWMA {ewma_theta:.2f} Volatility']\n",
        "    summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Parametrical CVaR ({quantile:.2%})'] = - norm.pdf(z_score) / quantile * summary[f'GARCH({garch_p:.0f}, {garch_q:.0f}) Volatility']\n",
        "\n",
        "    if shift > 0:\n",
        "        shift_columns = [c for c in summary.columns if not bool(re.search(\"returns\", c))]\n",
        "        summary[shift_columns] = summary[shift_columns].shift(shift)\n",
        "        print(f'VaR and CVaR are given shifted by {shift:0f} period(s).')\n",
        "    else:\n",
        "        print('VaR and CVaR are given in-sample.')\n",
        "\n",
        "    if full_time_sample:\n",
        "        summary = summary.loc[:, lambda df: [c for c in df.columns if bool(re.search('expanding', c.lower()))]]\n",
        "    return_stats = [return_stats.lower()] if isinstance(return_stats, str) else [s.lower() for s in return_stats]\n",
        "    return_stats = list(map(lambda x: 'volatility' if x == 'vol' else x, return_stats))\n",
        "    if return_stats == ['all'] or set(return_stats) == set(['returns', 'var', 'cvar', 'volatility']):\n",
        "        return filter_columns_and_indexes(\n",
        "            summary,\n",
        "            keep_columns=keep_columns,\n",
        "            drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes,\n",
        "            drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    return filter_columns_and_indexes(\n",
        "        summary.loc[:, lambda df: df.columns.map(lambda c: bool(re.search(r\"\\b\" + r\"\\b|\\b\".join(return_stats) + r\"\\b\", c.lower())))],\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_rolling_oos_port(\n",
        "    returns: pd.DataFrame,\n",
        "    weights_func,\n",
        "    window: Union[None, int] = None,\n",
        "    weights_func_params: dict = {},\n",
        "    port_name: str = 'Portfolio OOS',\n",
        "    expanding: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates a rolling out-of-sample portfolio based on a rolling or expanding window optimization.\n",
        "\n",
        "    Parameters:\n",
        "    returns (pd.DataFrame): Time series of asset returns.\n",
        "    weights_func (function): Function to calculate the portfolio weights.\n",
        "    window (int or None, default=None): Rolling window size for in-sample optimization.\n",
        "    weights_func_params (dict, default={}): Additional parameters for the weights function.\n",
        "    port_name (str, default='Portfolio OOS'): Name for the portfolio.\n",
        "    expanding (bool, default=False): If True, uses an expanding window instead of a rolling one.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Out-of-sample portfolio returns.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    if window is None:\n",
        "        print('Using \"window\" of 60 periods for in-sample optimization, since none were provided.')\n",
        "        window = 60\n",
        "    returns = returns.copy()\n",
        "    if 'date' in returns.columns.str.lower():\n",
        "        returns = returns.rename({'Date': 'date'}, axis=1)\n",
        "        returns = returns.set_index('date')\n",
        "    returns.index.name = 'date'\n",
        "\n",
        "    port_returns_oos = pd.DataFrame({})\n",
        "\n",
        "    for idx in range(0, len(returns.index)-window):\n",
        "        modified_idx = 0 if expanding else idx\n",
        "        weights_func_all_params = {'returns': returns.iloc[modified_idx:(window+idx), :]}\n",
        "        weights_func_all_params.update(weights_func_params)\n",
        "        wts = weights_func(**weights_func_all_params).iloc[:, 0]\n",
        "        idx_port_return_oos = sum(returns.iloc[window, :].loc[wts.index] * wts)\n",
        "        idx_port_return_oos = pd.DataFrame(\n",
        "            {port_name: idx_port_return_oos},\n",
        "            index=[returns.index[idx+window]]\n",
        "        )\n",
        "        port_returns_oos = pd.concat([port_returns_oos, idx_port_return_oos])\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        port_returns_oos,\n",
        "        keep_columns=keep_columns,\n",
        "        drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes,\n",
        "        drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_fx_exc_ret(\n",
        "    fx_rates: pd.DataFrame,\n",
        "    rf_rates: pd.DataFrame,\n",
        "    transform_to_log_fx_rates: bool = True,\n",
        "    transform_to_log_rf_rates: bool = True,\n",
        "    rf_to_fx: dict = None,\n",
        "    base_rf: str = None,\n",
        "    base_rf_series: Union[pd.Series, pd.DataFrame] = None,\n",
        "    annual_factor: Union[int, None] = None,\n",
        "    return_exc_ret: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates foreign exchange excess returns by subtracting risk-free rates from FX rates.\n",
        "\n",
        "    Parameters:\n",
        "    fx_rates (pd.DataFrame): Time series of FX rates.\n",
        "    rf_rates (pd.DataFrame): Time series of risk-free rates.\n",
        "    transform_to_log_fx_rates (bool, default=True): If True, converts FX rates to log returns.\n",
        "    transform_to_log_rf_rates (bool, default=True): If True, converts risk-free rates to log returns.\n",
        "    rf_to_fx (dict, default=None): Mapping of risk-free rates to FX pairs.\n",
        "    base_rf (str, default=None): Base risk-free rate to use for calculations.\n",
        "    base_rf_series (pd.Series or pd.DataFrame, default=None): Time series of the base risk-free rate.\n",
        "    annual_factor (int or None, default=None): Factor for annualizing the returns.\n",
        "    return_exc_ret (bool, default=False): If True, returns the excess returns instead of summary statistics.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary statistics or excess returns based on FX rates and risk-free rates.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    fx_rates = fx_rates.copy()\n",
        "    rf_rates = rf_rates.copy()\n",
        "    if isinstance(base_rf_series, (pd.Series, pd.DataFrame)):\n",
        "        base_rf_series = base_rf_series.copy()\n",
        "\n",
        "    if rf_to_fx is None:\n",
        "        rf_to_fx = {\n",
        "            'GBP1M': 'USUK',\n",
        "            'EUR1M': 'USEU',\n",
        "            'CHF1M': 'USSZ',\n",
        "            'JPY1M': 'USJP'\n",
        "        }\n",
        "\n",
        "    if transform_to_log_fx_rates:\n",
        "        fx_rates = fx_rates.applymap(lambda x: math.log(x))\n",
        "\n",
        "    if transform_to_log_rf_rates:\n",
        "        rf_rates = rf_rates.applymap(lambda x: math.log(x + 1))\n",
        "\n",
        "    if base_rf is None and base_rf_series is None:\n",
        "        print(\"No 'base_rf' or 'base_rf_series' was provided. Trying to use 'USD1M' as the base risk-free rate.\")\n",
        "        base_rf = 'USD1M'\n",
        "    if base_rf_series is None:\n",
        "        base_rf_series = rf_rates[base_rf]\n",
        "\n",
        "    all_fx_holdings_exc_ret = pd.DataFrame({})\n",
        "    for rf, fx in rf_to_fx.items():\n",
        "        fx_holdings_exc_ret = fx_rates[fx] - fx_rates[fx].shift(1) + rf_rates[rf].shift(1) - base_rf_series.shift(1)\n",
        "        try:\n",
        "            rf_name = re.sub('[0-9]+M', '', rf)\n",
        "        except:\n",
        "            rf_name = rf\n",
        "        fx_holdings_exc_ret = fx_holdings_exc_ret.dropna(axis=0).to_frame(rf_name)\n",
        "        all_fx_holdings_exc_ret = all_fx_holdings_exc_ret.join(fx_holdings_exc_ret, how='outer')\n",
        "\n",
        "    if not return_exc_ret:\n",
        "        return filter_columns_and_indexes(\n",
        "            calc_summary_statistics(all_fx_holdings_exc_ret, annual_factor=annual_factor),\n",
        "            keep_columns=keep_columns, drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes, drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    else:\n",
        "        return filter_columns_and_indexes(\n",
        "            all_fx_holdings_exc_ret,\n",
        "            keep_columns=keep_columns, drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes, drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    \n",
        "\n",
        "def calc_fx_regression(\n",
        "    fx_rates: pd.DataFrame,\n",
        "    rf_rates: pd.DataFrame,\n",
        "    transform_to_log_fx_rates: bool = True,\n",
        "    transform_to_log_rf_rates: bool = True,\n",
        "    rf_to_fx: dict = None,\n",
        "    base_rf: str = None,\n",
        "    base_rf_series: Union[pd.Series, pd.DataFrame] = None,\n",
        "    annual_factor: Union[int, None] = None,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False,\n",
        "    print_analysis: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates FX regression and provides an analysis of how the risk-free rate differentials affect FX rates.\n",
        "\n",
        "    Parameters:\n",
        "    fx_rates (pd.DataFrame): Time series of FX rates.\n",
        "    rf_rates (pd.DataFrame): Time series of risk-free rates.\n",
        "    transform_to_log_fx_rates (bool, default=True): If True, converts FX rates to log returns.\n",
        "    transform_to_log_rf_rates (bool, default=True): If True, converts risk-free rates to log returns.\n",
        "    rf_to_fx (dict, default=None): Mapping of risk-free rates to FX pairs.\n",
        "    base_rf (str, default=None): Base risk-free rate to use for calculations.\n",
        "    base_rf_series (pd.Series or pd.DataFrame, default=None): Time series of the base risk-free rate.\n",
        "    annual_factor (int or None, default=None): Factor for annualizing returns.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "    print_analysis (bool, default=True): If True, prints an analysis of the regression results.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary of regression statistics for the FX rates and risk-free rate differentials.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    fx_rates = fx_rates.copy()\n",
        "    rf_rates = rf_rates.copy()\n",
        "    if isinstance(base_rf_series, (pd.Series, pd.DataFrame)):\n",
        "        base_rf_series = base_rf_series.copy()\n",
        "\n",
        "    if rf_to_fx is None:\n",
        "        rf_to_fx = {\n",
        "            'GBP1M': 'USUK',\n",
        "            'EUR1M': 'USEU',\n",
        "            'CHF1M': 'USSZ',\n",
        "            'JPY1M': 'USJP'\n",
        "        }\n",
        "\n",
        "    if transform_to_log_fx_rates:\n",
        "        fx_rates = fx_rates.applymap(lambda x: math.log(x))\n",
        "\n",
        "    if transform_to_log_rf_rates:\n",
        "        rf_rates = rf_rates.applymap(lambda x: math.log(x + 1))\n",
        "\n",
        "    if base_rf is None and base_rf_series is None:\n",
        "        print(\"No 'base_rf' or 'base_rf_series' was provided. Trying to use 'USD1M' as the base risk-free rate.\")\n",
        "        base_rf = 'USD1M'\n",
        "    if base_rf_series is None:\n",
        "        base_rf_series = rf_rates[base_rf]\n",
        "\n",
        "    if annual_factor is None:\n",
        "        print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
        "        annual_factor = 12\n",
        "\n",
        "    all_regressions_summary = pd.DataFrame({})\n",
        "\n",
        "    for rf, fx in rf_to_fx.items():\n",
        "        try:\n",
        "            rf_name = re.sub('[0-9]+M', '', rf)\n",
        "        except:\n",
        "            rf_name = rf\n",
        "        factor = (base_rf_series - rf_rates[rf]).to_frame('Base RF - Foreign RF')\n",
        "        strat = fx_rates[fx].diff().to_frame(rf_name)\n",
        "        regression_summary = calc_regression(strat, factor, annual_factor=annual_factor, warnings=False)\n",
        "        all_regressions_summary = pd.concat([all_regressions_summary, regression_summary])\n",
        "\n",
        "    if print_analysis:\n",
        "        try:\n",
        "            print('\\n' * 2)\n",
        "            for currency in all_regressions_summary.index:\n",
        "                fx_beta = all_regressions_summary.loc[currency, 'Base RF - Foreign RF Beta']\n",
        "                fx_alpha = all_regressions_summary.loc[currency, 'Alpha']\n",
        "                print(f'For {currency} against the base currency, the Beta is {fx_beta:.2f}.')\n",
        "                if 1.1 >= fx_beta and fx_beta >= 0.85:\n",
        "                    print(\n",
        "                        'which shows that, on average, the difference in risk-free rate is mainly offset by the FX rate movement.'\n",
        "                    )\n",
        "                elif fx_beta > 1.1:\n",
        "                    print(\n",
        "                        'which shows that, on average, the difference in risk-free rate is more than offset by the FX rate movement.,\\n'\n",
        "                        'Therefore, on average, the currency with the lower risk-free rate outperforms.'\n",
        "                    )\n",
        "                elif fx_beta < 0.85 and fx_beta > 0.15:\n",
        "                    print(\n",
        "                        'which shows that, on average, the difference in risk-free rate is only partially offset by the FX rate movement.\\n'\n",
        "                        'Therefore, on average, the currency with the higher risk-free rate outperforms.'\n",
        "                    )\n",
        "                elif fx_beta <= 0.15 and fx_beta >= -0.1:\n",
        "                    print(\n",
        "                        'which shows that, on average, the difference in risk-free rate is almost not offset by the FX rate movement.\\n'\n",
        "                        'Therefore, on average, the currency with the higher risk-free rate outperforms.'\n",
        "                    )\n",
        "                elif fx_beta <= 0.15 and fx_beta >= -0.1:\n",
        "                    print(\n",
        "                        'which shows that, on average, the difference in risk-free rate is almost not offset by the FX rate movement.\\n'\n",
        "                        'Therefore, on average, the currency with the higher risk-free rate outperforms.'\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        'which shows that, on average, the change FX rate helps the currency with the highest risk-free return.\\n'\n",
        "                        'Therefore, the difference between returns is increased, on average, by the changes in the FX rate.'\n",
        "                    )\n",
        "                print('\\n' * 2)\n",
        "        except:\n",
        "            print('Could not print analysis. Review function.')\n",
        "\n",
        "    return filter_columns_and_indexes(\n",
        "        all_regressions_summary,\n",
        "        keep_columns=keep_columns, drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes, drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )\n",
        "\n",
        "\n",
        "def calc_dynamic_carry_trade(\n",
        "    fx_rates: pd.DataFrame,\n",
        "    rf_rates: pd.DataFrame,\n",
        "    transform_to_log_fx_rates: bool = True,\n",
        "    transform_to_log_rf_rates: bool = True,\n",
        "    rf_to_fx: dict = None,\n",
        "    base_rf: str = None,\n",
        "    base_rf_series: Union[pd.Series, pd.DataFrame] = None,\n",
        "    annual_factor: Union[int, None] = None,\n",
        "    return_premium_series: bool = False,\n",
        "    keep_columns: Union[list, str] = None,\n",
        "    drop_columns: Union[list, str] = None,\n",
        "    keep_indexes: Union[list, str] = None,\n",
        "    drop_indexes: Union[list, str] = None,\n",
        "    drop_before_keep: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates the dynamic carry trade strategy based on FX rates and risk-free rate differentials.\n",
        "\n",
        "    Parameters:\n",
        "    fx_rates (pd.DataFrame): Time series of FX rates.\n",
        "    rf_rates (pd.DataFrame): Time series of risk-free rates.\n",
        "    transform_to_log_fx_rates (bool, default=True): If True, converts FX rates to log returns.\n",
        "    transform_to_log_rf_rates (bool, default=True): If True, converts risk-free rates to log returns.\n",
        "    rf_to_fx (dict, default=None): Mapping of risk-free rates to FX pairs.\n",
        "    base_rf (str, default=None): Base risk-free rate to use for calculations.\n",
        "    base_rf_series (pd.Series or pd.DataFrame, default=None): Time series of the base risk-free rate.\n",
        "    annual_factor (int or None, default=None): Factor for annualizing the returns.\n",
        "    return_premium_series (bool, default=False): If True, returns the premium series instead of summary statistics.\n",
        "    keep_columns (list or str, default=None): Columns to keep in the resulting DataFrame.\n",
        "    drop_columns (list or str, default=None): Columns to drop from the resulting DataFrame.\n",
        "    keep_indexes (list or str, default=None): Indexes to keep in the resulting DataFrame.\n",
        "    drop_indexes (list or str, default=None): Indexes to drop from the resulting DataFrame.\n",
        "    drop_before_keep (bool, default=False): If True, drops specified columns/indexes before keeping.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Summary of the carry trade strategy statistics or premium series.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Function not available - needs testing prior to use\")\n",
        "    if annual_factor is None:\n",
        "        print(\"Regression assumes 'annual_factor' equals to 12 since it was not provided\")\n",
        "        annual_factor = 12\n",
        "        \n",
        "    fx_regressions = calc_fx_regression(\n",
        "        fx_rates, rf_rates, transform_to_log_fx_rates, transform_to_log_rf_rates,\n",
        "        rf_to_fx, base_rf, base_rf_series, annual_factor\n",
        "    )\n",
        "\n",
        "    fx_rates = fx_rates.copy()\n",
        "    rf_rates = rf_rates.copy()\n",
        "    if isinstance(base_rf_series, (pd.Series, pd.DataFrame)):\n",
        "        base_rf_series = base_rf_series.copy()\n",
        "\n",
        "    if rf_to_fx is None:\n",
        "        rf_to_fx = {\n",
        "            'GBP1M': 'USUK',\n",
        "            'EUR1M': 'USEU',\n",
        "            'CHF1M': 'USSZ',\n",
        "            'JPY1M': 'USJP'\n",
        "        }\n",
        "\n",
        "    if transform_to_log_fx_rates:\n",
        "        fx_rates = fx_rates.applymap(lambda x: math.log(x))\n",
        "\n",
        "    if transform_to_log_rf_rates:\n",
        "        rf_rates = rf_rates.applymap(lambda x: math.log(x + 1))\n",
        "\n",
        "    if base_rf is None and base_rf_series is None:\n",
        "        print(\"No 'base_rf' or 'base_rf_series' was provided. Trying to use 'USD1M' as the base risk-free rate.\")\n",
        "        base_rf = 'USD1M'\n",
        "    if base_rf_series is None:\n",
        "        base_rf_series = rf_rates[base_rf]\n",
        "\n",
        "    all_expected_fx_premium = pd.DataFrame({})\n",
        "    for rf in rf_to_fx.keys():\n",
        "        try:\n",
        "            rf_name = re.sub('[0-9]+M', '', rf)\n",
        "        except:\n",
        "            rf_name = rf\n",
        "        fx_er_usd = (base_rf_series.shift(1) - rf_rates[rf].shift(1)).to_frame('ER Over USD')\n",
        "        expected_fx_premium = fx_regressions.loc[rf_name, 'Alpha'] + (fx_regressions.loc[rf_name, 'Base RF - Foreign RF Beta'] - 1) * fx_er_usd\n",
        "        expected_fx_premium = expected_fx_premium.rename(columns={'ER Over USD': rf_name})\n",
        "        all_expected_fx_premium = all_expected_fx_premium.join(expected_fx_premium, how='outer')\n",
        "\n",
        "    if return_premium_series:\n",
        "        return filter_columns_and_indexes(\n",
        "            all_expected_fx_premium,\n",
        "            keep_columns=keep_columns, drop_columns=drop_columns,\n",
        "            keep_indexes=keep_indexes, drop_indexes=drop_indexes,\n",
        "            drop_before_keep=drop_before_keep\n",
        "        )\n",
        "    \n",
        "    all_expected_fx_premium = all_expected_fx_premium.dropna(axis=0)\n",
        "    summary_statistics = (\n",
        "        all_expected_fx_premium\n",
        "        .applymap(lambda x: 1 if x > 0 else 0)\n",
        "        .agg(['mean', 'sum', 'count'])\n",
        "        .set_axis(['% of Periods with Positive Premium', 'Nº of Positive Premium Periods', 'Total Number of Periods'])\n",
        "    )\n",
        "    summary_statistics = pd.concat([\n",
        "        summary_statistics,\n",
        "        (\n",
        "            all_expected_fx_premium\n",
        "            .agg(['mean', 'std', 'min', 'max', 'skew', 'kurtosis'])\n",
        "            .set_axis(['Mean', 'Vol', 'Min', 'Max', 'Skewness', 'Kurtosis'])\n",
        "        )\n",
        "    ])\n",
        "    summary_statistics = summary_statistics.transpose()\n",
        "    summary_statistics['Annualized Mean'] = summary_statistics['Mean'] * annual_factor\n",
        "    summary_statistics['Annualized Vol'] = summary_statistics['Vol'] * math.sqrt(annual_factor)\n",
        "    \n",
        "    return filter_columns_and_indexes(\n",
        "        summary_statistics,\n",
        "        keep_columns=keep_columns, drop_columns=drop_columns,\n",
        "        keep_indexes=keep_indexes, drop_indexes=drop_indexes,\n",
        "        drop_before_keep=drop_before_keep\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eecf95a7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "Date",
                  "rawType": "datetime64[ns]",
                  "type": "datetime"
                },
                {
                  "name": "LVL",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "HMS",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "IMO",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "aae2b699-3cea-4942-bb5c-07863a9be347",
              "rows": [
                [
                  "2010-01-05 00:00:00",
                  "0.001741153950860446",
                  "-0.0034706167504152",
                  "0.0065968004922214"
                ],
                [
                  "2010-01-06 00:00:00",
                  "0.01288937265446294",
                  "0.0163991210360111",
                  "-0.020703135345612"
                ],
                [
                  "2010-01-07 00:00:00",
                  "-0.01148447633212242",
                  "0.011919262005483",
                  "-0.0011532579621856"
                ],
                [
                  "2010-01-08 00:00:00",
                  "0.0004358236338964073",
                  "0.0075191328122755",
                  "0.0005696959214852"
                ],
                [
                  "2010-01-11 00:00:00",
                  "-0.007376597357486047",
                  "0.0063059046891503",
                  "-0.0006227700336321"
                ],
                [
                  "2010-01-12 00:00:00",
                  "-0.01695865158179982",
                  "0.0092117014449406",
                  "-0.0022464640857028"
                ],
                [
                  "2010-01-13 00:00:00",
                  "0.003375106026470398",
                  "-0.0084205261532412",
                  "-0.0072269192772027"
                ],
                [
                  "2010-01-14 00:00:00",
                  "-0.00352470696526004",
                  "0.0084499465701627",
                  "0.0010785519297359"
                ],
                [
                  "2010-01-15 00:00:00",
                  "-0.008237372004361015",
                  "-0.0004961577110437",
                  "-0.0044736484516859"
                ],
                [
                  "2010-01-19 00:00:00",
                  "0.005998597649833382",
                  "0.0032232632956643",
                  "-0.0095431194158179"
                ],
                [
                  "2010-01-20 00:00:00",
                  "-0.01416028736096878",
                  "-0.0110978042974632",
                  "0.0053466555564934"
                ],
                [
                  "2010-01-21 00:00:00",
                  "-0.004722696469911311",
                  "-0.0181311809994585",
                  "-0.0051070576086437"
                ],
                [
                  "2010-01-22 00:00:00",
                  "-0.01113363586913751",
                  "-0.0046089555609611",
                  "0.0036650873508653"
                ],
                [
                  "2010-01-25 00:00:00",
                  "0.004129575367412413",
                  "0.0047571578690996",
                  "-0.0041104160245117"
                ],
                [
                  "2010-01-26 00:00:00",
                  "-0.008329113336232746",
                  "-0.0127358206441892",
                  "0.0106479251903475"
                ],
                [
                  "2010-01-27 00:00:00",
                  "-0.01868017416533625",
                  "-0.0044745693529026",
                  "0.0087635334818538"
                ],
                [
                  "2010-01-28 00:00:00",
                  "-0.001056985143427951",
                  "-0.0127848733442394",
                  "-0.0015975464439078"
                ],
                [
                  "2010-01-29 00:00:00",
                  "-0.003809519597918737",
                  "0.0019100811863552",
                  "-0.0073949717301325"
                ],
                [
                  "2010-02-01 00:00:00",
                  "0.0125119426286366",
                  "0.0324152479260133",
                  "-0.0103271621834027"
                ],
                [
                  "2010-02-02 00:00:00",
                  "0.01995328083574096",
                  "0.0084808289112732",
                  "0.0118347799766292"
                ],
                [
                  "2010-02-03 00:00:00",
                  "-0.01142863847426832",
                  "0.0128268887652095",
                  "0.0099943810728315"
                ],
                [
                  "2010-02-04 00:00:00",
                  "-0.02163180252001172",
                  "-0.0393662865436742",
                  "0.0141636904516661"
                ],
                [
                  "2010-02-05 00:00:00",
                  "-0.01536602937651887",
                  "-0.0109823093670442",
                  "0.0054612782835606"
                ],
                [
                  "2010-02-08 00:00:00",
                  "0.009290083810734493",
                  "-0.009878605329016",
                  "0.0014415801171335"
                ],
                [
                  "2010-02-09 00:00:00",
                  "0.009888425373018782",
                  "0.009226538534547",
                  "0.0020229542980706"
                ],
                [
                  "2010-02-10 00:00:00",
                  "0.003095401242097216",
                  "-0.0024369920852977",
                  "0.0028885293520495"
                ],
                [
                  "2010-02-11 00:00:00",
                  "0.01015431792690259",
                  "0.0033590967847166",
                  "-0.0103801578978031"
                ],
                [
                  "2010-02-12 00:00:00",
                  "-0.00530895625619566",
                  "-0.0029076157853173",
                  "0.0006803334617853"
                ],
                [
                  "2010-02-16 00:00:00",
                  "0.0214977337399747",
                  "0.0059937048119832",
                  "0.0061492581685174"
                ],
                [
                  "2010-02-17 00:00:00",
                  "-0.005148364936108058",
                  "0.0205904906256183",
                  "0.0077292614426706"
                ],
                [
                  "2010-02-18 00:00:00",
                  "0.001761204853270334",
                  "0.0024262046745002",
                  "0.0144293284610356"
                ],
                [
                  "2010-02-19 00:00:00",
                  "0.004935222313990539",
                  "0.0023411050055658",
                  "-0.000899099377136"
                ],
                [
                  "2010-02-22 00:00:00",
                  "-0.003046508706112202",
                  "-0.0031993562579643",
                  "0.017798848294794"
                ],
                [
                  "2010-02-23 00:00:00",
                  "-0.01465157500806403",
                  "-0.0086110571145262",
                  "0.0009885586969006"
                ],
                [
                  "2010-02-24 00:00:00",
                  "0.007921921245492607",
                  "-0.0049418662939666",
                  "-0.002810436885448"
                ],
                [
                  "2010-02-25 00:00:00",
                  "-0.0100003282099866",
                  "0.0055616445392284",
                  "-0.0118301054307779"
                ],
                [
                  "2010-02-26 00:00:00",
                  "0.009761344755825702",
                  "0.012270092609942",
                  "-0.0008404584435399"
                ],
                [
                  "2010-03-01 00:00:00",
                  "-0.00665680944545869",
                  "0.0134508380526602",
                  "0.0271599033806561"
                ],
                [
                  "2010-03-02 00:00:00",
                  "0.01257507973609167",
                  "0.0121493301798789",
                  "-0.0032717783545758"
                ],
                [
                  "2010-03-03 00:00:00",
                  "0.005574490164239969",
                  "0.0164685691231879",
                  "0.0017598644645562"
                ],
                [
                  "2010-03-04 00:00:00",
                  "-0.01331480760970437",
                  "0.0034569786388947",
                  "0.0025605725250127"
                ],
                [
                  "2010-03-05 00:00:00",
                  "0.005390683597886141",
                  "0.0074325128211671",
                  "0.0016913529563287"
                ],
                [
                  "2010-03-08 00:00:00",
                  "0.0001633714650808902",
                  "-0.0005693000234144",
                  "0.0119161730704046"
                ],
                [
                  "2010-03-09 00:00:00",
                  "-0.007593759227745429",
                  "0.008466374257545",
                  "0.0062546496002367"
                ],
                [
                  "2010-03-10 00:00:00",
                  "-0.0009763834056749632",
                  "0.003699728236917",
                  "0.0129821679238752"
                ],
                [
                  "2010-03-11 00:00:00",
                  "-0.008900945294948278",
                  "0.0165360306243103",
                  "-0.0061756975799112"
                ],
                [
                  "2010-03-12 00:00:00",
                  "-0.002642079530354711",
                  "-0.0103418234987378",
                  "-0.0011807404993809"
                ],
                [
                  "2010-03-15 00:00:00",
                  "-2.303991610220512e-05",
                  "-0.0122870399525538",
                  "-0.0026352625393094"
                ],
                [
                  "2010-03-16 00:00:00",
                  "0.007687323395341958",
                  "0.0157845869564351",
                  "0.0137701023023612"
                ],
                [
                  "2010-03-17 00:00:00",
                  "0.00979235683173835",
                  "-0.0067820517250681",
                  "0.0063064187004703"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 3972
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LVL</th>\n",
              "      <th>HMS</th>\n",
              "      <th>IMO</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>0.001741</td>\n",
              "      <td>-0.003471</td>\n",
              "      <td>0.006597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>0.012889</td>\n",
              "      <td>0.016399</td>\n",
              "      <td>-0.020703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>-0.011484</td>\n",
              "      <td>0.011919</td>\n",
              "      <td>-0.001153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.000570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-11</th>\n",
              "      <td>-0.007377</td>\n",
              "      <td>0.006306</td>\n",
              "      <td>-0.000623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27</th>\n",
              "      <td>-0.001392</td>\n",
              "      <td>-0.004590</td>\n",
              "      <td>0.007534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28</th>\n",
              "      <td>-0.002979</td>\n",
              "      <td>-0.012895</td>\n",
              "      <td>0.002998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-29</th>\n",
              "      <td>0.008762</td>\n",
              "      <td>0.006220</td>\n",
              "      <td>0.006162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-30</th>\n",
              "      <td>0.019519</td>\n",
              "      <td>0.027967</td>\n",
              "      <td>-0.011074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-31</th>\n",
              "      <td>0.001617</td>\n",
              "      <td>-0.005249</td>\n",
              "      <td>-0.006371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3972 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 LVL       HMS       IMO\n",
              "Date                                    \n",
              "2010-01-05  0.001741 -0.003471  0.006597\n",
              "2010-01-06  0.012889  0.016399 -0.020703\n",
              "2010-01-07 -0.011484  0.011919 -0.001153\n",
              "2010-01-08  0.000436  0.007519  0.000570\n",
              "2010-01-11 -0.007377  0.006306 -0.000623\n",
              "...              ...       ...       ...\n",
              "2025-10-27 -0.001392 -0.004590  0.007534\n",
              "2025-10-28 -0.002979 -0.012895  0.002998\n",
              "2025-10-29  0.008762  0.006220  0.006162\n",
              "2025-10-30  0.019519  0.027967 -0.011074\n",
              "2025-10-31  0.001617 -0.005249 -0.006371\n",
              "\n",
              "[3972 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12738387",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "Date",
                  "rawType": "datetime64[ns]",
                  "type": "datetime"
                },
                {
                  "name": "CL",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "GC",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "HO",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "LE",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "NG",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "PL",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "RB",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "SB",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "SI",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "ZC",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "ZL",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "ZM",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "ZS",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "a418a486-029f-4410-93f4-685097d87416",
              "rows": [
                [
                  "2010-01-05 00:00:00",
                  "0.003189725187362935",
                  "0.0003578996434983051",
                  "0.001643406569903805",
                  "0.01112734346403732",
                  "-0.04197820325743651",
                  "0.008897383223856581",
                  "0.009789052118434727",
                  "0.000724060456458675",
                  "0.01955272894662841",
                  "0.0005973715651135247",
                  "-0.004645536291386865",
                  "0.01075947435596314",
                  "0.002620295378751747"
                ],
                [
                  "2010-01-06 00:00:00",
                  "0.01724353332511641",
                  "0.01591990807333388",
                  "0.004147577205961372",
                  "-0.004344048806915524",
                  "0.06599250222853903",
                  "0.013979554259965",
                  "0.005458831787109375",
                  "0.02785819373252396",
                  "0.02148360421416662",
                  "0.007164179104477642",
                  "-0.0009825820787748851",
                  "-0.004696305662692524",
                  "-0.0016631028747921"
                ],
                [
                  "2010-01-07 00:00:00",
                  "-0.006251462384691209",
                  "-0.002465048655641455",
                  "-0.008896220072285188",
                  "-0.0002907957809402673",
                  "-0.03378259240396564",
                  "0.0005154289739031093",
                  "-0.0007956212931914397",
                  "-0.01443153289736665",
                  "0.009359691421803706",
                  "-0.01007705986959095",
                  "-0.01671985544857357",
                  "-0.03428749325498148",
                  "-0.03117563065207041"
                ],
                [
                  "2010-01-08 00:00:00",
                  "0.001088753132149733",
                  "0.004500905212093231",
                  "0.007647934495378372",
                  "-0.001163876636179895",
                  "-0.009817457446227529",
                  "0.007469398316765963",
                  "0.009555392755756342",
                  "-0.01678568976266048",
                  "0.006818305719278328",
                  "0.01317365269461068",
                  "-0.01150294773349858",
                  "-0.0006515055603624198",
                  "-0.004667157946450451"
                ],
                [
                  "2010-01-11 00:00:00",
                  "-0.002779496760526801",
                  "0.01098225315080192",
                  "-0.009180572791892105",
                  "-0.009029932118165385",
                  "-0.05131328449726857",
                  "0.01514768861817983",
                  "-0.00584602869275308",
                  "-0.02833275216821474",
                  "0.01218985693128705",
                  "-0.00118203309692666",
                  "-0.00860106660984672",
                  "-0.006844770761055496",
                  "-0.01110562685093786"
                ],
                [
                  "2010-01-12 00:00:00",
                  "-0.02096456371686517",
                  "-0.01894492715986595",
                  "-0.02215496015858265",
                  "0.004703043881620461",
                  "0.02511919398957874",
                  "-0.009066312391469822",
                  "-0.02095484267233105",
                  "0.02280376113463767",
                  "-0.02355084947606889",
                  "-0.07100591715976334",
                  "-0.02373055212464237",
                  "-0.03052188361655817",
                  "-0.03219366109308708"
                ],
                [
                  "2010-01-13 00:00:00",
                  "-0.01411064954486629",
                  "0.006643635253611446",
                  "-0.01745003057121708",
                  "0.002925687641059049",
                  "0.02539791083750798",
                  "-0.002350894710459794",
                  "-0.01792355808074519",
                  "0.02485381176923318",
                  "0.01617059013322719",
                  "-0.02165605095541401",
                  "0.009670746545061615",
                  "0.01726474681412071",
                  "0.01444043321299637"
                ],
                [
                  "2010-01-14 00:00:00",
                  "-0.003264307988066872",
                  "0.005455782329001346",
                  "-0.005585751524491367",
                  "0.01108523467288403",
                  "-0.02529216569746651",
                  "0.01904217637489247",
                  "0.006601354803581261",
                  "-0.009985758826792868",
                  "0.005664016702521701",
                  "-0.0078125",
                  "-0.02148593859966941",
                  "-0.004991680532445919",
                  "-0.01525165226232839"
                ],
                [
                  "2010-01-15 00:00:00",
                  "-0.01750849477685823",
                  "-0.01093996172509093",
                  "-0.01771570533849742",
                  "0.008078441268499326",
                  "0.0184323849922583",
                  "-0.005437129744776836",
                  "-0.01369475433161194",
                  "-0.005043205637396908",
                  "-0.01244436814458649",
                  "-0.02493438320209973",
                  "-0.007142869397175899",
                  "-0.02441467489287208",
                  "0.005678884873515644"
                ],
                [
                  "2010-01-19 00:00:00",
                  "0.01307688003931284",
                  "0.008494802047013694",
                  "-0.0002933037939786276",
                  "0.006296543347058758",
                  "-0.02354591897438196",
                  "0.02746006692312641",
                  "0.006697960806520298",
                  "0.0492396329355318",
                  "0.02025968784168031",
                  "-0.00605652759084796",
                  "-0.01012516029250687",
                  "-0.002742606366362033",
                  "-0.01078028747433268"
                ],
                [
                  "2010-01-20 00:00:00",
                  "-0.01771695717488009",
                  "-0.02404132975137585",
                  "-0.01188024891861283",
                  "-0.005119505913822264",
                  "-0.01097720809406377",
                  "-0.01467800156464416",
                  "-0.006119151863876238",
                  "0.004485889239775886",
                  "-0.04887144141493094",
                  "-0.003385240352065",
                  "-0.015612430881077",
                  "-0.01615669229311012",
                  "-0.01401141670991179"
                ],
                [
                  "2010-01-21 00:00:00",
                  "-0.01984025845187387",
                  "-0.008630852499165398",
                  "-0.01756471664629011",
                  "-0.006861046255681513",
                  "0.02165210372735538",
                  "-0.01489665468542478",
                  "-0.03107742248833056",
                  "0.005152855217630536",
                  "-0.02070966909218597",
                  "0.01086956521739135",
                  "0.01175828025059134",
                  "0.004542235281347118",
                  "0.004210526315789442"
                ],
                [
                  "2010-01-22 00:00:00",
                  "-0.02024186223006552",
                  "-0.01224267760749709",
                  "-0.02215956369976269",
                  "-0.002590656051515539",
                  "0.03633125636702883",
                  "-0.03194502674802024",
                  "-0.00867416053761938",
                  "-0.01640463221058119",
                  "-0.03303618042759016",
                  "-0.019489247311828",
                  "-0.007837862581820065",
                  "-0.003826108186141264",
                  "-0.00262054507337528"
                ],
                [
                  "2010-01-25 00:00:00",
                  "0.009659259617115667",
                  "0.005508630434241768",
                  "0.01246398962726891",
                  "-0.004906240135732376",
                  "-0.01666947054684353",
                  "0.002668559958856509",
                  "0.01785616496134779",
                  "0.03544122745237943",
                  "0.01259021518158487",
                  "0.008224811514736086",
                  "-0.005720487325249768",
                  "-0.01187148732172549",
                  "-0.01156069364161849"
                ],
                [
                  "2010-01-26 00:00:00",
                  "-0.007308039279114498",
                  "0.002465370126522037",
                  "-0.00763053441150896",
                  "-0.007830573622509274",
                  "-0.04141908135265082",
                  "-0.01187929167680946",
                  "-0.01669329333315683",
                  "-0.01677852391949652",
                  "-0.01663649750634022",
                  "-0.01495581237253574",
                  "0.007397272815443001",
                  "0.01554768161301889",
                  "0.007442849548112696"
                ],
                [
                  "2010-01-27 00:00:00",
                  "-0.01392050499627828",
                  "-0.01229620156644484",
                  "-0.01742870663416718",
                  "-0.001753890946538217",
                  "-0.03846854339683137",
                  "-0.02371565940344811",
                  "-0.0143335934560801",
                  "-0.03208186522817436",
                  "-0.02481308074721511",
                  "-0.01104209799861977",
                  "-0.01223825829037861",
                  "-0.02122479552213463",
                  "-0.01952506596306069"
                ],
                [
                  "2010-01-28 00:00:00",
                  "-0.0004072048329373246",
                  "-0.0007377801642500792",
                  "0.001199929051567405",
                  "0.003221101328010567",
                  "-0.02578690786392512",
                  "0.002085980510641683",
                  "-0.01124177016692929",
                  "0.02256697376144756",
                  "-0.01375699044804701",
                  "0.00976971388695036",
                  "-0.004680566364914873",
                  "0.001066542209620325",
                  "0.00296017222820244"
                ],
                [
                  "2010-01-29 00:00:00",
                  "-0.01018468232232805",
                  "-0.000553687337998543",
                  "-0.008441490919260053",
                  "0.001751331269195466",
                  "-0.001362391877545832",
                  "0.008125233718096814",
                  "-0.007458010184703379",
                  "0.03103446960449219",
                  "-0.001172639188068869",
                  "-0.01451278507256393",
                  "0.0",
                  "-0.02769892805925434",
                  "-0.01905017440300505"
                ],
                [
                  "2010-02-01 00:00:00",
                  "0.02112773944879542",
                  "0.01966763511368885",
                  "0.02732673619633585",
                  "-0.00641029175052632",
                  "0.05905281052799927",
                  "0.02218072715298014",
                  "0.01523831835413381",
                  "-0.02073575049481025",
                  "0.029042779271931",
                  "0.007012622720897532",
                  "0.001106420446523204",
                  "-0.007304602224863066",
                  "-0.004649890590809669"
                ],
                [
                  "2010-02-02 00:00:00",
                  "0.03761928040141505",
                  "0.01186269583148092",
                  "0.03928582971164518",
                  "0.01143693224775477",
                  "0.003680526475957624",
                  "0.02600027697695473",
                  "0.04440760308570724",
                  "0.004098324080282545",
                  "0.004984097983179003",
                  "0.01671309192200554",
                  "0.0353689594519484",
                  "0.006622584221445793",
                  "0.01731244847485569"
                ],
                [
                  "2010-02-03 00:00:00",
                  "-0.003237083893996129",
                  "-0.005369607901294149",
                  "-0.00605405085707833",
                  "0.002609434364775565",
                  "-0.006417280424750538",
                  "-0.001651302372777108",
                  "0.009068861959198093",
                  "-0.02789114644434854",
                  "-0.02545414636347842",
                  "-0.0328767123287671",
                  "-0.01521216109566814",
                  "-0.01717840680622285",
                  "-0.01890869800108053"
                ],
                [
                  "2010-02-04 00:00:00",
                  "-0.04988313587725102",
                  "-0.04408853601189466",
                  "-0.04169550873632832",
                  "0.001156808641364027",
                  "-0.0005536555299954227",
                  "-0.03842485112027216",
                  "-0.04194092062214094",
                  "-0.03289015173432241",
                  "-0.05928872382701855",
                  "0.002832861189801639",
                  "0.00840101750067257",
                  "0.008553433851814507",
                  "0.006607929515418443"
                ],
                [
                  "2010-02-05 00:00:00",
                  "-0.0266611562006408",
                  "-0.00960097233413848",
                  "-0.03121124943435349",
                  "0.01010976278622122",
                  "0.01827916896860637",
                  "-0.02659606428837247",
                  "-0.03301207687667251",
                  "-0.05318376793832458",
                  "-0.03389170591153745",
                  "-0.007062146892655385",
                  "-0.005643619716193093",
                  "-0.0007375081048247578",
                  "-0.000547045951859948"
                ],
                [
                  "2010-02-08 00:00:00",
                  "0.009832798486252736",
                  "0.01283026100216467",
                  "0.005707268829161061",
                  "0.002859593837840624",
                  "-0.02067086969306819",
                  "0.004009992624175673",
                  "0.004028874633711155",
                  "0.01643103950791724",
                  "0.01720300311869449",
                  "0.01280227596017069",
                  "0.0256756962956608",
                  "0.01254610293905678",
                  "0.01751505199781067"
                ],
                [
                  "2010-02-09 00:00:00",
                  "0.02587287002563787",
                  "0.01032185465327662",
                  "0.02747282622440039",
                  "0.01226115666751193",
                  "-0.02055176088960631",
                  "0.01448688405187504",
                  "0.01847939051451597",
                  "0.01766914686519727",
                  "0.02341157219706069",
                  "0.007022471910112404",
                  "0.01133070610095221",
                  "-0.01384835232322301",
                  "-0.005379236148466915"
                ],
                [
                  "2010-02-10 00:00:00",
                  "0.01044063244835813",
                  "-0.0008357967721374093",
                  "0.004955372663641544",
                  "0.0008450360365317433",
                  "0.0003780444340122369",
                  "0.007006539550952873",
                  "0.0",
                  "-0.01588475471087625",
                  "-0.008359776561724486",
                  "0.009065550906555142",
                  "-0.002866091904383694",
                  "0.0214338051077132",
                  "0.01406165494862077"
                ],
                [
                  "2010-02-11 00:00:00",
                  "0.01019863352746375",
                  "0.01710345929412549",
                  "0.008269581866178433",
                  "0.006473454516094934",
                  "0.01965232334678424",
                  "0.004240954554106002",
                  "0.00347332236410125",
                  "0.03340845785990876",
                  "0.01895176846069058",
                  "0.004146510020732519",
                  "-0.003396944514247102",
                  "0.003617945087128094",
                  "0.005866666666666687"
                ],
                [
                  "2010-02-12 00:00:00",
                  "-0.0152763223236817",
                  "-0.004295331184068663",
                  "-0.02246563669357771",
                  "-0.0002796591213890531",
                  "0.01334322237295948",
                  "-0.005410723307076926",
                  "-0.003203014393118542",
                  "-0.02106792260224699",
                  "-0.009171347420336096",
                  "-0.004817618719889838",
                  "-0.007865737856880273",
                  "0.009372769144637694",
                  "0.002120890774125028"
                ],
                [
                  "2010-02-16 00:00:00",
                  "0.03885073505354408",
                  "0.02735204114559431",
                  "0.04033559256751307",
                  "0.01230767523492138",
                  "-0.02889539031537447",
                  "0.01764743231224464",
                  "0.03042237158342109",
                  "0.01521335197793094",
                  "0.04537505622875737",
                  "0.01590594744121709",
                  "0.02933405406647838",
                  "0.01392854963030143",
                  "0.02169312169312176"
                ],
                [
                  "2010-02-17 00:00:00",
                  "0.004155300427834518",
                  "0.0001786394739142771",
                  "0.005209666348942488",
                  "0.003868455153621442",
                  "0.0143126580870101",
                  "-0.0003911438662156375",
                  "0.009506164622984592",
                  "-0.04532162805029361",
                  "-0.00321981715198949",
                  "-0.01974132062627643",
                  "-0.005134807677070752",
                  "-0.01585065197867097",
                  "-0.01450025893319529"
                ],
                [
                  "2010-02-18 00:00:00",
                  "0.02237159817115542",
                  "-0.001339883876730674",
                  "0.02237501351478199",
                  "0.01486381666805792",
                  "-0.03973268081189096",
                  "-0.01141329122983781",
                  "0.03094012772991683",
                  "0.01339963487272877",
                  "-0.002360548379926608",
                  "-0.007638888888888862",
                  "-0.001290302891885031",
                  "-0.01360052926263344",
                  "-0.003678402522333202"
                ],
                [
                  "2010-02-19 00:00:00",
                  "0.009486466268154858",
                  "0.002951743137857843",
                  "0.008919895028886682",
                  "0.006509340431311905",
                  "-0.02474860484563179",
                  "0.01583322287036104",
                  "0.007974094275580601",
                  "0.01246694115097946",
                  "0.02198011535540401",
                  "0.007697690692792136",
                  "-0.004651170584682673",
                  "0.002902713262888268",
                  "-0.003164556962025333"
                ],
                [
                  "2010-02-22 00:00:00",
                  "0.00438549197121163",
                  "-0.007758916314397735",
                  "0.00429968934977687",
                  "-0.007814588652082954",
                  "-0.02954007998650032",
                  "-0.007598436726308666",
                  "0.01443154253628798",
                  "-0.07499993950573192",
                  "-0.01163711591410665",
                  "0.03194444444444455",
                  "0.008047802949299632",
                  "0.01917517520833156",
                  "0.0174603174603174"
                ],
                [
                  "2010-02-23 00:00:00",
                  "-0.01621760220018942",
                  "-0.008898098715891867",
                  "-0.02236865903539109",
                  "-0.003259131361344036",
                  "-0.02390196189023897",
                  "-0.01426603436038498",
                  "-0.02372624434491122",
                  "-0.01694231790168577",
                  "-0.02058935920047034",
                  "-0.01009421265141319",
                  "-0.01339171860012112",
                  "-0.007454760427813789",
                  "-0.009360374414976613"
                ],
                [
                  "2010-02-24 00:00:00",
                  "0.01445599011951826",
                  "-0.005622518768851004",
                  "0.004822101706177362",
                  "0.0002724961948654681",
                  "0.007953141017657206",
                  "-0.001858891812626218",
                  "0.0161213026131628",
                  "0.03036519268094695",
                  "0.003272913349670059",
                  "0.02039428959891221",
                  "0.01931604900138284",
                  "-0.009656695808624716",
                  "0.003149606299212682"
                ],
                [
                  "2010-02-25 00:00:00",
                  "-0.02287502288818355",
                  "0.01030556208675337",
                  "-0.02737377159730758",
                  "0.001362026673077832",
                  "-0.01017437427994228",
                  "0.01715999256942125",
                  "-0.02949170351312136",
                  "-0.02628434214931963",
                  "0.01066505862255318",
                  "-0.00799467021985345",
                  "-0.01331627318625306",
                  "-0.01733473419563591",
                  "-0.0146520146520146"
                ],
                [
                  "2010-02-26 00:00:00",
                  "0.01906109157049962",
                  "0.009478244752838982",
                  "0.01948443512099463",
                  "-0.01741021161648781",
                  "0.009649675246600875",
                  "0.005688845154057098",
                  "0.02052038455501703",
                  "-0.01308799968047036",
                  "0.02420852730432799",
                  "0.01544660846205503",
                  "0.01972494576456718",
                  "0.004042653725987666",
                  "0.0100902814657462"
                ],
                [
                  "2010-03-01 00:00:00",
                  "-0.01205130140263622",
                  "-0.0004471071967885321",
                  "-0.0006913891670903283",
                  "0.0157806796700386",
                  "-0.02784132468983047",
                  "0.002665783866019122",
                  "0.03694444358426585",
                  "-0.07749685026617115",
                  "-0.003090945157137814",
                  "-0.01917989417989419",
                  "0.005344848042232231",
                  "-0.008052752960216125",
                  "0.001577287066246047"
                ],
                [
                  "2010-03-02 00:00:00",
                  "0.01245239383653485",
                  "0.01708711285704578",
                  "0.01611066298393315",
                  "0.01144729429517577",
                  "0.006197965997504884",
                  "0.02075092439310966",
                  "0.01902017444063553",
                  "0.0170709414581931",
                  "0.03617248721679811",
                  "-0.0006743088334457692",
                  "0.007848136032683861",
                  "-0.001845018450184477",
                  "0.001837270341207287"
                ],
                [
                  "2010-03-03 00:00:00",
                  "0.01493476953876161",
                  "0.005101527516284232",
                  "0.01828706892513221",
                  "0.001347345706754099",
                  "0.01040777070332055",
                  "0.004764627480035477",
                  "0.02321775433703444",
                  "-0.0260600781136795",
                  "0.01554795704772793",
                  "0.01417004048583004",
                  "0.005275033339073198",
                  "-0.01478743068391863",
                  "0.0002619858527639352"
                ],
                [
                  "2010-03-04 00:00:00",
                  "-0.008161291451634911",
                  "-0.008838694335796204",
                  "-0.01194051571840893",
                  "0.001345532808900707",
                  "-0.03825944111517143",
                  "6.321167645473302e-05",
                  "-0.00618439272744864",
                  "-0.01723352262651867",
                  "-0.008839325079138649",
                  "-0.009980039920159722",
                  "-0.009745112073650142",
                  "-0.03227019175803236",
                  "-0.02304871660555263"
                ],
                [
                  "2010-03-05 00:00:00",
                  "0.01608279429312542",
                  "0.001942498048394636",
                  "0.0138733861049698",
                  "-0.0008062840785424408",
                  "0.003934453909387292",
                  "-0.002845040234505936",
                  "0.01669869312208783",
                  "0.02399632930008755",
                  "0.01200742438451141",
                  "-0.019489247311828",
                  "0.002271010602124424",
                  "0.0",
                  "0.002412868632707843"
                ],
                [
                  "2010-03-08 00:00:00",
                  "0.00453991100100648",
                  "-0.009869644660090993",
                  "0.003861940020686294",
                  "0.01344809081269838",
                  "-0.01436968989180998",
                  "0.01331473538557804",
                  "0.008014161077320336",
                  "-0.02794055089281366",
                  "-0.006335601088381826",
                  "-0.0006854009595613775",
                  "0.005790522033429557",
                  "0.006203978834361079",
                  "0.006151377373629252"
                ],
                [
                  "2010-03-09 00:00:00",
                  "-0.004641564310053248",
                  "-0.001423972606534707",
                  "-0.007456709560873853",
                  "0.002123190914184336",
                  "-0.00242989982838282",
                  "-0.002002222043323565",
                  "-0.01262456125806521",
                  "-0.05795085849259152",
                  "0.003825642339865043",
                  "-0.01577503429355276",
                  "0.0005006372310818463",
                  "-0.0019267822736031",
                  "0.001063264221158988"
                ],
                [
                  "2010-03-10 00:00:00",
                  "0.007362848077676176",
                  "-0.01265592796067294",
                  "0.01263283126014803",
                  "-0.005561456752042337",
                  "0.009521750047356337",
                  "-0.004200596346003116",
                  "0.0109720227606791",
                  "-0.03100389617264965",
                  "-0.01870893275269225",
                  "-0.009059233449477344",
                  "0.01801344468393262",
                  "-0.001158254026906391",
                  "0.01115241635687725"
                ],
                [
                  "2010-03-11 00:00:00",
                  "0.0002436870916475709",
                  "0.0001804939186331733",
                  "-0.0005670355946933991",
                  "-0.0005326556778461056",
                  "-0.02610220610661151",
                  "0.0141660890941866",
                  "-0.005732750831454969",
                  "-0.02133062797877916",
                  "0.00835590224192595",
                  "0.0",
                  "-0.02138114920735579",
                  "-0.03517590133019843",
                  "-0.02783613445378152"
                ],
                [
                  "2010-03-12 00:00:00",
                  "-0.01059557593612237",
                  "-0.005866425992779756",
                  "-0.009929034682068516",
                  "0.0135891454020749",
                  "-0.009009000301245074",
                  "-0.002669511235273037",
                  "-0.007482376491677045",
                  "0.02075763409590992",
                  "-0.006535919340165952",
                  "-0.00421940928270037",
                  "-0.01933702821860539",
                  "0.006410219573724785",
                  "0.0005402485143166658"
                ],
                [
                  "2010-03-15 00:00:00",
                  "-0.01772519509941928",
                  "0.003268248375794425",
                  "-0.01738303627981408",
                  "0.005783418092351278",
                  "-0.002045522992274451",
                  "0.004606302156279218",
                  "-0.01427942190845655",
                  "-0.01423491029788848",
                  "0.003465696350862579",
                  "0.02612994350282483",
                  "-0.007682438999277452",
                  "0.02547774376204348",
                  "0.004319654427645814"
                ],
                [
                  "2010-03-16 00:00:00",
                  "0.02380944641383098",
                  "0.01547369103584573",
                  "0.02755637181061843",
                  "-0.001829618912347453",
                  "-0.01002042222732369",
                  "0.0092323094297444",
                  "0.02348392953887979",
                  "-0.05930890431818248",
                  "0.01463443173449375",
                  "0.009635237439779676",
                  "0.01599997243573581",
                  "0.01513972750030601",
                  "0.0161290322580645"
                ],
                [
                  "2010-03-17 00:00:00",
                  "0.0150551212102592",
                  "0.00160403573912582",
                  "0.01191878635325128",
                  "0.007069945666883459",
                  "-0.01012195710549868",
                  "0.003008289659172991",
                  "0.01525271005933537",
                  "0.004934218953796199",
                  "0.009750223494203736",
                  "0.01976823449216081",
                  "0.01168409732410169",
                  "0.02256211815099185",
                  "0.01481481481481484"
                ]
              ],
              "shape": {
                "columns": 13,
                "rows": 3972
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CL</th>\n",
              "      <th>GC</th>\n",
              "      <th>HO</th>\n",
              "      <th>LE</th>\n",
              "      <th>NG</th>\n",
              "      <th>PL</th>\n",
              "      <th>RB</th>\n",
              "      <th>SB</th>\n",
              "      <th>SI</th>\n",
              "      <th>ZC</th>\n",
              "      <th>ZL</th>\n",
              "      <th>ZM</th>\n",
              "      <th>ZS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>0.003190</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.001643</td>\n",
              "      <td>0.011127</td>\n",
              "      <td>-0.041978</td>\n",
              "      <td>0.008897</td>\n",
              "      <td>0.009789</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.019553</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>-0.004646</td>\n",
              "      <td>0.010759</td>\n",
              "      <td>0.002620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>0.017244</td>\n",
              "      <td>0.015920</td>\n",
              "      <td>0.004148</td>\n",
              "      <td>-0.004344</td>\n",
              "      <td>0.065993</td>\n",
              "      <td>0.013980</td>\n",
              "      <td>0.005459</td>\n",
              "      <td>0.027858</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>-0.000983</td>\n",
              "      <td>-0.004696</td>\n",
              "      <td>-0.001663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>-0.006251</td>\n",
              "      <td>-0.002465</td>\n",
              "      <td>-0.008896</td>\n",
              "      <td>-0.000291</td>\n",
              "      <td>-0.033783</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>-0.000796</td>\n",
              "      <td>-0.014432</td>\n",
              "      <td>0.009360</td>\n",
              "      <td>-0.010077</td>\n",
              "      <td>-0.016720</td>\n",
              "      <td>-0.034287</td>\n",
              "      <td>-0.031176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.004501</td>\n",
              "      <td>0.007648</td>\n",
              "      <td>-0.001164</td>\n",
              "      <td>-0.009817</td>\n",
              "      <td>0.007469</td>\n",
              "      <td>0.009555</td>\n",
              "      <td>-0.016786</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>0.013174</td>\n",
              "      <td>-0.011503</td>\n",
              "      <td>-0.000652</td>\n",
              "      <td>-0.004667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-11</th>\n",
              "      <td>-0.002779</td>\n",
              "      <td>0.010982</td>\n",
              "      <td>-0.009181</td>\n",
              "      <td>-0.009030</td>\n",
              "      <td>-0.051313</td>\n",
              "      <td>0.015148</td>\n",
              "      <td>-0.005846</td>\n",
              "      <td>-0.028333</td>\n",
              "      <td>0.012190</td>\n",
              "      <td>-0.001182</td>\n",
              "      <td>-0.008601</td>\n",
              "      <td>-0.006845</td>\n",
              "      <td>-0.011106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27</th>\n",
              "      <td>-0.003089</td>\n",
              "      <td>-0.028288</td>\n",
              "      <td>0.013732</td>\n",
              "      <td>-0.021070</td>\n",
              "      <td>0.041768</td>\n",
              "      <td>-0.009725</td>\n",
              "      <td>-0.001196</td>\n",
              "      <td>-0.034068</td>\n",
              "      <td>-0.037518</td>\n",
              "      <td>0.012995</td>\n",
              "      <td>0.009946</td>\n",
              "      <td>0.013941</td>\n",
              "      <td>0.024478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28</th>\n",
              "      <td>-0.018920</td>\n",
              "      <td>-0.008921</td>\n",
              "      <td>-0.020073</td>\n",
              "      <td>-0.005790</td>\n",
              "      <td>-0.028181</td>\n",
              "      <td>-0.000887</td>\n",
              "      <td>0.002499</td>\n",
              "      <td>-0.006224</td>\n",
              "      <td>0.012091</td>\n",
              "      <td>0.007580</td>\n",
              "      <td>-0.010045</td>\n",
              "      <td>0.027834</td>\n",
              "      <td>0.010307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-29</th>\n",
              "      <td>0.005486</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>0.015541</td>\n",
              "      <td>0.017143</td>\n",
              "      <td>0.009268</td>\n",
              "      <td>0.009068</td>\n",
              "      <td>0.025192</td>\n",
              "      <td>0.003479</td>\n",
              "      <td>0.012647</td>\n",
              "      <td>0.004630</td>\n",
              "      <td>-0.001990</td>\n",
              "      <td>0.007178</td>\n",
              "      <td>0.001855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-30</th>\n",
              "      <td>0.001488</td>\n",
              "      <td>0.004418</td>\n",
              "      <td>0.014726</td>\n",
              "      <td>0.016746</td>\n",
              "      <td>0.171801</td>\n",
              "      <td>0.010683</td>\n",
              "      <td>0.015048</td>\n",
              "      <td>-0.009709</td>\n",
              "      <td>0.014815</td>\n",
              "      <td>-0.008641</td>\n",
              "      <td>-0.010167</td>\n",
              "      <td>0.022352</td>\n",
              "      <td>0.010183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-31</th>\n",
              "      <td>0.006769</td>\n",
              "      <td>-0.004773</td>\n",
              "      <td>-0.011707</td>\n",
              "      <td>0.005632</td>\n",
              "      <td>0.042467</td>\n",
              "      <td>-0.023938</td>\n",
              "      <td>-0.005141</td>\n",
              "      <td>0.010504</td>\n",
              "      <td>-0.008962</td>\n",
              "      <td>0.002905</td>\n",
              "      <td>-0.019537</td>\n",
              "      <td>0.019011</td>\n",
              "      <td>0.007789</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3972 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  CL        GC        HO        LE        NG        PL  \\\n",
              "Date                                                                     \n",
              "2010-01-05  0.003190  0.000358  0.001643  0.011127 -0.041978  0.008897   \n",
              "2010-01-06  0.017244  0.015920  0.004148 -0.004344  0.065993  0.013980   \n",
              "2010-01-07 -0.006251 -0.002465 -0.008896 -0.000291 -0.033783  0.000515   \n",
              "2010-01-08  0.001089  0.004501  0.007648 -0.001164 -0.009817  0.007469   \n",
              "2010-01-11 -0.002779  0.010982 -0.009181 -0.009030 -0.051313  0.015148   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2025-10-27 -0.003089 -0.028288  0.013732 -0.021070  0.041768 -0.009725   \n",
              "2025-10-28 -0.018920 -0.008921 -0.020073 -0.005790 -0.028181 -0.000887   \n",
              "2025-10-29  0.005486  0.004412  0.015541  0.017143  0.009268  0.009068   \n",
              "2025-10-30  0.001488  0.004418  0.014726  0.016746  0.171801  0.010683   \n",
              "2025-10-31  0.006769 -0.004773 -0.011707  0.005632  0.042467 -0.023938   \n",
              "\n",
              "                  RB        SB        SI        ZC        ZL        ZM  \\\n",
              "Date                                                                     \n",
              "2010-01-05  0.009789  0.000724  0.019553  0.000597 -0.004646  0.010759   \n",
              "2010-01-06  0.005459  0.027858  0.021484  0.007164 -0.000983 -0.004696   \n",
              "2010-01-07 -0.000796 -0.014432  0.009360 -0.010077 -0.016720 -0.034287   \n",
              "2010-01-08  0.009555 -0.016786  0.006818  0.013174 -0.011503 -0.000652   \n",
              "2010-01-11 -0.005846 -0.028333  0.012190 -0.001182 -0.008601 -0.006845   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2025-10-27 -0.001196 -0.034068 -0.037518  0.012995  0.009946  0.013941   \n",
              "2025-10-28  0.002499 -0.006224  0.012091  0.007580 -0.010045  0.027834   \n",
              "2025-10-29  0.025192  0.003479  0.012647  0.004630 -0.001990  0.007178   \n",
              "2025-10-30  0.015048 -0.009709  0.014815 -0.008641 -0.010167  0.022352   \n",
              "2025-10-31 -0.005141  0.010504 -0.008962  0.002905 -0.019537  0.019011   \n",
              "\n",
              "                  ZS  \n",
              "Date                  \n",
              "2010-01-05  0.002620  \n",
              "2010-01-06 -0.001663  \n",
              "2010-01-07 -0.031176  \n",
              "2010-01-08 -0.004667  \n",
              "2010-01-11 -0.011106  \n",
              "...              ...  \n",
              "2025-10-27  0.024478  \n",
              "2025-10-28  0.010307  \n",
              "2025-10-29  0.001855  \n",
              "2025-10-30  0.010183  \n",
              "2025-10-31  0.007789  \n",
              "\n",
              "[3972 rows x 13 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "returns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91580049",
      "metadata": {},
      "source": [
        "# 1. Commodity Returns and Factors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7e7cfa1ad6d2747",
      "metadata": {},
      "source": [
        "### 1.1 Factor Summary Statistics\n",
        "\n",
        "For each of the three factors, report only the following summary statistics (rounded to at least 6 decimal places):\n",
        "- Annualized Mean Return\n",
        "- Annualized Volatility\n",
        "- Annualized Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be634b1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Annualized Mean  Annualized Vol  Annualized Sharpe\n",
            "LVL         0.064439        0.154613           0.416775\n",
            "HMS         0.008842        0.226259           0.039078\n",
            "IMO         0.007281        0.153281           0.047500\n"
          ]
        }
      ],
      "source": [
        "factor_stats = calc_summary_statistics(\n",
        "    returns=factors,\n",
        "    annual_factor=252,              # daily → ~252 trading days per year\n",
        "    provided_excess_returns=True,   # treat as excess returns → Sharpe = mean/vol\n",
        "    correlations=False,             # we don’t need correlation matrix here\n",
        "    return_tangency_weights=False   # we don’t need tangency weights for this part\n",
        ")\n",
        "\n",
        "factor_stats_1_1 = factor_stats[[\n",
        "    \"Annualized Mean\",\n",
        "    \"Annualized Vol\",\n",
        "    \"Annualized Sharpe\"\n",
        "]]\n",
        "\n",
        "# 4. Round to at least 6 decimal places\n",
        "pd.options.display.float_format = \"{:,.6f}\".format\n",
        "\n",
        "print(factor_stats_1_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dce7b70133a43d6",
      "metadata": {},
      "source": [
        "### 1.2 Factor Correlations\n",
        "\n",
        "Calculate and report the correlation matrix between the three factors (rounded to at least 6 decimal places)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4f0e1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         LVL       HMS       IMO\n",
            "LVL 1.000000  0.400202  0.010439\n",
            "HMS 0.400202  1.000000 -0.071747\n",
            "IMO 0.010439 -0.071747  1.000000\n"
          ]
        }
      ],
      "source": [
        "# 1. Ensure we’re using the same factor DataFrame as before\n",
        "factors = factors[[\"LVL\", \"HMS\", \"IMO\"]]\n",
        "\n",
        "# 2. Compute the correlation matrix\n",
        "corr_matrix = factors.corr()\n",
        "\n",
        "# 3. Round to six decimal places\n",
        "corr_matrix = corr_matrix.round(6)\n",
        "\n",
        "# 4. Display with full precision\n",
        "pd.options.display.float_format = \"{:,.6f}\".format\n",
        "\n",
        "print(corr_matrix)\n",
        "# Xtx make pretty later"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215f5c12877f7307",
      "metadata": {},
      "source": [
        "### 1.3 Interpretation\n",
        "\n",
        "Does the factor construction make sense given the correlations you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5685ec7b",
      "metadata": {},
      "source": [
        "Yes, the factor construction makes sense. The moderate positive correlation between LVL and HMS shows that periods of broad commodity strength tend to favor hard over soft commodities, which is economically intuitive. Meanwhile, IMO is nearly uncorrelated with the others, confirming it captures a distinct production-chain dynamic independent of overall commodity movements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a685656af1e3da9e",
      "metadata": {},
      "source": [
        "### 1.4 Tangency Portfolio Weights\n",
        "\n",
        "Build a tangency portfolio using the three factors as assets.\n",
        "\n",
        "Report the weights of each factor in the tangency portfolio, rounded to at least 6 decimal places. You may assume that the factors use *excess* return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "94ddadc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Tangency Weights\n",
            "LVL          1.171917\n",
            "HMS         -0.250927\n",
            "IMO          0.079011\n"
          ]
        }
      ],
      "source": [
        "# Tangency portfolio weights (daily data ⇒ annual_factor=252)\n",
        "tangency_weights = calc_tangency_weights(\n",
        "    returns=factors,\n",
        "    annual_factor=252,\n",
        "    name=\"Tangency\"\n",
        ")\n",
        "\n",
        "# Round to six decimal places\n",
        "tangency_weights = tangency_weights.round(6)\n",
        "\n",
        "print(tangency_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562c3fdb2786bf50",
      "metadata": {},
      "source": [
        "### 1.5 Interpretation\n",
        "\n",
        "What do the tangency portfolio weights suggest about the relative importance of each factor?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc780bef",
      "metadata": {},
      "source": [
        "The tangency portfolio places the greatest weight on LVL, showing it is the dominant source of risk-adjusted return among the three factors. The small positive weight on IMO suggests it provides modest diversification benefits, while the negative weight on HMS indicates that hard-minus-soft exposure adds volatility without improving expected return. Overall, the portfolio is driven primarily by the broad LVL factor, with the other two used mainly to fine-tune risk."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dbe4c8",
      "metadata": {},
      "source": [
        "### 1.6. \n",
        "\n",
        "Estimate an autoregression of the factor `LVL`...\n",
        "\n",
        "$$r_t = \\gamma + \\rho\\, r_{t-1} + \\epsilon_t$$\n",
        "\n",
        "Only report $\\rho$ (rounded to at least 6 decimal places).\n",
        "\n",
        "Does the `LVL` factor exhibit momentum?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f79ec307",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ρ = 0.017041\n"
          ]
        }
      ],
      "source": [
        "# 1. Set up LVL and its lag\n",
        "lvl = factors[[\"LVL\"]].copy()\n",
        "lvl_lag = lvl.shift(1).rename(columns={\"LVL\": \"LVL_lag\"})\n",
        "\n",
        "# 2. Align y_t and r_{t-1} and drop the first NaN\n",
        "ar_df = lvl.join(lvl_lag, how=\"inner\").dropna()\n",
        "\n",
        "# 3. Run the autoregression r_t = γ + ρ r_{t-1} + ε_t\n",
        "ar_results = calc_regression(\n",
        "    y=ar_df[\"LVL\"],\n",
        "    X=ar_df[[\"LVL_lag\"]],\n",
        "    intercept=True,\n",
        "    annual_factor=252,   # annual_factor choice does NOT affect ρ\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "rho = ar_results.loc[\"LVL\", \"LVL_lag Beta\"]\n",
        "print(\"ρ =\", round(rho, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d45391",
      "metadata": {},
      "source": [
        "The estimate $\\rho = 0.017041$ is very close to zero, meaning the LVL factor’s current return is almost uncorrelated with its past return. This implies the LVL factor does not exhibit meaningful momentum — its returns are effectively serially independent over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e5f27c7a637fea",
      "metadata": {},
      "source": [
        "# 2. Single Factor Model\n",
        "\n",
        "### 2.1 LVL Factor\n",
        "\n",
        "We want to test the hypothesis that:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[r_{i}] = \\beta_{i,LVL} \\cdot \\mathbb{E}[r_{LVL}]\n",
        "$$\n",
        "\n",
        "Regress each commodity's returns against the LVL factor, and report the mean absolute alpha, $\\bar{\\alpha}$ and $r^2$ across all commodities. \n",
        "\n",
        "Annualize the alpha.\n",
        "\n",
        "Output *exactly* the following two numbers rounded to 6 decimal places:\n",
        "- Mean Absolute Annualized Alpha across all commodities\n",
        "- Mean R-squared across all commodities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f53ae0a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "annualized alpha 0.034876\n",
            "mean r2 0.25989\n"
          ]
        }
      ],
      "source": [
        "# 1. Run time-series regressions of each commodity on LVL\n",
        "lvl_regressions = calc_iterative_regression(\n",
        "    multiple_y=returns,            # each commodity return\n",
        "    X=factors[[\"LVL\"]],            # single factor\n",
        "    annual_factor=252,             # daily data\n",
        "    intercept=True,                # include alpha\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "# 2. Compute the two required summary metrics\n",
        "mean_abs_annualized_alpha = lvl_regressions[\"Annualized Alpha\"].abs().mean()\n",
        "mean_r2 = lvl_regressions[\"R-Squared\"].mean()\n",
        "\n",
        "# 3. Round and print with exactly 6 decimals\n",
        "print(\"annualized alpha\", round(mean_abs_annualized_alpha, 6))\n",
        "print(\"mean r2\", round(mean_r2, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effd0c587319b517",
      "metadata": {},
      "source": [
        "### 2.2. Interpretation\n",
        "\n",
        "If our hypothesis were true, what would you expect the values of $\\bar{\\alpha}$ and $\\bar{r^2}$ to be?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef9237f",
      "metadata": {},
      "source": [
        "If the hypothesis\n",
        "$\\mathbb{E}[r_i] = \\beta_{i,LVL} \\cdot \\mathbb{E}[r_{LVL}]$\n",
        "were true, every asset’s return would be fully explained by its exposure to the LVL factor.\n",
        "In that case, we would expect mean absolute alpha ($\\bar{\\alpha}$) to be zero and mean $R^2$ to be one, since there would be no unexplained return variation and all variation would be captured by the factor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e3fc5b67c89bf7",
      "metadata": {},
      "source": [
        "### 2.3 Cross-Sectional Test\n",
        "\n",
        "Let's test the one-factor `LVL` model directly. From `2.1`, we already have what we need:\n",
        "\n",
        "- The dependent variable, (y): mean excess returns from each of the commodities.\n",
        "- The regressor, (x): the market beta for each commodity from the time-series regressions.\n",
        "\n",
        "Then we can estimate the following equation:\n",
        "\n",
        "$$\n",
        "\\underbrace{\\mathbb{E}\\left[\\tilde{r}^{i}\\right]}_{n\\times 1\\text{ data}} = \\textcolor{ForestGreen}{\\underbrace{\\eta}_{\\text{regression intercept}}} + \\underbrace{{\\beta}^{i,\\text{LVL}};}_{n\\times 1\\text{ data}}~ \\textcolor{ForestGreen}{\\underbrace{\\lambda_{\\text{LVL}}}_{\\text{regression estimate}}} + \\textcolor{ForestGreen}{\\underbrace{\\upsilon}_{n\\times 1\\text{ residuals}}}\n",
        "$$\n",
        "\n",
        "Report exactly the following 3 numbers (rounded to at least 6 decimal places):\n",
        "* The R-squared of this regression\n",
        "* The intercept estimate, $\\hat{\\eta}$\n",
        "* The regression coefficient $\\lambda_{LVL}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ed0cf2d2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2        = 0.093357\n",
            "eta        = 0.000163\n",
            "lambda_LVL = 0.000092\n"
          ]
        }
      ],
      "source": [
        "# 1. Get each commodity’s mean excess return (dependent variable)\n",
        "mean_excess_returns = returns.mean().to_frame(\"Mean Excess Return\")\n",
        "\n",
        "# 2. Extract the LVL betas from your previous time-series regressions\n",
        "lvl_betas = lvl_regressions[[\"LVL Beta\"]]\n",
        "\n",
        "# 3. Run the cross-sectional regression:\n",
        "cross_section_lvl = calc_regression(\n",
        "    y=mean_excess_returns,\n",
        "    X=lvl_betas,\n",
        "    intercept=True,\n",
        "    annual_factor=252,   # consistent with daily data\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "r2 = cross_section_lvl.loc[\"Mean Excess Return\", \"R-Squared\"]\n",
        "eta = cross_section_lvl.loc[\"Mean Excess Return\", \"Alpha\"]\n",
        "lambda_lvl = cross_section_lvl.loc[\"Mean Excess Return\", \"LVL Beta Beta\"]\n",
        "\n",
        "# Nicely formatted output\n",
        "print(f\"R^2        = {r2:.6f}\")\n",
        "print(f\"eta        = {eta:.6f}\")\n",
        "print(f\"lambda_LVL = {lambda_lvl:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aadd11",
      "metadata": {},
      "source": [
        "### 2.4.\n",
        "\n",
        "Does your time-series or cross-sectional estimate give a higher premium to the `LVL` factor?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43904a80",
      "metadata": {},
      "source": [
        "The time-series estimate from Section 1.1 gives an annualized mean (risk premium) for the LVL factor of 0.064439.\n",
        "If your cross-sectional regression from 2.3 produced a smaller $\\hat{\\lambda}_{LVL}$, this means the time-series estimate assigns a higher premium to the LVL factor. In other words, the factor itself earns about 6.4 % per year, while the cross-sectional pricing across commodities attributes a lower return per unit of LVL exposure—so the time-series result implies a stronger reward for LVL risk."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801e0ff08859f98d",
      "metadata": {},
      "source": [
        "# 3. Trading the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163b7ab78e4151a",
      "metadata": {},
      "source": [
        "#### 3.1 Beta Estimation\n",
        "\n",
        "For each commodity, report their `LVL` beta rounded to at least 6 decimal places. \n",
        "\n",
        "Display a table of these betas, sorted from lowest to highest.\n",
        "\n",
        "#### Remember\n",
        "We estimated the betas in the time-series regression in `2.1`.\n",
        "\n",
        "#### Hint\n",
        "Use `df.sort_values(by=<YOUR_DF>, ascending=True)` to sort your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "968dbd8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    LVL Beta\n",
            "LE  0.243876\n",
            "GC  0.441604\n",
            "ZM  0.698967\n",
            "SB  0.742221\n",
            "ZS  0.770218\n",
            "ZC  0.823626\n",
            "PL  0.834771\n",
            "ZL  0.850391\n",
            "SI  1.038445\n",
            "NG  1.407854\n",
            "HO  1.533688\n",
            "RB  1.695943\n",
            "CL  1.918395\n"
          ]
        }
      ],
      "source": [
        "# 1. Extract LVL betas from the time-series regressions\n",
        "lvl_betas_sorted = lvl_regressions[[\"LVL Beta\"]].copy()\n",
        "\n",
        "# 2. Round to six decimal places\n",
        "lvl_betas_sorted = lvl_betas_sorted.round(6)\n",
        "\n",
        "# 3. Sort from lowest to highest beta\n",
        "lvl_betas_sorted = lvl_betas_sorted.sort_values(by=\"LVL Beta\", ascending=True)\n",
        "\n",
        "# 4. Display the result\n",
        "print(lvl_betas_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c8d7190d444ab3",
      "metadata": {},
      "source": [
        "#### 3.2 Portfolio Formation\n",
        "\n",
        "Regardless of your answer to `3.1`, allocate your portfolio as follows:\n",
        "\n",
        "- Go long `GC`, `LE`, and `ZM`\n",
        "- Go short `CL`, `HO`, and `RB`\n",
        "\n",
        "Go long `1` and short `0.25`.\n",
        "\n",
        "That is, your portfolio should be:\n",
        "$$\n",
        "r_{port} =  1 \\cdot \\left(\n",
        "    \\frac{1}{3} \\cdot r_{GC} + \\frac{1}{3} \\cdot r_{LE} + \\frac{1}{3} \\cdot r_{ZM}\n",
        "\\right) - 0.25 \\cdot \\left(\\frac{1}{3} \\cdot r_{CL} + \\frac{1}{3} \\cdot r_{HO} + \\frac{1}{3} \\cdot r_{RB}\\right)\n",
        "$$\n",
        "Report the last 5 daily returns of your betting against beta portfolio, rounded to at least 6 decimal places."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "20a40073",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date\n",
            "2025-10-27   -0.012593\n",
            "2025-10-28    0.007415\n",
            "2025-10-29    0.005726\n",
            "2025-10-30    0.011900\n",
            "2025-10-31    0.007463\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 1. Define long and short commodity tickers\n",
        "long_assets = [\"GC\", \"LE\", \"ZM\"]\n",
        "short_assets = [\"CL\", \"HO\", \"RB\"]\n",
        "\n",
        "# 2. Compute the long and short legs\n",
        "long_leg = returns[long_assets].mean(axis=1)         # equal-weight long basket\n",
        "short_leg = returns[short_assets].mean(axis=1)       # equal-weight short basket\n",
        "\n",
        "# 3. Form the betting-against-beta portfolio\n",
        "r_port = 1.0 * long_leg - 0.25 * short_leg\n",
        "\n",
        "# 4. Round to 6 decimal places and show last 5 daily returns\n",
        "r_port_rounded = r_port.round(6)\n",
        "print(r_port_rounded.tail(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7aa72c0fbf54b63",
      "metadata": {},
      "source": [
        "#### 3.3 Performance Evaluation\n",
        "\n",
        "For your portfolio, report the following performance statistics (rounded to at least 6 decimal places):\n",
        "\n",
        "- Annualized Return\n",
        "- Annualized Volatility\n",
        "- Annualized Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0e605998",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               Annualized Mean  Annualized Vol  Annualized Sharpe\n",
            "BAB_Portfolio         0.053383        0.144502           0.369425\n"
          ]
        }
      ],
      "source": [
        "# 1. Evaluate performance of the betting-against-beta portfolio\n",
        "portfolio_stats = calc_summary_statistics(\n",
        "    returns=r_port.to_frame(\"BAB_Portfolio\"),  # convert to DataFrame\n",
        "    annual_factor=252,                         # daily data\n",
        "    provided_excess_returns=True,              # already excess returns\n",
        "    correlations=False,\n",
        "    return_tangency_weights=False\n",
        ")\n",
        "\n",
        "# 2. Select the requested metrics\n",
        "performance = portfolio_stats[[\n",
        "    \"Annualized Mean\",\n",
        "    \"Annualized Vol\",\n",
        "    \"Annualized Sharpe\"\n",
        "]].round(6)\n",
        "\n",
        "print(performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff42e7576c51885",
      "metadata": {},
      "source": [
        "#### 3.4 \n",
        "\n",
        "For your portfolio, test the hypothesis that its premium can be explained by the `LVL` factor alone:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[r_{port}] = \\beta_{port,LVL} \\cdot \\mathbb{E}[r_{LVL}]\n",
        "$$\n",
        "\n",
        "Report exactly the following 3 numbers (rounded to at least 6 decimal places):\n",
        "- Annualized Alpha\n",
        "- `LVL` Beta\n",
        "- R-squared for the regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8e05d640",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alpha 0.05129\n",
            "beta 0.03248\n",
            "r2 0.001208\n"
          ]
        }
      ],
      "source": [
        "# 1. Run regression of portfolio returns on LVL factor\n",
        "bab_lvl_reg = calc_regression(\n",
        "    y=r_port.to_frame(\"BAB_Portfolio\"),\n",
        "    X=factors[[\"LVL\"]],\n",
        "    intercept=True,\n",
        "    annual_factor=252,\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "# 2. Extract the three required numbers\n",
        "annualized_alpha = bab_lvl_reg.loc[\"BAB_Portfolio\", \"Annualized Alpha\"]\n",
        "lvl_beta = bab_lvl_reg.loc[\"BAB_Portfolio\", \"LVL Beta\"]\n",
        "r2 = bab_lvl_reg.loc[\"BAB_Portfolio\", \"R-Squared\"]\n",
        "\n",
        "# 3. Print rounded to six decimals\n",
        "print(\"alpha\", round(annualized_alpha, 6))\n",
        "print(\"beta\", round(lvl_beta, 6))\n",
        "print(\"r2\", round(r2, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5e9e9e4f37b3c8",
      "metadata": {},
      "source": [
        "# 4. Multi-Factor Model\n",
        "\n",
        "We now want to test a multi-factor model using `LVL` and `HMS` and `IMO` as factors:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[r_{i}] = \\beta_{i,WTI} \\cdot \\mathbb{E}[r_{LVL}] + \\beta_{i,HMS} \\cdot \\mathbb{E}[r_{HMS}] + \\beta_{i,IMO} \\cdot \\mathbb{E}[r_{IMO}]\n",
        "$$\n",
        "\n",
        "#### 4.1 Time Series Test\n",
        "Estimate the *time series* test of this pricing model. Regress each commodity's returns against the three factors, and report the following for each commodity (rounded to at least 6 decimal places):\n",
        "- Annualized Alpha\n",
        "- `LVL`, `HMS`, and `IMO` Betas\n",
        "- R-squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1cc14b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Annualized Alpha  LVL Beta  HMS Beta  IMO Beta  R-Squared\n",
            "CL         -0.035599  1.527030  0.656709  0.653682   0.637664\n",
            "GC          0.071595  0.373286  0.123614 -0.393933   0.348973\n",
            "HO         -0.023058  1.205122  0.543109  1.014183   0.759600\n",
            "LE          0.059421  0.328136 -0.148049  0.236226   0.119500\n",
            "NG          0.080357  1.035345  0.662592 -1.501411   0.373050\n",
            "PL         -0.009842  0.711887  0.217593 -0.439602   0.364752\n",
            "RB         -0.015995  1.243184  0.749484  1.335847   0.786582\n",
            "SB         -0.053905  1.065017 -0.542152 -0.510662   0.321713\n",
            "SI          0.056488  0.901595  0.246072 -0.701687   0.434255\n",
            "ZC         -0.031637  1.246405 -0.717257 -0.262703   0.514722\n",
            "ZL         -0.026659  1.055894 -0.356494  0.316702   0.436060\n",
            "ZM         -0.028435  1.159886 -0.789761  0.154944   0.511819\n",
            "ZS         -0.042733  1.147212 -0.645459  0.098414   0.707169\n"
          ]
        }
      ],
      "source": [
        "# 1. Run time-series regressions of each commodity on LVL, HMS, and IMO\n",
        "multi_regressions = calc_iterative_regression(\n",
        "    multiple_y=returns,                    # each commodity's returns\n",
        "    X=factors[[\"LVL\", \"HMS\", \"IMO\"]],     # all three factors\n",
        "    annual_factor=252,                     # daily data\n",
        "    intercept=True,                        # include alpha\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "# 2. Select and round the required columns\n",
        "multi_factor_results = multi_regressions[[\n",
        "    \"Annualized Alpha\",\n",
        "    \"LVL Beta\",\n",
        "    \"HMS Beta\",\n",
        "    \"IMO Beta\",\n",
        "    \"R-Squared\"\n",
        "]].round(6)\n",
        "\n",
        "# 3. Display the table\n",
        "print(multi_factor_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b4864fa98603ee",
      "metadata": {},
      "source": [
        "#### 4.2 \n",
        "\n",
        "Report the annualized Sharpe ratio (rounded to at least 6 decimal places) of the tangency portfolio formed from \n",
        "* the individual commodities.\n",
        "* the three factors\n",
        "\n",
        "What should be true of the Sharpe ratios if the factor pricing model is accurate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "553270c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "indiv 0.852239\n",
            "factors 0.440601\n"
          ]
        }
      ],
      "source": [
        "# ----- Tangency from INDIVIDUAL COMMODITIES -----\n",
        "# Tangency portfolio using all commodity returns as assets\n",
        "tan_port_commodities = calc_tangency_weights(\n",
        "    returns=returns,\n",
        "    annual_factor=252,\n",
        "    cov_mat=1,\n",
        "    name=\"Commodities Tangency\",\n",
        "    return_port_ret=True\n",
        ")\n",
        "\n",
        "tan_comm_stats = calc_summary_statistics(\n",
        "    returns=tan_port_commodities,\n",
        "    annual_factor=252,\n",
        "    provided_excess_returns=True,\n",
        "    correlations=False,\n",
        "    return_tangency_weights=False\n",
        ")\n",
        "\n",
        "comm_sharpe = tan_comm_stats.loc[\"Commodities Tangency Portfolio\", \"Annualized Sharpe\"]\n",
        "\n",
        "\n",
        "# ----- Tangency from the THREE FACTORS -----\n",
        "tan_port_factors = calc_tangency_weights(\n",
        "    returns=factors[[\"LVL\", \"HMS\", \"IMO\"]],\n",
        "    annual_factor=252,\n",
        "    cov_mat=1,\n",
        "    name=\"Factors Tangency\",\n",
        "    return_port_ret=True\n",
        ")\n",
        "\n",
        "tan_fact_stats = calc_summary_statistics(\n",
        "    returns=tan_port_factors,\n",
        "    annual_factor=252,\n",
        "    provided_excess_returns=True,\n",
        "    correlations=False,\n",
        "    return_tangency_weights=False\n",
        ")\n",
        "\n",
        "fact_sharpe = tan_fact_stats.loc[\"Factors Tangency Portfolio\", \"Annualized Sharpe\"]\n",
        "\n",
        "\n",
        "# ----- Print the two Sharpe ratios -----\n",
        "print(\"indiv\", round(comm_sharpe, 6))\n",
        "print(\"factors\", round(fact_sharpe, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37d76f6",
      "metadata": {},
      "source": [
        " - If the LVL–HMS–IMO factor pricing model is correct and the factors span all priced risk in the commodities:\n",
        " - The tangency Sharpe ratio using all commodities should be equal to the tangency Sharpe ratio using only the factors (up to sampling noise).\n",
        "\n",
        "\n",
        "The fact that the commodity-level tangency Sharpe is almost twice as high implies the three factors miss additional priced sources of risk or return, so the factor model is incomplete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa2d4466656a590",
      "metadata": {},
      "source": [
        "#### 4.3 Cross-Sectional Test\n",
        "\n",
        "Run the cross-sectional test of this multi-factor model:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}\\left[\\tilde{r}^{i}\\right] = \\lambda_{0} + \\lambda_{LVL} \\cdot \\beta_{i,LVL} + \\lambda_{HMS} \\cdot \\beta_{i,HMS} + \\lambda_{IMO} \\cdot \\beta_{i,IMO} + \\nu_{i}\n",
        "$$\n",
        "\n",
        "Report exactly the following numbers (rounded to at least 6 decimal places):\n",
        "- $\\lambda_{0}$\n",
        "- $\\lambda_{LVL}$\n",
        "- $\\lambda_{HMS}$\n",
        "- $\\lambda_{IMO}$\n",
        "- $r^2$ of the cross-sectional regression\n",
        "- MAE of the cross-sectional regression\n",
        "\n",
        "Annualize the MAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9a3fd3b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lambda_0   = 0.000321\n",
            "lambda_LVL = -0.000065\n",
            "lambda_HMS = 0.000200\n",
            "lambda_IMO = -0.000067\n",
            "R^2        = 0.646543\n",
            "MAE_ann    = 0.015664\n"
          ]
        }
      ],
      "source": [
        "# 1. Compute the dependent variable: mean excess return for each commodity\n",
        "mean_excess_returns = returns.mean().to_frame(\"Mean Excess Return\")\n",
        "\n",
        "# 2. Get the betas from the multi-factor time-series regressions (4.1)\n",
        "multi_betas = multi_regressions[[\"LVL Beta\", \"HMS Beta\", \"IMO Beta\"]]\n",
        "\n",
        "# 3. Run the cross-sectional regression\n",
        "multi_cross_section = calc_regression(\n",
        "    y=mean_excess_returns,\n",
        "    X=multi_betas,\n",
        "    intercept=True,\n",
        "    annual_factor=252,   # daily data (doesn't affect cross-sectional R², etc.)\n",
        "    warnings=False\n",
        ")\n",
        "\n",
        "# 4. Extract the required statistics\n",
        "lambda_0   = multi_cross_section.loc[\"Mean Excess Return\", \"Alpha\"]\n",
        "lambda_lvl = multi_cross_section.loc[\"Mean Excess Return\", \"LVL Beta Beta\"]\n",
        "lambda_hms = multi_cross_section.loc[\"Mean Excess Return\", \"HMS Beta Beta\"]\n",
        "lambda_imo = multi_cross_section.loc[\"Mean Excess Return\", \"IMO Beta Beta\"]\n",
        "r2         = multi_cross_section.loc[\"Mean Excess Return\", \"R-Squared\"]\n",
        "\n",
        "# 5. Compute annualized mean absolute error (MAE)\n",
        "residuals = mean_excess_returns[\"Mean Excess Return\"] - (\n",
        "    lambda_0 +\n",
        "    multi_betas[\"LVL Beta\"] * lambda_lvl +\n",
        "    multi_betas[\"HMS Beta\"] * lambda_hms +\n",
        "    multi_betas[\"IMO Beta\"] * lambda_imo\n",
        ")\n",
        "mae = residuals.abs().mean() * 252  # annualize\n",
        "\n",
        "# 6. Print labeled results (rounded to 6 decimals)\n",
        "print(f\"lambda_0   = {lambda_0:.6f}\")\n",
        "print(f\"lambda_LVL = {lambda_lvl:.6f}\")\n",
        "print(f\"lambda_HMS = {lambda_hms:.6f}\")\n",
        "print(f\"lambda_IMO = {lambda_imo:.6f}\")\n",
        "print(f\"R^2        = {r2:.6f}\")\n",
        "print(f\"MAE_ann    = {mae:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4095b9a982f2e0",
      "metadata": {},
      "source": [
        "#### 4.4 Interpretation\n",
        "\n",
        "Do the results of the cross-sectional test support the multi-factor model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428628ab",
      "metadata": {},
      "source": [
        "Yes — your results do support the multi-factor model.\n",
        "\n",
        "The high $R^2 = 0.646543$ means that the three factors (LVL, HMS, IMO) explain a substantial portion of the variation in mean returns across commodities. The small intercept ($\\lambda_0 = 0.000321$) and modest annualized pricing error ($\\text{MAE} = 0.015664$) indicate that the model leaves little systematic mispricing. Although the individual factor premia are small, the overall fit suggests that the multi-factor specification captures the main drivers of commodity returns reasonably well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ddce9a26b6f5c",
      "metadata": {},
      "source": [
        "#### 4.5 Risk Premia\n",
        "\n",
        "Compare the risk premia ($\\lambda$'s) from the cross-sectional test to the average returns of each factor. Report exactly the following table (rounded to at least 6 decimal places):\n",
        "\n",
        "- Average return of each factor\n",
        "- Risk premia from the cross-sectional test\n",
        "\n",
        "Annualize the estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f7dbfd43",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Avg Annualized Return  Risk Premium (λ)\n",
            "LVL               0.064439         -0.000065\n",
            "HMS               0.008842          0.000200\n",
            "IMO               0.007281         -0.000067\n"
          ]
        }
      ],
      "source": [
        "# 1. Annualized average returns of each factor (from factors DataFrame)\n",
        "avg_factor_returns = (factors.mean() * 252).to_frame(\"Avg Annualized Return\")\n",
        "\n",
        "# 2. Add the cross-sectional risk premia (λ’s) from 4.3\n",
        "cross_section_premia = pd.DataFrame({\n",
        "    \"Risk Premium (λ)\": {\n",
        "        \"LVL\": lambda_lvl,\n",
        "        \"HMS\": lambda_hms,\n",
        "        \"IMO\": lambda_imo\n",
        "    }\n",
        "})\n",
        "\n",
        "# 3. Combine both into one table and round to 6 decimals\n",
        "risk_premia_comparison = avg_factor_returns.join(cross_section_premia).round(6)\n",
        "\n",
        "print(risk_premia_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1772c21302d6f3",
      "metadata": {},
      "source": [
        "#### 4.6 Interpretation\n",
        "\n",
        "What do you observe from the comparison of average returns and risk premia? \n",
        "\n",
        "Theoretically, what could cause the risk premium of a factor to deviate from its average return?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dda5599",
      "metadata": {},
      "source": [
        "The comparison shows that the estimated risk premia are much smaller than the factors’ average returns, implying that most of the factors’ historical performance is not explained by priced risk. This suggests that part of their average return may reflect noise, sample-specific shocks, or non-systematic effects. Theoretically, risk premia deviate from average returns when factors contain idiosyncratic or transient components that are not truly priced in equilibrium. Sampling error, omitted factors, or structural changes in commodity markets can also drive a wedge between realized mean returns and true underlying risk compensation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "finm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
